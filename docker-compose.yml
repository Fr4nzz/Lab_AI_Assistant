services:
  # LangGraph Backend
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      - LLM_PROVIDER=${LLM_PROVIDER:-gemini}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - GEMINI_API_KEYS=${GEMINI_API_KEYS}
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
      - DATABASE_URL=sqlite:///./data/checkpoints.db
      - HEADLESS=${HEADLESS:-false}
      - DISPLAY=${DISPLAY:-:0}
    volumes:
      - ./backend/data:/app/data
      - ./backend/browser_data:/app/browser_data
      - /tmp/.X11-unix:/tmp/.X11-unix  # For Playwright display

  # LobeChat Frontend (using official image)
  frontend:
    image: lobehub/lobe-chat:latest
    ports:
      - "3210:3210"
    environment:
      # Disable default OpenAI (we use our own backend as OpenAI proxy)
      - ENABLED_OPENAI=0

      # Lab Assistant backend (appears as custom OpenAI-compatible provider)
      - OPENAI_PROXY_URL=http://backend:8000/v1
      - OPENAI_API_KEY=dummy
      - CUSTOM_MODELS=lab-assistant=Lab Assistant<100000:vision:fc>

      # OpenRouter for auxiliary tasks (topic naming, translation)
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
      - ENABLED_OPENROUTER=1
      - OPENROUTER_MODEL_LIST=-all,+meta-llama/llama-3.2-3b-instruct:free=Llama 3.2 Free,+google/gemma-2-9b-it:free=Gemma 2 Free
    depends_on:
      - backend

volumes:
  postgres_data:
