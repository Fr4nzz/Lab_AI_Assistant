"""
FastAPI server with LangGraph integration.

DOCUMENTATION:
- LangGraph + FastAPI: https://langchain-ai.github.io/langgraph/how-tos/deploy-self-hosted/
- Streaming: https://langchain-ai.github.io/langgraph/concepts/streaming/
- Checkpointing: https://langchain-ai.github.io/langgraph/concepts/persistence/

ENDPOINTS:
- POST /api/chat: Send message, get response
- GET /api/chat/{thread_id}/history: Get conversation history
- GET /api/browser/screenshot: Get current browser state
- GET /api/health: Health check

NO APPROVAL ENDPOINTS NEEDED - Website's Save button is the human-in-the-loop.
"""
import os
import sys
import asyncio
import base64
import uuid
import json
import logging
import re
import csv
from datetime import datetime
from typing import Optional, List
from contextlib import asynccontextmanager
from pathlib import Path

# Configure logging - use INFO level to reduce noise
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

# Exams list file (generated by scripts/process_tarifas.py)
EXAMS_FILE = Path(__file__).parent / "config" / "lista_de_examenes.csv"

# Reduce noise from various loggers
logging.getLogger("asyncio").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)

# Set up Windows event loop policy if on Windows
if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from langchain_core.messages import HumanMessage, AIMessage

# LangGraph imports
from langgraph.checkpoint.memory import MemorySaver
# For production with SQLite:
# from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver

# Local imports
from graph.agent import create_lab_agent, compile_agent
from graph.tools import set_browser, close_all_tabs, get_active_tabs, _get_browser_tabs_impl, reset_tab_state, ALL_TOOLS
from browser_manager import BrowserManager
from extractors import EXTRACT_ORDENES_JS
from config import settings
from prompts import SYSTEM_PROMPT
from stream_adapter import StreamAdapter


def load_exams_from_csv() -> List[dict]:
    """
    Load available exams from CSV file.
    Returns list of exams with codigo, nombre, and prices.
    """
    if not EXAMS_FILE.exists():
        logger.warning(f"[Exams] File not found: {EXAMS_FILE}")
        return []

    exams = []
    with open(EXAMS_FILE, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            exams.append({
                'codigo': row.get('codigo', ''),
                'nombre': row.get('nombre', ''),
                'precio': float(row.get('precio_particular', 0) or 0),
                'precio_desc': float(row.get('precio_descuento', 0) or 0),
            })

    logger.info(f"[Exams] Loaded {len(exams)} exams from {EXAMS_FILE.name}")
    return exams


# Cache exams at module level (loaded once at startup)
_cached_exams: List[dict] = []


def get_available_exams_context() -> str:
    """Generate context string for available exams."""
    global _cached_exams

    if not _cached_exams:
        _cached_exams = load_exams_from_csv()

    if not _cached_exams:
        return ""

    lines = ["# ExÃ¡menes Disponibles (CÃ³digo - Nombre - Precio)"]
    for exam in _cached_exams:
        precio = f"${exam['precio']:.2f}" if exam['precio'] else "N/A"
        lines.append(f"- {exam['codigo']}: {exam['nombre'][:45]} ({precio})")

    lines.append("")
    lines.append("*Para cotizaciÃ³n: create_new_order(cedula=\"\", exams=[\"CODIGO1\",...])*")

    return "\n".join(lines)


# ============================================================
# PYDANTIC MODELS
# ============================================================

class ChatRequest(BaseModel):
    message: str
    thread_id: Optional[str] = None


class ChatResponse(BaseModel):
    status: str  # "complete", "error"
    message: str
    thread_id: str
    iterations: Optional[int] = None


# ============================================================
# GLOBAL STATE
# ============================================================

browser: Optional[BrowserManager] = None
graphs: dict = {}  # Dictionary of graphs, keyed by model name
checkpointer = None
initial_orders_context: str = ""  # Store initial orders for context
orders_context_sent: bool = False  # Track if we've sent valid orders context

# Available models (must match frontend MODEL_CONFIGS)
AVAILABLE_MODELS = [
    "gemini-3-flash-preview",
    "gemini-flash-latest",
]
DEFAULT_MODEL = "gemini-3-flash-preview"


# ============================================================
# HELPER FUNCTIONS
# ============================================================

def is_logged_in() -> bool:
    """Check if the browser is logged in (not on login page)."""
    if browser and browser.page:
        return "/login" not in browser.page.url
    return False


async def get_orders_context() -> str:
    """
    Get orders context, checking login state first.
    Returns empty string with login message if not logged in.
    Returns orders table if logged in and orders found.
    """
    global browser, initial_orders_context, orders_context_sent

    if not is_logged_in():
        logger.info("[Context] User not logged in - browser is on login page")
        return "âš ï¸ SESIÃ“N NO INICIADA: El navegador estÃ¡ en la pÃ¡gina de login. Por favor, inicia sesiÃ³n en el navegador para que pueda acceder a las Ã³rdenes del laboratorio."

    # If we haven't sent valid orders yet, try to extract them
    if not orders_context_sent or not initial_orders_context:
        logger.info("[Context] Extracting orders context...")
        initial_orders_context = await extract_initial_context()
        if initial_orders_context and "Ã“rdenes Recientes" in initial_orders_context:
            orders_context_sent = True
            logger.info(f"[Context] Extracted {initial_orders_context.count('|') // 8} orders")

    return initial_orders_context


async def get_browser_tabs_context() -> str:
    """
    Get browser tabs context with state tracking.

    - Shows all tabs with IDs, patient names, and enumeration for duplicates
    - For NEW tabs: shows full state (exams, fields, etc.)
    - For KNOWN tabs: shows only what CHANGED since last message
    - Marks which tab is active
    """
    try:
        tabs_info = await _get_browser_tabs_impl()

        if not tabs_info.get("tabs"):
            return "# PestaÃ±as del Navegador\nNo hay pestaÃ±as abiertas. Para editar resultados, primero usa get_order_results(order_nums) para abrir la pestaÃ±a de resultados."

        lines = ["# PestaÃ±as del Navegador"]

        # Track if there are any results tabs
        has_results_tabs = any(t.get("type") == "resultados" for t in tabs_info.get("tabs", []))

        type_display = {
            "ordenes_list": "Lista de Ã“rdenes",
            "nueva_orden": "Nueva Orden",
            "orden_edit": "Editar Orden",
            "resultados": "Resultados",
            "login": "Login",
            "unknown": "Otra"
        }

        for idx, tab in enumerate(tabs_info.get("tabs", [])):
            tab_type = tab.get("type", "unknown")
            is_active = tab.get("active", False)
            is_new = tab.get("is_new", False)
            tab_id = tab.get("id")
            paciente = tab.get("paciente")
            instance = tab.get("instance")  # For duplicate enumeration
            state = tab.get("state")  # Full state for new tabs
            changes = tab.get("changes")  # Only changes for known tabs

            # Build tab header - always include tab_index
            marker = "â†’ " if is_active else "  "
            tab_line = f"{marker}[tab_index={idx}] {type_display.get(tab_type, tab_type)}"

            # Add ID for saved orders
            if tab_id:
                if tab_type == "resultados":
                    tab_line += f" (order_num={tab_id})"
                elif tab_type == "orden_edit":
                    tab_line += f" (order_id={tab_id})"

            # Add instance number for duplicates
            if instance:
                tab_line += f" #{instance}"

            # Add patient name
            if paciente:
                tab_line += f" - {paciente[:25]}"

            # Add NEW marker for unsaved orders
            if is_new:
                tab_line += " [NUEVA - sin guardar]"

            lines.append(tab_line)

            # For new tabs, show full state
            if is_new and state:
                if tab_type == "resultados":
                    lines.append(f"    ExÃ¡menes: {state.get('examenes_count', 0)}")
                    # Show field values (limited)
                    field_values = state.get("field_values", {})
                    filled = [(k, v) for k, v in field_values.items() if v]
                    if filled:
                        lines.append(f"    Campos con valor: {len(filled)}")
                elif tab_type in ["orden_edit", "nueva_orden"]:
                    exams = state.get("exams", [])
                    if exams:
                        lines.append(f"    ExÃ¡menes: {', '.join(exams[:8])}")
                    if state.get("total"):
                        lines.append(f"    Total: {state.get('total')}")

            # For known tabs, show only changes
            elif changes:
                lines.append("    **Cambios detectados:**")
                if "field_values" in changes:
                    # Show which fields changed
                    for field_key, new_value in list(changes["field_values"].items())[:5]:
                        if new_value:
                            lines.append(f"    - {field_key}: â†’ {new_value}")
                if "exams" in changes:
                    lines.append(f"    - ExÃ¡menes: {', '.join(changes['exams'][:5])}")
                if "total" in changes:
                    lines.append(f"    - Total: {changes['total']}")

        # Add guidance if no results tabs are open
        if not has_results_tabs:
            lines.append("")
            lines.append("âš ï¸ No hay pestaÃ±as de resultados abiertas. Para editar resultados, usa get_order_results(order_nums) primero.")

        return "\n".join(lines)

    except Exception as e:
        logger.warning(f"[Context] Could not get browser tabs context: {e}")
        return ""


# ============================================================
# LIFESPAN
# ============================================================

async def extract_initial_context() -> str:
    """Extract initial orders list and available exams from the page for AI context."""
    global browser
    lines = []

    try:
        # Fetch orders from page 1
        page1_url = "https://laboratoriofranz.orion-labs.com/ordenes?page=1"
        logger.info("[Context] Fetching orders page 1...")
        await browser.page.goto(page1_url, timeout=30000)
        # Wait for table rows instead of fixed 2s delay
        try:
            await browser.page.wait_for_selector('table tbody tr, .order-row', timeout=5000)
        except Exception:
            await browser.page.wait_for_timeout(500)
        ordenes_page1 = await browser.page.evaluate(EXTRACT_ORDENES_JS) or []

        # Fetch orders from page 2
        page2_url = "https://laboratoriofranz.orion-labs.com/ordenes?page=2"
        logger.info("[Context] Fetching orders page 2...")
        await browser.page.goto(page2_url, timeout=30000)
        try:
            await browser.page.wait_for_selector('table tbody tr, .order-row', timeout=5000)
        except Exception:
            await browser.page.wait_for_timeout(500)
        ordenes_page2 = await browser.page.evaluate(EXTRACT_ORDENES_JS) or []

        # Combine orders from both pages
        all_ordenes = ordenes_page1 + ordenes_page2

        if all_ordenes:
            lines.append("# Ã“rdenes Recientes (40 mÃ¡s recientes)")
            lines.append("| # | Orden | Fecha | Paciente | CÃ©dula | Estado | ID |")
            lines.append("|---|-------|-------|----------|--------|--------|-----|")
            for i, o in enumerate(all_ordenes[:40]):
                paciente = (o.get('paciente', '') or '')[:30]
                lines.append(f"| {i+1} | {o.get('num','')} | {o.get('fecha','')} | {paciente} | {o.get('cedula','')} | {o.get('estado','')} | {o.get('id','')} |")
            logger.info(f"[Context] Extracted {len(all_ordenes)} orders from 2 pages")

        # Navigate back to orders page
        await browser.page.goto("https://laboratoriofranz.orion-labs.com/ordenes", timeout=30000)

    except Exception as e:
        logger.warning(f"Could not extract orders context: {e}")

    # Add available exams from CSV file (faster than scraping, complete list)
    exams_context = get_available_exams_context()
    if exams_context:
        lines.append("")
        lines.append(exams_context)

    return "\n".join(lines) if lines else ""


@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Application lifespan - initialize browser and LangGraph.

    For fast startup, we skip initial orders extraction - it will be done
    lazily on first chat message instead.
    """
    global browser, graph, checkpointer, initial_orders_context

    print("Starting Lab Assistant with LangGraph...")

    # Create data directory if it doesn't exist
    data_dir = Path(__file__).parent / "data"
    data_dir.mkdir(exist_ok=True)

    # Initialize browser (required before accepting requests)
    browser = BrowserManager(user_data_dir=settings.browser_data_dir)
    await browser.start(headless=settings.headless, browser=settings.browser_channel)
    await browser.navigate(settings.target_url)
    set_browser(browser)

    # Check if we ended up at the welcome page (session redirect) and navigate to orders
    if browser.page and "/bienvenida" in browser.page.url:
        logger.info("[Startup] Redirected to welcome page, navigating to orders...")
        await browser.navigate("https://laboratoriofranz.orion-labs.com/ordenes?page=1")

    # Check login status - if on login page, auto-click the "Ingresar" button
    if browser.page and "/login" in browser.page.url:
        logger.info("[Startup] On login page - attempting auto-login by clicking 'Ingresar' button...")
        try:
            # Click the Ingresar button to login with saved session
            ingresar_button = browser.page.locator("button:has-text('Ingresar')")
            if await ingresar_button.count() > 0:
                await ingresar_button.click()
                logger.info("[Startup] Clicked 'Ingresar' button, waiting for navigation...")
                # Wait for navigation to complete
                await browser.page.wait_for_load_state("networkidle", timeout=10000)
                logger.info(f"[Startup] After login click, now at: {browser.page.url}")

                # If we're still on login page, the session might have expired
                if "/login" in browser.page.url:
                    logger.warning("[Startup] Still on login page - session may have expired. User needs to login manually.")
                    initial_orders_context = "âš ï¸ SESIÃ“N NO INICIADA: El navegador estÃ¡ en la pÃ¡gina de login. Por favor, inicia sesiÃ³n manualmente."
                else:
                    # Navigate to orders page
                    if "/bienvenida" in browser.page.url:
                        logger.info("[Startup] Redirected to welcome page, navigating to orders...")
                        await browser.navigate("https://laboratoriofranz.orion-labs.com/ordenes?page=1")

                    # Extract orders
                    logger.info("[Startup] Extracting orders context after login...")
                    try:
                        orders_context = await extract_initial_context()
                        if orders_context:
                            initial_orders_context = orders_context
                            logger.info(f"[Startup] Extracted {len(orders_context)} chars of orders context")
                        else:
                            initial_orders_context = ""
                            logger.info("[Startup] No orders found")
                    except Exception as e:
                        logger.error(f"[Startup] Failed to extract orders: {e}")
                        initial_orders_context = ""
            else:
                logger.warning("[Startup] 'Ingresar' button not found on login page")
                initial_orders_context = "âš ï¸ SESIÃ“N NO INICIADA: No se encontrÃ³ el botÃ³n 'Ingresar'."
        except Exception as e:
            logger.error(f"[Startup] Auto-login failed: {e}")
            initial_orders_context = f"âš ï¸ SESIÃ“N NO INICIADA: Error al intentar login automÃ¡tico: {e}"
    else:
        logger.info("[Startup] Extracting orders context on startup...")
        try:
            orders_context = await extract_initial_context()
            if orders_context:
                initial_orders_context = orders_context
                logger.info(f"[Startup] Extracted {len(orders_context)} chars of orders context")
            else:
                initial_orders_context = ""
                logger.info("[Startup] No orders found")
        except Exception as e:
            logger.error(f"[Startup] Failed to extract orders: {e}")
            initial_orders_context = ""

    # Initialize checkpointer for conversation persistence
    # Using MemorySaver for development (in-memory, not persistent across restarts)
    # For production, use AsyncSqliteSaver or PostgresSaver
    checkpointer = MemorySaver()

    # Build and compile a graph for each available model
    for model_name in AVAILABLE_MODELS:
        logger.info(f"[Startup] Creating graph for model: {model_name}")
        builder = create_lab_agent(browser, model_name=model_name)
        graphs[model_name] = compile_agent(builder, checkpointer)
        logger.info(f"[Startup] Graph for {model_name} ready")

    print(f"Lab Assistant ready! Browser at: {browser.page.url}")
    print(f"Available models: {', '.join(AVAILABLE_MODELS)}")

    yield

    # Cleanup
    print("Shutting down...")
    close_all_tabs()
    await browser.stop()


# ============================================================
# FASTAPI APP
# ============================================================

app = FastAPI(
    title="Lab Assistant API",
    description="LangGraph-powered lab assistant for clinical laboratory data entry",
    version="2.0.0",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ============================================================
# ENDPOINTS
# ============================================================

@app.get("/api/health")
async def health_check():
    """Health check endpoint."""
    return {
        "status": "ok",
        "browser_url": browser.page.url if browser and browser.page else None,
        "graphs_ready": list(graphs.keys()),
        "default_model": DEFAULT_MODEL
    }


@app.post("/api/chat", response_model=ChatResponse)
async def chat(
    thread_id: str = Form(default=None),
    message: str = Form(...),
    files: List[UploadFile] = File(default=[])
):
    """
    Send a message to the agent and get a response.

    Supports multi-modal input (text, images, audio).
    The agent will execute tools as needed and return when done.

    Args:
        thread_id: Conversation thread ID (generated if not provided)
        message: User message text
        files: Optional image or audio files

    Returns:
        Agent's response with thread_id for continuation
    """
    # Generate thread_id if not provided
    if not thread_id:
        thread_id = str(uuid.uuid4())

    config = {"configurable": {"thread_id": thread_id}}

    # Build message content (multi-modal support)
    content = []

    if message:
        content.append({"type": "text", "text": message})

    # Process uploaded files
    for file in files:
        file_content = await file.read()
        encoded = base64.b64encode(file_content).decode('utf-8')

        if file.content_type and file.content_type.startswith("image/"):
            # Image for vision models
            content.append({
                "type": "image_url",
                "image_url": {"url": f"data:{file.content_type};base64,{encoded}"}
            })
        elif file.content_type and file.content_type.startswith("audio/"):
            # Audio for Gemini (native audio support)
            content.append({
                "type": "media",
                "data": encoded,
                "mime_type": file.content_type
            })

    # Create human message
    if len(content) == 1 and content[0]["type"] == "text":
        human_msg = HumanMessage(content=message)
    else:
        human_msg = HumanMessage(content=content)

    try:
        # Use default model graph (this endpoint doesn't accept model parameter)
        graph = graphs.get(DEFAULT_MODEL)
        if not graph:
            raise ValueError(f"Default graph '{DEFAULT_MODEL}' not found")

        # Invoke graph - it will loop internally until done
        result = await graph.ainvoke(
            {"messages": [human_msg]},
            config
        )

        # Get the final response
        last_msg = result["messages"][-1]
        response_text = last_msg.content if hasattr(last_msg, 'content') else str(last_msg)

        # Count iterations (based on tool messages)
        tool_messages = [m for m in result["messages"] if hasattr(m, 'type') and getattr(m, 'type', None) == 'tool']
        iterations = len(tool_messages)

        return ChatResponse(
            status="complete",
            message=response_text,
            thread_id=thread_id,
            iterations=iterations
        )

    except Exception as e:
        import traceback
        traceback.print_exc()
        return ChatResponse(
            status="error",
            message=f"Error: {str(e)}",
            thread_id=thread_id
        )


@app.post("/api/chat/stream")
async def chat_stream(request: ChatRequest):
    """
    Stream chat responses in real-time using Server-Sent Events.

    DOCUMENTATION:
    - astream_events: https://langchain-ai.github.io/langgraph/concepts/streaming/

    This streams:
    - Token-by-token LLM output
    - Tool execution notifications
    - Final completion
    """
    thread_id = request.thread_id or str(uuid.uuid4())
    config = {"configurable": {"thread_id": thread_id}}

    # Use default model graph (this endpoint doesn't accept model parameter)
    graph = graphs.get(DEFAULT_MODEL)

    async def generate():
        try:
            async for event in graph.astream_events(
                {"messages": [HumanMessage(content=request.message)]},
                config,
                version="v2"
            ):
                event_type = event.get("event", "")

                # Stream LLM tokens
                if event_type == "on_chat_model_stream":
                    chunk = event["data"].get("chunk")
                    if chunk and hasattr(chunk, 'content') and chunk.content:
                        yield f"data: {json.dumps({'type': 'token', 'content': chunk.content})}\n\n"

                # Notify tool execution
                elif event_type == "on_tool_start":
                    tool_name = event["name"]
                    yield f"data: {json.dumps({'type': 'tool_start', 'tool': tool_name})}\n\n"

                elif event_type == "on_tool_end":
                    tool_name = event["name"]
                    yield f"data: {json.dumps({'type': 'tool_end', 'tool': tool_name})}\n\n"

            yield f"data: {json.dumps({'type': 'done', 'thread_id': thread_id})}\n\n"

        except Exception as e:
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={"X-Thread-ID": thread_id}
    )


@app.get("/api/chat/{thread_id}/history")
async def get_history(thread_id: str):
    """
    Get conversation history for a thread.

    Returns list of messages with role and content.
    """
    config = {"configurable": {"thread_id": thread_id}}

    # Use default graph for state retrieval (all graphs share the same checkpointer)
    graph = graphs.get(DEFAULT_MODEL)

    try:
        state = await graph.aget_state(config)
        messages = state.values.get("messages", [])

        return [
            {
                "role": "user" if isinstance(m, HumanMessage) else "assistant",
                "content": m.content if hasattr(m, 'content') else str(m),
                "type": getattr(m, 'type', 'unknown')
            }
            for m in messages
            if not (hasattr(m, 'type') and getattr(m, 'type', None) == 'tool')  # Skip tool messages
        ]
    except Exception as e:
        return []


@app.get("/api/browser/screenshot")
async def get_screenshot():
    """Get current browser screenshot as base64."""
    if browser and browser.page:
        try:
            screenshot_bytes = await browser.page.screenshot(type='png')
            encoded = base64.b64encode(screenshot_bytes).decode('utf-8')
            return {"screenshot": f"data:image/png;base64,{encoded}"}
        except Exception as e:
            raise HTTPException(500, f"Screenshot failed: {str(e)}")
    raise HTTPException(503, "Browser not available")


@app.get("/api/browser/tabs")
async def get_tabs():
    """Get list of open browser tabs."""
    active_tabs = get_active_tabs()
    return {
        "tabs": list(active_tabs.keys()),
        "count": len(active_tabs)
    }


@app.get("/api/browser/tabs/detailed")
async def get_tabs_detailed():
    """Get detailed info about all browser tabs including state."""
    try:
        tabs_info = await _get_browser_tabs_impl()
        return tabs_info
    except Exception as e:
        logger.error(f"Failed to get detailed tabs: {e}")
        return {"error": str(e), "tabs": []}


@app.post("/api/browser/close-tabs")
async def close_tabs():
    """Close all open browser tabs (cleanup)."""
    close_all_tabs()
    return {"status": "ok", "message": "All tabs closed"}


# ============================================================
# MANUAL TOOL EXECUTION ENDPOINT
# ============================================================

class ManualToolRequest(BaseModel):
    tool: str
    args: dict

@app.post("/api/tools/execute")
async def execute_tool(request: ManualToolRequest):
    """
    Execute a tool manually (for UI-based editing).
    Supports: edit_results, edit_order_exams, create_new_order
    """
    from graph.tools import (
        edit_results,
        edit_order_exams,
        create_new_order,
    )

    tool_name = request.tool
    args = request.args

    logger.info(f"[Manual Tool] Executing {tool_name} with args: {args}")

    try:
        if tool_name == "edit_results":
            # edit_results expects data=[{orden, e, f, v}]
            data = args.get("data", [])
            result = await edit_results.ainvoke({"data": data})
            return {"success": True, "message": f"Editados {len(data)} campos", "result": result}

        elif tool_name == "edit_order_exams":
            # edit_order_exams now supports order_id, tab_index, cedula, add, remove
            order_id = args.get("order_id")
            tab_index = args.get("tab_index")
            cedula = args.get("cedula")
            add_exams = args.get("add", [])
            remove_exams = args.get("remove", [])
            result = await edit_order_exams.ainvoke({
                "order_id": order_id,
                "tab_index": tab_index,
                "cedula": cedula,
                "add": add_exams,
                "remove": remove_exams
            })
            changes = []
            if cedula:
                changes.append(f"cÃ©dula: {cedula}")
            if add_exams:
                changes.append(f"agregados: {', '.join(add_exams)}")
            if remove_exams:
                changes.append(f"removidos: {', '.join(remove_exams)}")
            return {"success": True, "message": f"Cambios: {' | '.join(changes) if changes else 'ninguno'}", "result": result}

        elif tool_name == "create_new_order":
            cedula = args.get("cedula", "")
            exams = args.get("exams", [])
            result = await create_new_order.ainvoke({"cedula": cedula, "exams": exams})
            return {"success": True, "message": f"Orden creada con {len(exams)} exÃ¡menes", "result": result}

        else:
            return {"success": False, "error": f"Tool '{tool_name}' not supported for manual execution"}

    except Exception as e:
        logger.error(f"[Manual Tool] Error: {e}")
        return {"success": False, "error": str(e)}


@app.get("/api/exams")
async def get_exams():
    """Get list of available exams from CSV (cached)."""
    global _cached_exams
    if not _cached_exams:
        _cached_exams = load_exams_from_csv()
    return {"exams": [{"codigo": e["codigo"], "nombre": e["nombre"]} for e in _cached_exams]}


@app.get("/api/usage")
async def get_usage():
    """Get current usage stats for all models."""
    from models import get_usage_stats, get_daily_limit, get_num_api_keys

    stats = get_usage_stats()
    daily_limit = get_daily_limit()
    num_keys = get_num_api_keys()

    return {
        "date": stats.get("date"),
        "models": stats.get("models", {}),
        "dailyLimit": daily_limit,
        "numApiKeys": num_keys,
        "freePerKey": 20  # Google AI Studio free tier limit per key
    }


# ============================================================
# IMAGE ROTATION DETECTION ENDPOINT
# ============================================================

class ImageRotationRequest(BaseModel):
    image: str  # base64 data (with or without data URL prefix)
    mimeType: str = "image/jpeg"


@app.post("/api/detect-rotation")
async def detect_image_rotation(request: ImageRotationRequest):
    """
    Detect if an image needs rotation correction using Gemini vision.
    Uses the existing key rotation system for rate limit handling.
    """
    import time
    from models import get_chat_model

    start_time = time.time()

    try:
        # Get Gemini model with key rotation - use gemini-3-flash-preview for best accuracy
        # Use minimal thinking for faster rotation detection while maintaining quality
        model = get_chat_model(provider="gemini", model_name="gemini-3-flash-preview", thinking_level="minimal")

        # Extract base64 data (remove data URL prefix if present)
        base64_data = request.image
        if base64_data.startswith("data:"):
            base64_data = base64_data.split(",", 1)[1]

        # Create message with image
        message = HumanMessage(content=[
            {
                "type": "text",
                "text": "Look at this image. If the image appears rotated or upside down, respond with ONLY a number: 90, 180, or 270 (degrees clockwise to fix it). If the image is correctly oriented, respond with ONLY: 0"
            },
            {
                "type": "image_url",
                "image_url": {"url": f"data:{request.mimeType};base64,{base64_data}"}
            }
        ])

        # Call model
        result = await model.ainvoke([message])
        response_text = result.content.strip()

        # Parse rotation from response
        rotation = 0
        import re
        match = re.search(r'\b(0|90|180|270)\b', response_text)
        if match:
            rotation = int(match.group(1))

        elapsed_ms = int((time.time() - start_time) * 1000)
        logger.info(f"[Rotation] Gemini detected {rotation}Â° in {elapsed_ms}ms")

        return {
            "rotation": rotation,
            "detected": rotation != 0,
            "provider": "gemini",
            "timing": elapsed_ms,
            "raw": response_text
        }

    except Exception as e:
        elapsed_ms = int((time.time() - start_time) * 1000)
        logger.error(f"[Rotation] Gemini error ({elapsed_ms}ms): {e}")
        return {
            "rotation": 0,
            "detected": False,
            "provider": "gemini",
            "error": str(e),
            "timing": elapsed_ms
        }


# ============================================================
# AI SDK DATA STREAM PROTOCOL ENDPOINT
# ============================================================

class AISdkChatRequest(BaseModel):
    messages: List[dict]
    chatId: Optional[str] = None
    model: Optional[str] = None
    showStats: bool = True


@app.post("/api/chat/aisdk")
async def chat_aisdk(request: AISdkChatRequest):
    """
    AI SDK Data Stream Protocol v1 compatible endpoint.

    This endpoint streams responses in the Vercel AI SDK format,
    which can be consumed directly by useChat on the frontend.

    Documentation: https://sdk.vercel.ai/docs/ai-sdk-ui/stream-protocol
    """
    global orders_context_sent

    thread_id = request.chatId or str(uuid.uuid4())
    config = {"configurable": {"thread_id": thread_id}}

    # Select the appropriate graph based on requested model
    model_name = request.model or DEFAULT_MODEL
    if model_name not in graphs:
        logger.warning(f"[AI SDK] Unknown model '{model_name}', using default: {DEFAULT_MODEL}")
        model_name = DEFAULT_MODEL
    graph = graphs[model_name]
    logger.info(f"[AI SDK] Using model: {model_name}")

    # Convert messages to LangChain format
    conversation_messages = []
    for msg in request.messages:
        role = msg.get("role", "")
        content = msg.get("content", "")

        # Handle multimodal content
        if isinstance(content, list):
            lc_content = []
            for part in content:
                if isinstance(part, dict):
                    if part.get("type") == "text":
                        lc_content.append({"type": "text", "text": part.get("text", "")})
                    elif part.get("type") == "image_url":
                        image_url = part.get("image_url", {})
                        url = image_url.get("url", "") if isinstance(image_url, dict) else image_url
                        lc_content.append({"type": "image_url", "image_url": {"url": url}})
                    elif part.get("type") == "media":
                        lc_content.append({
                            "type": "media",
                            "data": part.get("data", ""),
                            "mime_type": part.get("mime_type", "audio/webm")
                        })
            content = lc_content if lc_content else ""

        if role == "user":
            conversation_messages.append(HumanMessage(content=content))
        elif role == "assistant":
            conversation_messages.append(AIMessage(content=content))

    if not conversation_messages:
        logger.error("[AI SDK] No messages found in request")
        async def error_stream():
            yield StreamAdapter.error("No messages found")
            yield StreamAdapter.finish("error")
        return StreamingResponse(
            error_stream(),
            media_type="text/plain",
            headers={
                "x-vercel-ai-data-stream": "v1",
                "Cache-Control": "no-cache",
                "X-Chat-Id": thread_id,
            }
        )

    # Log request
    last_user_msg = None
    for msg in reversed(request.messages):
        if msg.get("role") == "user":
            last_user_msg = msg.get("content")
            break

    logger.info("=" * 60)
    if isinstance(last_user_msg, list):
        msg_parts = []
        for part in last_user_msg:
            if isinstance(part, dict):
                if part.get('type') == 'text':
                    msg_parts.append(part.get('text', '')[:100])
                elif part.get('type') == 'image_url':
                    # Log image size for debugging
                    image_url_data = part.get('image_url', {})
                    url = image_url_data.get('url', '') if isinstance(image_url_data, dict) else str(image_url_data)
                    if url.startswith('data:'):
                        base64_parts = url.split(',', 1)
                        if len(base64_parts) > 1:
                            base64_size = len(base64_parts[1])
                            approx_bytes = int(base64_size * 0.75)
                            msg_parts.append(f'[IMAGE: ~{approx_bytes // 1024}KB]')
                        else:
                            msg_parts.append('[IMAGE]')
                    else:
                        msg_parts.append('[IMAGE: URL]')
                elif part.get('type') == 'media':
                    msg_parts.append('[MEDIA]')
        user_msg_display = ' + '.join(msg_parts)
    else:
        user_msg_display = str(last_user_msg)[:200] if last_user_msg else ""
    logger.info(f"[AI SDK] USER: {user_msg_display}")
    logger.info(f"[AI SDK] Thread: {thread_id}, Messages: {len(conversation_messages)}")

    async def generate():
        try:
            # Check if first message and reset state
            is_first_message = len(conversation_messages) <= 2
            if is_first_message:
                reset_tab_state()

            # Get context
            current_context = await get_orders_context()
            tabs_context = await get_browser_tabs_context()

            # Build initial state
            initial_state = {"messages": conversation_messages}
            context_parts = []

            should_include_orders = is_first_message or not orders_context_sent
            if should_include_orders and current_context:
                context_parts.append(current_context)
            if tabs_context:
                context_parts.append(tabs_context)
            if context_parts:
                initial_state["current_page_context"] = "\n\n".join(context_parts)

            # Stream events using new AI SDK v6 protocol
            adapter = StreamAdapter()
            full_response = []
            total_input_tokens = 0
            total_output_tokens = 0
            ai_responses = 0  # Track actual AI responses (for usage tracking)

            # Gemini pricing (per 1M tokens)
            # Gemini Flash: $0.075 input, $0.30 output
            INPUT_PRICE_PER_1M = 0.075
            OUTPUT_PRICE_PER_1M = 0.30

            # Import usage tracking
            from models import increment_usage, get_usage_stats, get_daily_limit

            # Start the message
            yield adapter.start_message()

            async for event in graph.astream_events(
                initial_state,
                config,
                version="v2"
            ):
                event_type = event.get("event", "")

                if event_type == "on_tool_start":
                    tool_name = event.get("name", "unknown")
                    tool_input = event.get("data", {}).get("input", {})
                    run_id = event.get("run_id", "")
                    tool_call_id = f"call_{run_id[:12]}" if run_id else None
                    logger.info(f"[AI SDK] Tool: {tool_name}")
                    yield adapter.tool_status(tool_name, "start", tool_input, tool_call_id=tool_call_id)

                elif event_type == "on_tool_end":
                    tool_name = event.get("name", "unknown")
                    run_id = event.get("run_id", "")
                    tool_call_id = f"call_{run_id[:12]}" if run_id else None
                    tool_output = event.get("data", {}).get("output", "")
                    result_str = str(tool_output)[:500] if tool_output else "completed"
                    logger.info(f"[AI SDK] Tool end: {tool_name}")
                    yield adapter.tool_status(tool_name, "end", tool_call_id=tool_call_id, result=result_str)

                elif event_type == "on_chat_model_stream":
                    chunk = event["data"].get("chunk")
                    if chunk and hasattr(chunk, 'content') and chunk.content:
                        content = chunk.content
                        if isinstance(content, list):
                            text_parts = [p.get('text', '') for p in content if isinstance(p, dict) and p.get('type') == 'text']
                            content = ''.join(text_parts)
                        if content:
                            full_response.append(content)
                            yield adapter.text_delta(content)

                elif event_type == "on_chat_model_end":
                    output = event.get("data", {}).get("output")
                    if output:
                        # Count this as an AI response and increment usage
                        ai_responses += 1
                        increment_usage(model_name)

                        # Handle usage metadata
                        usage = getattr(output, 'usage_metadata', None)
                        if usage and isinstance(usage, dict):
                            total_input_tokens += usage.get('input_tokens', 0) or usage.get('prompt_token_count', 0) or 0
                            total_output_tokens += usage.get('output_tokens', 0) or usage.get('candidates_token_count', 0) or 0

                        # If streaming didn't happen, yield the full content here
                        if not full_response and hasattr(output, 'content') and output.content:
                            content = output.content
                            if isinstance(content, str) and content:
                                full_response.append(content)
                                yield adapter.text_delta(content)
                            elif isinstance(content, list):
                                text_parts = [p.get('text', '') for p in content if isinstance(p, dict) and p.get('type') == 'text']
                                text = ''.join(text_parts)
                                if text:
                                    full_response.append(text)
                                    yield adapter.text_delta(text)

            # Calculate and send usage summary as text (if enabled)
            total_tokens = total_input_tokens + total_output_tokens
            input_cost = (total_input_tokens / 1_000_000) * INPUT_PRICE_PER_1M
            output_cost = (total_output_tokens / 1_000_000) * OUTPUT_PRICE_PER_1M
            total_cost = input_cost + output_cost

            if request.showStats:
                # Get current usage stats for this model
                usage_stats = get_usage_stats()
                daily_limit = get_daily_limit()
                model_usage = usage_stats.get("models", {}).get(model_name, 0)

                # Send usage summary at the end
                usage_summary = f"\n\n---\nðŸ“Š **Stats**: {ai_responses} prompts ({model_usage}/{daily_limit} today)"
                if total_tokens > 0:
                    usage_summary += f" | Tokens: {total_input_tokens:,} in + {total_output_tokens:,} out"
                    usage_summary += f" | ${total_cost:.6f}"
                usage_summary += "\n"

                yield adapter.text_delta(usage_summary)

            # Send finish with usage
            usage = None
            if total_input_tokens or total_output_tokens:
                usage = {
                    "promptTokens": total_input_tokens,
                    "completionTokens": total_output_tokens,
                    "totalTokens": total_input_tokens + total_output_tokens
                }
            yield adapter.finish("stop", usage)

            # Log summary
            response_preview = ''.join(full_response)[:100]
            logger.info(f"[AI SDK] Done: {ai_responses} AI responses, {total_tokens} tokens, response: {response_preview}...")

        except Exception as e:
            logger.error(f"[AI SDK] Error: {e}", exc_info=True)
            adapter = StreamAdapter()
            yield adapter.error(str(e))
            yield adapter.finish("error")

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "x-vercel-ai-ui-message-stream": "v1",
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Chat-Id": thread_id,
        }
    )


# ============================================================
# OPENAI-COMPATIBLE ENDPOINT (Optional - for LobeChat integration)
# ============================================================

class OpenAIChatRequest(BaseModel):
    model: str
    messages: List[dict]
    stream: bool = False
    temperature: float = 0.7


@app.post("/v1/chat/completions")
async def openai_compatible_chat(request: OpenAIChatRequest):
    """
    OpenAI-compatible chat completions endpoint.

    This translates OpenAI format to our LangGraph agent format,
    allowing LobeChat to use our agent as a model provider.
    """
    import time as time_module

    # Debug: log raw request
    logger.debug(f"[Request] Messages count: {len(request.messages)}")
    for i, msg in enumerate(request.messages):
        content = msg.get('content', '')
        # Handle multimodal content (list with text/images/audio)
        if isinstance(content, list):
            content_summary = []
            for part in content:
                if isinstance(part, dict):
                    if part.get('type') == 'text':
                        content_summary.append(f"text:{part.get('text', '')[:50]}")
                    elif part.get('type') == 'image_url':
                        content_summary.append("[IMAGE]")
                    elif part.get('type') == 'media':
                        mime = part.get('mime_type', 'unknown')
                        content_summary.append(f"[AUDIO:{mime}]" if 'audio' in mime else f"[VIDEO:{mime}]")
                    else:
                        content_summary.append(f"[{part.get('type', 'unknown')}]")
                else:
                    content_summary.append(str(part)[:30])
            content_str = ', '.join(content_summary)
        else:
            content_str = str(content)[:100]
        logger.debug(f"[Request] [{i}] role={msg.get('role')}, content={content_str}")

    # Detect and reject LobeChat's auxiliary requests (topic naming, translation, etc.)
    # These have role=developer/system with summarization/translation prompts
    for msg in request.messages:
        role = msg.get("role", "")
        if role in ["developer", "system"]:
            content = str(msg.get("content", "")).lower()
            if any(keyword in content for keyword in ["summarizer", "summarize", "title", "translate", "translation", "compress"]):
                logger.info(f"[Request] Skipping auxiliary request (topic naming/translation)")
                # Return a simple response without invoking the agent
                response_id = f"chatcmpl-{uuid.uuid4().hex[:8]}"
                created_time = int(time_module.time())
                if request.stream:
                    async def simple_stream():
                        data = {
                            "id": response_id,
                            "object": "chat.completion.chunk",
                            "created": created_time,
                            "model": request.model,
                            "choices": [{"index": 0, "delta": {"content": "Lab Assistant"}, "finish_reason": None}]
                        }
                        yield f"data: {json.dumps(data)}\n\n"
                        final = {
                            "id": response_id,
                            "object": "chat.completion.chunk",
                            "created": created_time,
                            "model": request.model,
                            "choices": [{"index": 0, "delta": {}, "finish_reason": "stop"}]
                        }
                        yield f"data: {json.dumps(final)}\n\n"
                        yield "data: [DONE]\n\n"
                    return StreamingResponse(simple_stream(), media_type="text/event-stream")
                else:
                    return {
                        "id": response_id,
                        "object": "chat.completion",
                        "created": created_time,
                        "model": request.model,
                        "choices": [{"index": 0, "message": {"role": "assistant", "content": "Lab Assistant"}, "finish_reason": "stop"}]
                    }

    thread_id = str(uuid.uuid4())
    config = {"configurable": {"thread_id": thread_id}}

    # Select the appropriate graph based on requested model
    # The request.model from OpenAI format may differ from our internal model names
    model_name = request.model if request.model in graphs else DEFAULT_MODEL
    graph = graphs[model_name]
    logger.info(f"[OpenAI] Using model: {model_name}")

    # Convert OpenAI-format messages to LangGraph messages
    # This preserves full conversation history from the frontend
    conversation_messages = []
    for msg in request.messages:
        role = msg.get("role", "")
        content = msg.get("content", "")

        # Handle multimodal content (images, audio, video) - convert to LangChain format
        if isinstance(content, list):
            # Convert OpenAI multimodal format to LangChain format
            lc_content = []
            for part in content:
                if isinstance(part, dict):
                    if part.get("type") == "text":
                        lc_content.append({"type": "text", "text": part.get("text", "")})
                    elif part.get("type") == "image_url":
                        image_url = part.get("image_url", {})
                        url = image_url.get("url", "") if isinstance(image_url, dict) else image_url
                        lc_content.append({"type": "image_url", "image_url": {"url": url}})
                    elif part.get("type") == "media":
                        # Audio/video for Gemini (native format)
                        lc_content.append({
                            "type": "media",
                            "data": part.get("data", ""),
                            "mime_type": part.get("mime_type", "audio/webm")
                        })
            content = lc_content if lc_content else ""

        if role == "user":
            conversation_messages.append(HumanMessage(content=content))
        elif role == "assistant":
            conversation_messages.append(AIMessage(content=content))
        # Skip system/developer roles - they're handled separately

    if not conversation_messages:
        logger.error("No messages found in request")
        return {"error": "No messages found"}

    # Get the last user message for logging
    last_user_message = None
    for msg in reversed(request.messages):
        if msg["role"] == "user":
            last_user_message = msg["content"]
            break

    logger.info("=" * 60)
    # Handle multimodal user message for logging
    if isinstance(last_user_message, list):
        msg_parts = []
        for part in last_user_message:
            if isinstance(part, dict):
                if part.get('type') == 'text':
                    msg_parts.append(part.get('text', '')[:100])
                elif part.get('type') == 'image_url':
                    # Log image size for debugging
                    image_url_data = part.get('image_url', {})
                    url = image_url_data.get('url', '') if isinstance(image_url_data, dict) else str(image_url_data)
                    if url.startswith('data:'):
                        # Extract base64 size to estimate original image size
                        base64_parts = url.split(',', 1)
                        if len(base64_parts) > 1:
                            base64_size = len(base64_parts[1])
                            approx_bytes = int(base64_size * 0.75)  # base64 is ~33% larger
                            msg_parts.append(f'[IMAGE: ~{approx_bytes // 1024}KB]')
                        else:
                            msg_parts.append('[IMAGE]')
                    else:
                        msg_parts.append('[IMAGE: URL]')
                elif part.get('type') == 'media':
                    mime = part.get('mime_type', 'unknown')
                    msg_parts.append(f'[AUDIO:{mime}]' if 'audio' in mime else f'[VIDEO:{mime}]')
                else:
                    msg_parts.append(f"[{part.get('type', 'unknown')}]")
        user_msg_display = ' + '.join(msg_parts)
    else:
        user_msg_display = str(last_user_message)[:200]
    logger.info(f"USER MESSAGE: {user_msg_display}{'...' if len(str(last_user_message)) > 200 else ''}")
    logger.info(f"Thread ID: {thread_id}, Stream: {request.stream}, History: {len(conversation_messages)} messages")

    if request.stream:
        async def generate():
            global orders_context_sent
            full_response = []
            response_id = f"chatcmpl-{uuid.uuid4().hex[:8]}"  # Same ID for all chunks
            created_time = int(time_module.time())  # Unix timestamp

            # Track AI responses (for usage tracking)
            ai_responses = 0
            total_input_tokens = 0
            total_output_tokens = 0

            # Import usage tracking
            from models import increment_usage, get_usage_stats, get_daily_limit

            # Gemini pricing (per 1M tokens) - adjust based on model
            # Gemini 3 Flash Preview: $0.50 input, $3.00 output (incl. thinking tokens)
            INPUT_PRICE_PER_1M = 0.50
            OUTPUT_PRICE_PER_1M = 3.00

            try:
                # Check if this is first message in conversation (from frontend history)
                is_first_message = len(conversation_messages) <= 2  # Just system + first user msg

                # Reset tab state tracking on new conversation
                if is_first_message:
                    reset_tab_state()

                # Get current context (checks login state, fetches orders if needed)
                current_context = await get_orders_context()

                # Get browser tabs context (always at each user message)
                tabs_context = await get_browser_tabs_context()

                # Pass full conversation history to the graph
                initial_state = {"messages": conversation_messages}

                # Build full context
                context_parts = []

                # Include orders context if: first message OR we haven't sent valid orders yet
                should_include_orders = is_first_message or not orders_context_sent
                if should_include_orders and current_context:
                    context_parts.append(current_context)
                    if "SESIÃ“N NO INICIADA" in current_context:
                        logger.info("[Chat] Not logged in - sending login reminder")
                    else:
                        logger.info("[Chat] Including orders context")

                # Always include tabs context at every message
                if tabs_context:
                    context_parts.append(tabs_context)
                    logger.info("[Chat] Including browser tabs context")

                if context_parts:
                    initial_state["current_page_context"] = "\n\n".join(context_parts)

                async for event in graph.astream_events(
                    initial_state,
                    config,
                    version="v2"
                ):
                    event_type = event.get("event", "")

                    # Note: AI response counting now done in on_chat_model_end
                    if event_type == "on_chat_model_start":
                        pass  # Used for debugging only if needed

                    # Stream tool calls to show "thinking" in LobeChat
                    if event_type == "on_tool_start":
                        tool_name = event.get("name", "unknown")
                        tool_input = event.get("data", {}).get("input", {})
                        logger.info(f"TOOL CALL: {tool_name}")
                        logger.debug(f"  Input: {json.dumps(tool_input, ensure_ascii=False)[:500]}")

                        # Send tool call as a "thinking" step to frontend
                        tool_display = f"ðŸ”§ **{tool_name}**"
                        if tool_input:
                            # Show ALL parameters
                            params = []
                            for k, v in tool_input.items():
                                if isinstance(v, str):
                                    # Truncate long strings
                                    display_v = v if len(v) < 50 else v[:47] + "..."
                                    params.append(f"{k}={display_v}")
                                elif isinstance(v, list):
                                    # Show list with all items (truncate if too many)
                                    if len(v) <= 10:
                                        params.append(f"{k}={v}")
                                    else:
                                        params.append(f"{k}=[{', '.join(str(x) for x in v[:10])}... +{len(v)-10} more]")
                                elif isinstance(v, (int, float, bool)):
                                    params.append(f"{k}={v}")
                            if params:
                                tool_display += f" ({', '.join(params)})"
                        tool_display += "\n"

                        data = {
                            "id": response_id,
                            "object": "chat.completion.chunk",
                            "created": created_time,
                            "model": request.model,
                            "choices": [{
                                "index": 0,
                                "delta": {"content": tool_display},
                                "finish_reason": None
                            }]
                        }
                        yield f"data: {json.dumps(data)}\n\n"

                    elif event_type == "on_tool_end":
                        tool_name = event.get("name", "unknown")
                        tool_output = event.get("data", {}).get("output", "")
                        logger.info(f"TOOL RESULT: {tool_name}")
                        logger.debug(f"  Output: {str(tool_output)[:500]}")

                        # Send brief result indicator
                        result_display = f"âœ“ {tool_name} completado\n\n"
                        data = {
                            "id": response_id,
                            "object": "chat.completion.chunk",
                            "created": created_time,
                            "model": request.model,
                            "choices": [{
                                "index": 0,
                                "delta": {"content": result_display},
                                "finish_reason": None
                            }]
                        }
                        yield f"data: {json.dumps(data)}\n\n"

                    elif event_type == "on_chat_model_stream":
                        chunk = event["data"].get("chunk")
                        if chunk and hasattr(chunk, 'content') and chunk.content:
                            # Handle both string and list content (Gemini 3 with thinking)
                            content = chunk.content
                            if isinstance(content, list):
                                # Extract text parts only, skip thinking parts
                                text_parts = [p.get('text', '') for p in content if isinstance(p, dict) and p.get('type') == 'text']
                                content = ''.join(text_parts)
                            if content:
                                full_response.append(content)
                                data = {
                                    "id": response_id,
                                    "object": "chat.completion.chunk",
                                    "created": created_time,
                                    "model": request.model,
                                    "choices": [{
                                        "index": 0,
                                        "delta": {"content": content},
                                        "finish_reason": None
                                    }]
                                }
                                yield f"data: {json.dumps(data)}\n\n"

                    # Handle non-streaming model responses (after key rotation) and extract token usage
                    elif event_type == "on_chat_model_end":
                        output = event.get("data", {}).get("output")

                        if output:
                            # Count this as an AI response and increment usage
                            ai_responses += 1
                            increment_usage(model_name)

                            # Try to extract token usage from response
                            # Check for usage_metadata on AIMessage (it's a dict, not object)
                            # LangChain format: {'input_tokens': X, 'output_tokens': Y, 'total_tokens': Z}
                            usage = getattr(output, 'usage_metadata', None)
                            if usage and isinstance(usage, dict):
                                input_tokens = usage.get('input_tokens', 0) or usage.get('prompt_token_count', 0) or 0
                                output_tokens = usage.get('output_tokens', 0) or usage.get('candidates_token_count', 0) or 0
                                if input_tokens or output_tokens:
                                    total_input_tokens += input_tokens
                                    total_output_tokens += output_tokens
                                    # Log thinking tokens if available
                                    output_details = usage.get('output_token_details', {})
                                    thinking_tokens = output_details.get('reasoning', 0) if output_details else 0
                                    logger.info(f"[AI {ai_responses}] Tokens: in={input_tokens}, out={output_tokens}" +
                                               (f" (thinking={thinking_tokens})" if thinking_tokens else ""))

                            # Also check response_metadata for langchain (Google-specific format)
                            resp_meta = getattr(output, 'response_metadata', {}) or {}
                            if resp_meta and 'usage_metadata' in resp_meta:
                                usage_meta = resp_meta['usage_metadata']
                                # Google format: prompt_token_count, candidates_token_count
                                input_tokens = usage_meta.get('prompt_token_count', 0)
                                output_tokens = usage_meta.get('candidates_token_count', 0)
                                # Avoid double counting - only add if we didn't get from usage_metadata above
                                if (input_tokens or output_tokens) and not usage:
                                    total_input_tokens += input_tokens
                                    total_output_tokens += output_tokens
                                    logger.info(f"[AI {ai_responses}] Tokens (from response_meta): in={input_tokens}, out={output_tokens}")

                        if output and hasattr(output, 'content') and output.content:
                            content = output.content
                            if isinstance(content, list):
                                text_parts = [p.get('text', '') for p in content if isinstance(p, dict) and p.get('type') == 'text']
                                content = ''.join(text_parts)
                            # Only send if we haven't already streamed this content
                            if content and content not in ''.join(full_response):
                                full_response.append(content)
                                data = {
                                    "id": response_id,
                                    "object": "chat.completion.chunk",
                                    "created": created_time,
                                    "model": request.model,
                                    "choices": [{
                                        "index": 0,
                                        "delta": {"content": content},
                                        "finish_reason": None
                                    }]
                                }
                                yield f"data: {json.dumps(data)}\n\n"

                if full_response:
                    logger.info(f"AI RESPONSE: {''.join(full_response)[:300]}{'...' if len(''.join(full_response)) > 300 else ''}")

                # Calculate and display usage summary
                total_tokens = total_input_tokens + total_output_tokens
                input_cost = (total_input_tokens / 1_000_000) * INPUT_PRICE_PER_1M
                output_cost = (total_output_tokens / 1_000_000) * OUTPUT_PRICE_PER_1M
                total_cost = input_cost + output_cost

                # Get current usage stats for this model
                usage_stats = get_usage_stats()
                daily_limit = get_daily_limit()
                model_usage = usage_stats.get("models", {}).get(model_name, 0)

                # Send usage summary at the end
                usage_summary = f"\n\n---\nðŸ“Š **Stats**: {ai_responses} prompts ({model_usage}/{daily_limit} today)"
                if total_tokens > 0:
                    usage_summary += f" | Tokens: {total_input_tokens:,} in + {total_output_tokens:,} out"
                    usage_summary += f" | ${total_cost:.6f}"
                usage_summary += "\n"

                data = {
                    "id": response_id,
                    "object": "chat.completion.chunk",
                    "created": created_time,
                    "model": request.model,
                    "choices": [{
                        "index": 0,
                        "delta": {"content": usage_summary},
                        "finish_reason": None
                    }]
                }
                yield f"data: {json.dumps(data)}\n\n"

                logger.info(f"[Usage] AI responses: {ai_responses}, Input: {total_input_tokens}, Output: {total_output_tokens}, Cost: ${total_cost:.6f}")

                # Send final chunk with finish_reason to signal completion
                final_chunk = {
                    "id": response_id,
                    "object": "chat.completion.chunk",
                    "created": created_time,
                    "model": request.model,
                    "choices": [{
                        "index": 0,
                        "delta": {},
                        "finish_reason": "stop"
                    }]
                }
                yield f"data: {json.dumps(final_chunk)}\n\n"
                yield "data: [DONE]\n\n"

            except Exception as e:
                logger.error(f"Stream error: {str(e)}", exc_info=True)
                yield f"data: {json.dumps({'error': str(e)})}\n\n"

        return StreamingResponse(
            generate(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",  # Disable nginx buffering
            }
        )

    else:
        try:
            # Check if this is first message in conversation (from frontend history)
            is_first_message = len(conversation_messages) <= 2  # Just system + first user msg

            # Reset tab state tracking on new conversation
            if is_first_message:
                reset_tab_state()

            # Get current context (checks login state, fetches orders if needed)
            current_context = await get_orders_context()

            # Get browser tabs context (always at each user message)
            tabs_context = await get_browser_tabs_context()

            # Pass full conversation history to the graph
            initial_state = {"messages": conversation_messages}

            # Build full context
            context_parts = []

            # Include orders context if: first message OR we haven't sent valid orders yet
            should_include_orders = is_first_message or not orders_context_sent
            if should_include_orders and current_context:
                context_parts.append(current_context)
                if "SESIÃ“N NO INICIADA" in current_context:
                    logger.info("[Chat] Not logged in - sending login reminder")
                else:
                    logger.info("[Chat] Including orders context")

            # Always include tabs context at every message
            if tabs_context:
                context_parts.append(tabs_context)
                logger.info("[Chat] Including browser tabs context")

            if context_parts:
                initial_state["current_page_context"] = "\n\n".join(context_parts)

            logger.info("Invoking LangGraph agent...")
            result = await graph.ainvoke(initial_state, config)

            # Log all messages for debugging
            for i, msg in enumerate(result["messages"]):
                msg_type = type(msg).__name__
                content = msg.content if hasattr(msg, 'content') else str(msg)
                if hasattr(msg, 'tool_calls') and msg.tool_calls:
                    logger.info(f"  [{i}] {msg_type}: {len(msg.tool_calls)} tool calls")
                    for tc in msg.tool_calls:
                        logger.info(f"      -> {tc.get('name', 'unknown')}: {json.dumps(tc.get('args', {}), ensure_ascii=False)[:200]}")
                else:
                    logger.info(f"  [{i}] {msg_type}: {content[:150]}{'...' if len(content) > 150 else ''}")

            last_msg = result["messages"][-1]
            response_text = last_msg.content if hasattr(last_msg, 'content') else str(last_msg)

            logger.info(f"AI RESPONSE: {response_text[:300]}{'...' if len(response_text) > 300 else ''}")
            logger.info("=" * 60)

            return {
                "id": f"chatcmpl-{uuid.uuid4().hex[:8]}",
                "object": "chat.completion",
                "model": request.model,
                "choices": [{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": response_text
                    },
                    "finish_reason": "stop"
                }]
            }
        except Exception as e:
            logger.error(f"Error invoking agent: {str(e)}", exc_info=True)
            return {
                "id": f"chatcmpl-{uuid.uuid4().hex[:8]}",
                "object": "chat.completion",
                "model": request.model,
                "choices": [{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": f"Error: {str(e)}"
                    },
                    "finish_reason": "stop"
                }]
            }


# ============================================================
# MAIN
# ============================================================

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "server:app",
        host=settings.host,
        port=settings.port,
        reload=settings.debug
    )
