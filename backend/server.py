"""
FastAPI server with LangGraph integration.

DOCUMENTATION:
- LangGraph + FastAPI: https://langchain-ai.github.io/langgraph/how-tos/deploy-self-hosted/
- Streaming: https://langchain-ai.github.io/langgraph/concepts/streaming/
- Checkpointing: https://langchain-ai.github.io/langgraph/concepts/persistence/

ENDPOINTS:
- POST /api/chat: Send message, get response
- GET /api/chat/{thread_id}/history: Get conversation history
- GET /api/browser/screenshot: Get current browser state
- GET /api/health: Health check

NO APPROVAL ENDPOINTS NEEDED - Website's Save button is the human-in-the-loop.
"""
import os
import sys
import asyncio
import base64
import uuid
import json
import logging
import re
import csv
from datetime import datetime
from typing import Optional, List
from contextlib import asynccontextmanager
from pathlib import Path

# Configure logging - use INFO level to reduce noise
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

# Exams list file (generated by scripts/process_tarifas.py)
EXAMS_FILE = Path(__file__).parent / "config" / "lista_de_examenes.csv"
EXAMS_LAST_UPDATE_FILE = Path(__file__).parent / "config" / "exams_last_update.txt"

# Reduce noise from various loggers
logging.getLogger("asyncio").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)

# Set up Windows event loop policy if on Windows
if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from langchain_core.messages import HumanMessage, AIMessage

# LangGraph imports
from langgraph.checkpoint.memory import MemorySaver
# For production with SQLite:
# from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver

# Local imports
from graph.agent import create_lab_agent, compile_agent
from graph.tools import set_browser, close_all_tabs, get_active_tabs, _get_browser_tabs_impl, reset_tab_state, ALL_TOOLS
from browser_manager import BrowserManager
from extractors import EXTRACT_ORDENES_JS
from config import settings
from prompts import SYSTEM_PROMPT, load_prompts, save_prompts, reload_prompts, get_default_prompts
from stream_adapter import StreamAdapter


def load_exams_from_csv() -> List[dict]:
    """
    Load available exams from CSV file.
    Returns list of exams with codigo, nombre, and prices.
    """
    if not EXAMS_FILE.exists():
        logger.warning(f"[Exams] File not found: {EXAMS_FILE}")
        return []

    exams = []
    with open(EXAMS_FILE, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            exams.append({
                'codigo': row.get('codigo', ''),
                'nombre': row.get('nombre', ''),
                'precio': float(row.get('precio_particular', 0) or 0),
                'precio_desc': float(row.get('precio_descuento', 0) or 0),
            })

    logger.info(f"[Exams] Loaded {len(exams)} exams from {EXAMS_FILE.name}")
    return exams


# Cache exams at module level (loaded once at startup)
_cached_exams: List[dict] = []

# Cache for preprocessing results (prevents duplicate processing)
# Key: hash of image data, Value: (result, timestamp)
import hashlib
import time as time_module
_preprocessing_cache: dict = {}
_PREPROCESSING_CACHE_TTL = 60  # 60 seconds TTL


def _get_preprocessing_cache_key(images: List) -> str:
    """Generate cache key from image data."""
    hasher = hashlib.md5()
    for img in images:
        data = img.data if hasattr(img, 'data') else str(img)
        # Normalize: strip data: prefix if present
        if isinstance(data, str) and data.startswith("data:"):
            data = data.split(",", 1)[1] if "," in data else data
        # Hash first 2000 chars of actual base64 data for reliable matching
        chunk = data[:2000] if isinstance(data, str) else str(data)[:2000]
        hasher.update(chunk.encode())
    return hasher.hexdigest()


def _get_cached_preprocessing(cache_key: str):
    """Get cached preprocessing result if still valid."""
    if cache_key in _preprocessing_cache:
        result, timestamp = _preprocessing_cache[cache_key]
        if time_module.time() - timestamp < _PREPROCESSING_CACHE_TTL:
            return result
        else:
            del _preprocessing_cache[cache_key]
    return None


def _set_preprocessing_cache(cache_key: str, result: dict):
    """Cache preprocessing result."""
    _preprocessing_cache[cache_key] = (result, time_module.time())
    # Clean old entries (keep max 20)
    if len(_preprocessing_cache) > 20:
        oldest_key = min(_preprocessing_cache.keys(), key=lambda k: _preprocessing_cache[k][1])
        del _preprocessing_cache[oldest_key]


def get_available_exams_context() -> str:
    """Generate context string for available exams."""
    global _cached_exams

    if not _cached_exams:
        _cached_exams = load_exams_from_csv()

    if not _cached_exams:
        return ""

    lines = ["# Exámenes Disponibles (Código - Nombre - Precio)"]
    for exam in _cached_exams:
        precio = f"${exam['precio']:.2f}" if exam['precio'] else "N/A"
        lines.append(f"- {exam['codigo']}: {exam['nombre'][:45]} ({precio})")

    lines.append("")
    lines.append("*Para cotización: create_new_order(cedula=\"\", exams=[\"CODIGO1\",...])*")

    return "\n".join(lines)


# ============================================================
# PYDANTIC MODELS
# ============================================================

class ChatRequest(BaseModel):
    message: str
    thread_id: Optional[str] = None


class ChatResponse(BaseModel):
    status: str  # "complete", "error"
    message: str
    thread_id: str
    iterations: Optional[int] = None


# ============================================================
# GLOBAL STATE
# ============================================================

browser: Optional[BrowserManager] = None
graphs: dict = {}  # Dictionary of graphs, keyed by model name
checkpointer = None
initial_orders_context: str = ""  # Store initial orders for context
orders_context_sent: bool = False  # Track if we've sent valid orders context
orders_context_timestamp: float = 0  # When orders were last fetched
ORDERS_FRESHNESS_SECONDS = 120  # Orders are fresh for 2 minutes

# Available models (must match frontend MODEL_CONFIGS)
AVAILABLE_MODELS = [
    "gemini-3-flash-preview",
    "gemini-flash-latest",
]
DEFAULT_MODEL = "gemini-3-flash-preview"


# ============================================================
# HELPER FUNCTIONS
# ============================================================

def is_logged_in() -> bool:
    """Check if the browser is logged in (not on login page)."""
    if browser and browser.page:
        return "/login" not in browser.page.url
    return False


async def get_orders_context(force_refresh: bool = False) -> str:
    """
    Get orders context, checking login state first.
    Returns empty string with login message if not logged in.
    Returns orders table if logged in and orders found.

    Args:
        force_refresh: If True, re-fetch orders even if already cached.
                       Use this for new chats to get fresh data.
    """
    global browser, initial_orders_context, orders_context_sent, orders_context_timestamp

    if not is_logged_in():
        logger.info("[Context] User not logged in - browser is on login page")
        return "⚠️ SESIÓN NO INICIADA: El navegador está en la página de login. Por favor, inicia sesión en el navegador para que pueda acceder a las órdenes del laboratorio."

    # Check if orders are stale (older than ORDERS_FRESHNESS_SECONDS)
    import time
    current_time = time.time()
    is_stale = (current_time - orders_context_timestamp) > ORDERS_FRESHNESS_SECONDS

    # Force refresh for new chats, stale data, or if we haven't sent valid orders yet
    if force_refresh or is_stale or not orders_context_sent or not initial_orders_context:
        if is_stale and orders_context_sent:
            logger.info(f"[Context] Orders are stale ({int(current_time - orders_context_timestamp)}s old), refreshing...")
        else:
            logger.info("[Context] Extracting orders context...")
        initial_orders_context = await extract_initial_context()
        orders_context_timestamp = current_time
        if initial_orders_context and "Órdenes Recientes" in initial_orders_context:
            orders_context_sent = True
            # Count orders: 7 pipes per row, subtract 2 for header/separator
            order_count = max(0, (initial_orders_context.count('|') // 7) - 2)
            logger.info(f"[Context] Extracted {order_count} orders")

    return initial_orders_context


def get_orders_freshness() -> dict:
    """Get current orders context freshness status."""
    import time
    current_time = time.time()
    age_seconds = current_time - orders_context_timestamp if orders_context_timestamp else 0
    is_fresh = age_seconds < ORDERS_FRESHNESS_SECONDS

    return {
        "has_orders": bool(initial_orders_context and "Órdenes Recientes" in initial_orders_context),
        "age_seconds": int(age_seconds),
        "is_fresh": is_fresh,
        "freshness_threshold": ORDERS_FRESHNESS_SECONDS
    }


async def get_browser_tabs_context() -> str:
    """
    Get browser tabs context with state tracking.

    - Shows all tabs with IDs, patient names, and enumeration for duplicates
    - For NEW tabs: shows full state (exams, fields, etc.)
    - For KNOWN tabs: shows only what CHANGED since last message
    - Marks which tab is active
    """
    try:
        tabs_info = await _get_browser_tabs_impl()

        if not tabs_info.get("tabs"):
            return "# Pestañas del Navegador\nNo hay pestañas abiertas. Para editar resultados, primero usa get_order_results(order_nums) para abrir la pestaña de resultados."

        lines = ["# Pestañas del Navegador"]

        # Track if there are any results tabs
        has_results_tabs = any(t.get("type") == "resultados" for t in tabs_info.get("tabs", []))

        type_display = {
            "ordenes_list": "Lista de Órdenes",
            "nueva_orden": "Nueva Orden",
            "orden_edit": "Editar Orden",
            "resultados": "Resultados",
            "login": "Login",
            "unknown": "Otra"
        }

        for idx, tab in enumerate(tabs_info.get("tabs", [])):
            tab_type = tab.get("type", "unknown")
            is_active = tab.get("active", False)
            is_new = tab.get("is_new", False)
            tab_id = tab.get("id")
            paciente = tab.get("paciente")
            instance = tab.get("instance")  # For duplicate enumeration
            state = tab.get("state")  # Full state for new tabs
            changes = tab.get("changes")  # Only changes for known tabs

            # Build tab header - always include tab_index
            marker = "→ " if is_active else "  "
            tab_line = f"{marker}[tab_index={idx}] {type_display.get(tab_type, tab_type)}"

            # Add ID for saved orders
            if tab_id:
                if tab_type == "resultados":
                    tab_line += f" (order_num={tab_id})"
                elif tab_type == "orden_edit":
                    tab_line += f" (order_id={tab_id})"

            # Add instance number for duplicates
            if instance:
                tab_line += f" #{instance}"

            # Add patient name
            if paciente:
                tab_line += f" - {paciente[:25]}"

            # Add NEW marker for unsaved orders
            if is_new:
                tab_line += " [NUEVA - sin guardar]"

            lines.append(tab_line)

            # For new tabs, show full state
            if is_new and state:
                if tab_type == "resultados":
                    lines.append(f"    Exámenes: {state.get('examenes_count', 0)}")
                    # Show field values (limited)
                    field_values = state.get("field_values", {})
                    filled = [(k, v) for k, v in field_values.items() if v]
                    if filled:
                        lines.append(f"    Campos con valor: {len(filled)}")
                elif tab_type in ["orden_edit", "nueva_orden"]:
                    exams = state.get("exams", [])
                    if exams:
                        lines.append(f"    Exámenes: {', '.join(exams[:8])}")
                    if state.get("total"):
                        lines.append(f"    Total: {state.get('total')}")

            # For known tabs, show only changes
            elif changes:
                lines.append("    **Cambios detectados:**")
                if "field_values" in changes:
                    # Show which fields changed
                    for field_key, new_value in list(changes["field_values"].items())[:5]:
                        if new_value:
                            lines.append(f"    - {field_key}: → {new_value}")
                if "exams" in changes:
                    lines.append(f"    - Exámenes: {', '.join(changes['exams'][:5])}")
                if "total" in changes:
                    lines.append(f"    - Total: {changes['total']}")

        # Add guidance if no results tabs are open
        if not has_results_tabs:
            lines.append("")
            lines.append("⚠️ No hay pestañas de resultados abiertas. Para editar resultados, usa get_order_results(order_nums) primero.")

        return "\n".join(lines)

    except Exception as e:
        logger.warning(f"[Context] Could not get browser tabs context: {e}")
        return ""


# ============================================================
# LIFESPAN
# ============================================================

# Track if an auto-update is already in progress
_orders_auto_update_in_progress = False


async def _trigger_orders_auto_update():
    """Background task to auto-update orders list."""
    global _orders_auto_update_in_progress, browser

    if _orders_auto_update_in_progress:
        logger.info("[Context] Orders auto-update already in progress, skipping...")
        return

    _orders_auto_update_in_progress = True
    try:
        logger.info("[Context] Starting automatic orders list update...")

        if not browser:
            logger.warning("[Context] Browser not initialized, cannot auto-update orders")
            return

        await browser.ensure_page()

        # Create a new tab for this operation
        new_page = await browser.context.new_page()

        try:
            # Navigate to ordenes report page
            logger.info("[Orders-Auto] Navigating to ordenes report page...")
            await new_page.goto("https://laboratoriofranz.orion-labs.com/informes/ordenes", timeout=30000)
            await new_page.wait_for_load_state("domcontentloaded", timeout=10000)
            await asyncio.sleep(1)

            # Calculate 1 year ago from today
            from datetime import timedelta
            one_year_ago = (datetime.now() - timedelta(days=365)).strftime("%Y-%m-%d")

            # Set fecha-desde
            logger.info(f"[Orders-Auto] Setting date range from {one_year_ago}...")
            fecha_desde = new_page.locator("#fecha-desde")
            await fecha_desde.click()
            await new_page.keyboard.press("Control+a")
            await fecha_desde.type(one_year_ago, delay=50)
            await asyncio.sleep(0.3)

            # Press Escape to close the datepicker popup before clicking elsewhere
            await new_page.keyboard.press("Escape")
            await asyncio.sleep(0.3)

            # Set up download handler
            download_dir = Path(__file__).parent / "downloads"
            download_dir.mkdir(exist_ok=True)

            # Click "Generar informe" dropdown
            logger.info("[Orders-Auto] Clicking 'Generar informe' dropdown...")
            dropdown_btn = new_page.locator("button.dropdown-toggle", has_text="Generar informe")
            await dropdown_btn.click()
            await asyncio.sleep(0.5)

            # Click "Excel" option and wait for download
            logger.info("[Orders-Auto] Clicking 'Excel' and waiting for download...")
            async with new_page.expect_download(timeout=120000) as download_info:
                excel_btn = new_page.locator("#generar-informe-ordenes-excel")
                await excel_btn.click()

            download = await download_info.value

            # Save the downloaded file
            xlsx_path = download_dir / download.suggested_filename
            await download.save_as(xlsx_path)
            logger.info(f"[Orders-Auto] Downloaded: {xlsx_path}")

            # Process the XLSX
            logger.info("[Orders-Auto] Processing XLSX...")
            from scripts.process_ordenes import process_ordenes
            process_ordenes(str(xlsx_path))

            # Reload the orders cache
            from orders_cache import reload_orders_cache, set_orders_last_update
            orders = reload_orders_cache()

            # Save the update timestamp
            timestamp = datetime.now().isoformat()
            set_orders_last_update(timestamp)

            # Clean up
            xlsx_path.unlink(missing_ok=True)

            logger.info(f"[Orders-Auto] Update complete! {len(orders)} orders loaded.")

        finally:
            await new_page.close()

    except Exception as e:
        logger.error(f"[Orders-Auto] Auto-update failed: {e}")
        import traceback
        traceback.print_exc()
    finally:
        _orders_auto_update_in_progress = False


async def extract_initial_context() -> str:
    """
    Extract orders list and available exams from the page for AI context.

    Only fetches page 1 (20 orders), checks overlap with cached orders,
    and triggers update if no overlap exists or cache is empty.
    """
    global browser
    lines = []
    needs_update = False

    try:
        from orders_cache import (
            get_cached_orders, check_overlap, merge_orders,
            reload_orders_cache
        )

        # Fetch orders from page 1 only
        page1_url = "https://laboratoriofranz.orion-labs.com/ordenes?page=1"
        logger.info("[Context] Fetching orders page 1...")
        await browser.page.goto(page1_url, timeout=30000)
        # Wait for table rows instead of fixed 2s delay
        try:
            await browser.page.wait_for_selector('table tbody tr, .order-row', timeout=5000)
        except Exception:
            await browser.page.wait_for_timeout(500)
        ordenes_page1 = await browser.page.evaluate(EXTRACT_ORDENES_JS) or []

        logger.info(f"[Context] Extracted {len(ordenes_page1)} orders from page 1")

        # Get page 1 order numbers
        page1_order_nums = {o.get('num', '') for o in ordenes_page1 if o.get('num')}

        # Check overlap with cached orders
        cached_orders = get_cached_orders()
        has_overlap, overlap_count = check_overlap(page1_order_nums)

        # Determine if we need to trigger an update
        if not cached_orders:
            needs_update = True
            logger.warning("[Context] No orders cache found - triggering auto-update...")
        elif not has_overlap:
            needs_update = True
            logger.warning("[Context] No overlap between page 1 and cached orders - triggering auto-update...")

        # Trigger background update if needed (silently, no AI notification to save tokens)
        if needs_update and not _orders_auto_update_in_progress:
            asyncio.create_task(_trigger_orders_auto_update())

        # Merge page 1 with cached orders (20 most recent)
        if cached_orders:
            all_ordenes = merge_orders(ordenes_page1, cached_orders, max_orders=20)
            logger.info(f"[Context] Merged {len(ordenes_page1)} page 1 orders with cache, showing {len(all_ordenes)}")
        else:
            all_ordenes = ordenes_page1[:20]
            logger.info(f"[Context] No cached orders, showing {len(all_ordenes)} from page 1")

        if all_ordenes:
            lines.append("# Órdenes Recientes (20 más recientes)")
            lines.append("| # | Orden | Fecha | Paciente | Cédula | Estado |")
            lines.append("|---|-------|-------|----------|--------|--------|")
            for i, o in enumerate(all_ordenes[:20]):
                paciente = (o.get('paciente', '') or o.get('patient_name', '') or '')[:30]
                lines.append(f"| {i+1} | {o.get('num','')} | {o.get('fecha','')} | {paciente} | {o.get('cedula','')} | {o.get('estado','')} |")

    except Exception as e:
        logger.warning(f"Could not extract orders context: {e}")

    # Add available exams from CSV file (faster than scraping, complete list)
    exams_context = get_available_exams_context()
    if exams_context:
        lines.append("")
        lines.append(exams_context)

    return "\n".join(lines) if lines else ""


@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Application lifespan - initialize browser and LangGraph.

    For fast startup, we skip initial orders extraction - it will be done
    lazily on first chat message instead.
    """
    global browser, graph, checkpointer, initial_orders_context

    print("Starting Lab Assistant with LangGraph...")

    # Create data directory if it doesn't exist
    data_dir = Path(__file__).parent / "data"
    data_dir.mkdir(exist_ok=True)

    # Initialize browser (required before accepting requests)
    browser = BrowserManager(user_data_dir=settings.browser_data_dir)
    await browser.start(headless=settings.headless, browser=settings.browser_channel)
    await browser.navigate(settings.target_url)
    set_browser(browser)

    # Check if we ended up at the welcome page (session redirect) and navigate to orders
    if browser.page and "/bienvenida" in browser.page.url:
        logger.info("[Startup] Redirected to welcome page, navigating to orders...")
        await browser.navigate("https://laboratoriofranz.orion-labs.com/ordenes?page=1")

    # Check login status - if on login page, auto-click the "Ingresar" button
    if browser.page and "/login" in browser.page.url:
        logger.info("[Startup] On login page - attempting auto-login by clicking 'Ingresar' button...")
        try:
            # Click the Ingresar button to login with saved session
            ingresar_button = browser.page.locator("button:has-text('Ingresar')")
            if await ingresar_button.count() > 0:
                await ingresar_button.click()
                logger.info("[Startup] Clicked 'Ingresar' button, waiting for navigation...")
                # Wait for navigation to complete
                await browser.page.wait_for_load_state("networkidle", timeout=10000)
                logger.info(f"[Startup] After login click, now at: {browser.page.url}")

                # If we're still on login page, the session might have expired
                if "/login" in browser.page.url:
                    logger.warning("[Startup] Still on login page - session may have expired. User needs to login manually.")
                    initial_orders_context = "⚠️ SESIÓN NO INICIADA: El navegador está en la página de login. Por favor, inicia sesión manualmente."
                else:
                    # Navigate to orders page
                    if "/bienvenida" in browser.page.url:
                        logger.info("[Startup] Redirected to welcome page, navigating to orders...")
                        await browser.navigate("https://laboratoriofranz.orion-labs.com/ordenes?page=1")
                    # Orders context will be extracted on-demand by get_orders_context()
                    initial_orders_context = ""
                    logger.info("[Startup] Login successful, orders will be fetched on-demand")
            else:
                logger.warning("[Startup] 'Ingresar' button not found on login page")
                initial_orders_context = "⚠️ SESIÓN NO INICIADA: No se encontró el botón 'Ingresar'."
        except Exception as e:
            logger.error(f"[Startup] Auto-login failed: {e}")
            initial_orders_context = f"⚠️ SESIÓN NO INICIADA: Error al intentar login automático: {e}"
    else:
        # Already logged in - orders context will be extracted on-demand by get_orders_context()
        initial_orders_context = ""
        logger.info("[Startup] Already logged in, orders will be fetched on-demand")

    # Initialize checkpointer for conversation persistence
    # Using MemorySaver for development (in-memory, not persistent across restarts)
    # For production, use AsyncSqliteSaver or PostgresSaver
    checkpointer = MemorySaver()

    # Build and compile a graph for each available model
    for model_name in AVAILABLE_MODELS:
        logger.info(f"[Startup] Creating graph for model: {model_name}")
        builder = create_lab_agent(browser, model_name=model_name)
        graphs[model_name] = compile_agent(builder, checkpointer)
        logger.info(f"[Startup] Graph for {model_name} ready")

    print(f"Lab Assistant ready! Browser at: {browser.page.url}")
    print(f"Available models: {', '.join(AVAILABLE_MODELS)}")

    yield

    # Cleanup
    print("Shutting down...")
    close_all_tabs()
    await browser.stop()


# ============================================================
# FASTAPI APP
# ============================================================

app = FastAPI(
    title="Lab Assistant API",
    description="LangGraph-powered lab assistant for clinical laboratory data entry",
    version="2.0.0",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ============================================================
# ENDPOINTS
# ============================================================

@app.get("/api/health")
async def health_check():
    """Health check endpoint."""
    return {
        "status": "ok",
        "browser_url": browser.page.url if browser and browser.page else None,
        "graphs_ready": list(graphs.keys()),
        "default_model": DEFAULT_MODEL
    }


@app.post("/api/chat", response_model=ChatResponse)
async def chat(
    thread_id: str = Form(default=None),
    message: str = Form(...),
    files: List[UploadFile] = File(default=[])
):
    """
    Send a message to the agent and get a response.

    Supports multi-modal input (text, images, audio).
    The agent will execute tools as needed and return when done.

    Args:
        thread_id: Conversation thread ID (generated if not provided)
        message: User message text
        files: Optional image or audio files

    Returns:
        Agent's response with thread_id for continuation
    """
    # Generate thread_id if not provided
    if not thread_id:
        thread_id = str(uuid.uuid4())

    config = {"configurable": {"thread_id": thread_id}}

    # Build message content (multi-modal support)
    content = []

    if message:
        content.append({"type": "text", "text": message})

    # Process uploaded files
    for file in files:
        file_content = await file.read()
        encoded = base64.b64encode(file_content).decode('utf-8')

        if file.content_type and file.content_type.startswith("image/"):
            # Image for vision models
            content.append({
                "type": "image_url",
                "image_url": {"url": f"data:{file.content_type};base64,{encoded}"}
            })
        elif file.content_type and file.content_type.startswith("audio/"):
            # Audio for Gemini (native audio support)
            content.append({
                "type": "media",
                "data": encoded,
                "mime_type": file.content_type
            })

    # Create human message
    if len(content) == 1 and content[0]["type"] == "text":
        human_msg = HumanMessage(content=message)
    else:
        human_msg = HumanMessage(content=content)

    try:
        # Use default model graph (this endpoint doesn't accept model parameter)
        graph = graphs.get(DEFAULT_MODEL)
        if not graph:
            raise ValueError(f"Default graph '{DEFAULT_MODEL}' not found")

        # Invoke graph - it will loop internally until done
        result = await graph.ainvoke(
            {"messages": [human_msg]},
            config
        )

        # Get the final response
        last_msg = result["messages"][-1]
        response_text = last_msg.content if hasattr(last_msg, 'content') else str(last_msg)

        # Count iterations (based on tool messages)
        tool_messages = [m for m in result["messages"] if hasattr(m, 'type') and getattr(m, 'type', None) == 'tool']
        iterations = len(tool_messages)

        return ChatResponse(
            status="complete",
            message=response_text,
            thread_id=thread_id,
            iterations=iterations
        )

    except Exception as e:
        import traceback
        traceback.print_exc()
        return ChatResponse(
            status="error",
            message=f"Error: {str(e)}",
            thread_id=thread_id
        )


@app.post("/api/chat/stream")
async def chat_stream(request: ChatRequest):
    """
    Stream chat responses in real-time using Server-Sent Events.

    DOCUMENTATION:
    - astream_events: https://langchain-ai.github.io/langgraph/concepts/streaming/

    This streams:
    - Token-by-token LLM output
    - Tool execution notifications
    - Final completion
    """
    thread_id = request.thread_id or str(uuid.uuid4())
    config = {"configurable": {"thread_id": thread_id}}

    # Use default model graph (this endpoint doesn't accept model parameter)
    graph = graphs.get(DEFAULT_MODEL)

    async def generate():
        try:
            async for event in graph.astream_events(
                {"messages": [HumanMessage(content=request.message)]},
                config,
                version="v2"
            ):
                event_type = event.get("event", "")

                # Stream LLM tokens
                if event_type == "on_chat_model_stream":
                    chunk = event["data"].get("chunk")
                    if chunk and hasattr(chunk, 'content') and chunk.content:
                        yield f"data: {json.dumps({'type': 'token', 'content': chunk.content})}\n\n"

                # Notify tool execution
                elif event_type == "on_tool_start":
                    tool_name = event["name"]
                    yield f"data: {json.dumps({'type': 'tool_start', 'tool': tool_name})}\n\n"

                elif event_type == "on_tool_end":
                    tool_name = event["name"]
                    yield f"data: {json.dumps({'type': 'tool_end', 'tool': tool_name})}\n\n"

            yield f"data: {json.dumps({'type': 'done', 'thread_id': thread_id})}\n\n"

        except Exception as e:
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={"X-Thread-ID": thread_id}
    )


@app.get("/api/chat/{thread_id}/history")
async def get_history(thread_id: str):
    """
    Get conversation history for a thread.

    Returns list of messages with role and content.
    """
    config = {"configurable": {"thread_id": thread_id}}

    # Use default graph for state retrieval (all graphs share the same checkpointer)
    graph = graphs.get(DEFAULT_MODEL)

    try:
        state = await graph.aget_state(config)
        messages = state.values.get("messages", [])

        return [
            {
                "role": "user" if isinstance(m, HumanMessage) else "assistant",
                "content": m.content if hasattr(m, 'content') else str(m),
                "type": getattr(m, 'type', 'unknown')
            }
            for m in messages
            if not (hasattr(m, 'type') and getattr(m, 'type', None) == 'tool')  # Skip tool messages
        ]
    except Exception as e:
        return []


@app.get("/api/browser/screenshot")
async def get_screenshot():
    """Get current browser screenshot as base64."""
    if browser and browser.page:
        try:
            screenshot_bytes = await browser.page.screenshot(type='png')
            encoded = base64.b64encode(screenshot_bytes).decode('utf-8')
            return {"screenshot": f"data:image/png;base64,{encoded}"}
        except Exception as e:
            raise HTTPException(500, f"Screenshot failed: {str(e)}")
    raise HTTPException(503, "Browser not available")


@app.get("/api/browser/tabs")
async def get_tabs():
    """Get list of open browser tabs."""
    active_tabs = get_active_tabs()
    return {
        "tabs": list(active_tabs.keys()),
        "count": len(active_tabs)
    }


@app.get("/api/browser/tabs/detailed")
async def get_tabs_detailed():
    """Get detailed info about all browser tabs including state."""
    try:
        tabs_info = await _get_browser_tabs_impl()
        return tabs_info
    except Exception as e:
        logger.error(f"Failed to get detailed tabs: {e}")
        return {"error": str(e), "tabs": []}


@app.post("/api/browser/close-tabs")
async def close_tabs():
    """Close all open browser tabs (cleanup)."""
    close_all_tabs()
    return {"status": "ok", "message": "All tabs closed"}


# ============================================================
# MANUAL TOOL EXECUTION ENDPOINT
# ============================================================

class ManualToolRequest(BaseModel):
    tool: str
    args: dict

@app.post("/api/tools/execute")
async def execute_tool(request: ManualToolRequest):
    """
    Execute a tool manually (for UI-based editing).
    Supports: edit_results, edit_order_exams, create_new_order
    """
    from graph.tools import (
        edit_results,
        edit_order_exams,
        create_new_order,
    )

    tool_name = request.tool
    args = request.args

    logger.info(f"[Manual Tool] Executing {tool_name} with args: {args}")

    try:
        if tool_name == "edit_results":
            # edit_results expects data=[{orden, e, f, v}]
            data = args.get("data", [])
            result = await edit_results.ainvoke({"data": data})
            return {"success": True, "message": f"Editados {len(data)} campos", "result": result}

        elif tool_name == "edit_order_exams":
            # edit_order_exams now supports order_id, tab_index, cedula, add, remove
            order_id = args.get("order_id")
            tab_index = args.get("tab_index")
            cedula = args.get("cedula")
            add_exams = args.get("add", [])
            remove_exams = args.get("remove", [])
            result = await edit_order_exams.ainvoke({
                "order_id": order_id,
                "tab_index": tab_index,
                "cedula": cedula,
                "add": add_exams,
                "remove": remove_exams
            })
            changes = []
            if cedula:
                changes.append(f"cédula: {cedula}")
            if add_exams:
                changes.append(f"agregados: {', '.join(add_exams)}")
            if remove_exams:
                changes.append(f"removidos: {', '.join(remove_exams)}")
            return {"success": True, "message": f"Cambios: {' | '.join(changes) if changes else 'ninguno'}", "result": result}

        elif tool_name == "create_new_order":
            cedula = args.get("cedula", "")
            exams = args.get("exams", [])
            result = await create_new_order.ainvoke({"cedula": cedula, "exams": exams})
            return {"success": True, "message": f"Orden creada con {len(exams)} exámenes", "result": result}

        else:
            return {"success": False, "error": f"Tool '{tool_name}' not supported for manual execution"}

    except Exception as e:
        logger.error(f"[Manual Tool] Error: {e}")
        return {"success": False, "error": str(e)}


@app.get("/api/exams")
async def get_exams():
    """Get list of available exams from CSV (cached)."""
    global _cached_exams
    if not _cached_exams:
        _cached_exams = load_exams_from_csv()
    return {"exams": [{"codigo": e["codigo"], "nombre": e["nombre"]} for e in _cached_exams]}


@app.get("/api/exams/last-update")
async def get_exams_last_update():
    """Get the timestamp of the last exam list update."""
    if EXAMS_LAST_UPDATE_FILE.exists():
        timestamp = EXAMS_LAST_UPDATE_FILE.read_text().strip()
        return {"lastUpdate": timestamp}
    return {"lastUpdate": None}


@app.post("/api/exams/update")
async def update_exams_list():
    """
    Download the tarifas CSV from the website and update the exams list.

    Uses Playwright to:
    1. Navigate to the tarifas page
    2. Check "Incluir exámenes con valor cero"
    3. Click "Generar informe" -> "Excel"
    4. Wait for CSV download
    5. Process with process_tarifas.py
    """
    global _cached_exams, browser

    try:
        logger.info("[Exams] Starting exam list update...")

        # Use the existing browser
        if not browser:
            raise HTTPException(status_code=503, detail="Browser not initialized")

        page = await browser.ensure_page()

        # Create a new tab for this operation
        new_page = await browser.context.new_page()

        try:
            # Navigate to tarifas page
            logger.info("[Exams] Navigating to tarifas page...")
            await new_page.goto("https://laboratoriofranz.orion-labs.com/informes/tarifas", timeout=30000)
            await new_page.wait_for_load_state("domcontentloaded", timeout=10000)
            await asyncio.sleep(1)  # Wait for page to settle

            # Check the "Incluir exámenes con valor cero" checkbox
            logger.info("[Exams] Checking 'Incluir exámenes con valor cero' checkbox...")
            checkbox = new_page.locator("#examenes-valor-cero")
            if not await checkbox.is_checked():
                await checkbox.check()
                await asyncio.sleep(0.5)

            # Set up download handler
            download_dir = Path(__file__).parent / "downloads"
            download_dir.mkdir(exist_ok=True)

            # Click "Generar informe" dropdown
            logger.info("[Exams] Clicking 'Generar informe' dropdown...")
            dropdown_btn = new_page.locator("button.dropdown-toggle", has_text="Generar informe")
            await dropdown_btn.click()
            await asyncio.sleep(0.5)

            # Click "Excel" option and wait for download
            logger.info("[Exams] Clicking 'Excel' and waiting for download...")
            async with new_page.expect_download(timeout=60000) as download_info:
                excel_btn = new_page.locator(".dropdown-item", has_text="Excel")
                await excel_btn.click()

            download = await download_info.value

            # Save the downloaded file
            csv_path = download_dir / download.suggested_filename
            await download.save_as(csv_path)
            logger.info(f"[Exams] Downloaded: {csv_path}")

            # Process the CSV with process_tarifas.py
            logger.info("[Exams] Processing CSV...")
            from scripts.process_tarifas import process_tarifas
            process_tarifas(str(csv_path))

            # Reload the exams cache
            _cached_exams = load_exams_from_csv()

            # Save the update timestamp
            timestamp = datetime.now().isoformat()
            EXAMS_LAST_UPDATE_FILE.parent.mkdir(parents=True, exist_ok=True)
            EXAMS_LAST_UPDATE_FILE.write_text(timestamp)

            # Clean up downloaded file
            csv_path.unlink(missing_ok=True)

            logger.info(f"[Exams] Update complete! {len(_cached_exams)} exams loaded.")

            return {
                "success": True,
                "message": f"Lista de exámenes actualizada: {len(_cached_exams)} exámenes",
                "examCount": len(_cached_exams),
                "lastUpdate": timestamp
            }

        finally:
            # Close the tab we opened
            await new_page.close()

    except Exception as e:
        logger.error(f"[Exams] Update failed: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


# ============================================================
# ORDERS LIST UPDATE ENDPOINTS
# ============================================================

ORDERS_LAST_UPDATE_FILE = Path(__file__).parent / "config" / "ordenes_last_update.txt"


@app.get("/api/orders/last-update")
async def get_orders_last_update():
    """Get the timestamp of the last orders list update."""
    if ORDERS_LAST_UPDATE_FILE.exists():
        timestamp = ORDERS_LAST_UPDATE_FILE.read_text().strip()
        return {"lastUpdate": timestamp}
    return {"lastUpdate": None}


@app.post("/api/orders/prefetch")
async def prefetch_orders_context():
    """
    Prefetch orders context in background.
    Call this when receiving an image to have orders ready when user sends prompt.

    Returns freshness info and triggers background fetch if needed.
    """
    # Get current freshness status
    freshness = get_orders_freshness()

    # If orders are stale or missing, fetch them
    if not freshness["is_fresh"] or not freshness["has_orders"]:
        logger.info("[Prefetch] Orders are stale/missing, prefetching...")
        await get_orders_context(force_refresh=True)
        freshness = get_orders_freshness()
        logger.info(f"[Prefetch] Orders prefetched, now {freshness['age_seconds']}s old")

    return {
        "success": True,
        "freshness": freshness
    }


@app.get("/api/orders/freshness")
async def get_orders_freshness_status():
    """Get current orders context freshness status without fetching."""
    return get_orders_freshness()


@app.post("/api/orders/update")
async def update_orders_list():
    """
    Download the orders XLSX from the website and update the orders list.

    Uses Playwright to:
    1. Navigate to the ordenes report page
    2. Set date range: desde=1 year ago, hasta=today
    3. Click "Generar informe" -> "Excel"
    4. Wait for XLSX download
    5. Process with scripts/process_ordenes.py
    """
    global browser

    try:
        logger.info("[Orders] Starting orders list update...")

        # Use the existing browser
        if not browser:
            raise HTTPException(status_code=503, detail="Browser not initialized")

        await browser.ensure_page()

        # Create a new tab for this operation
        new_page = await browser.context.new_page()

        try:
            # Navigate to ordenes report page
            logger.info("[Orders] Navigating to ordenes report page...")
            await new_page.goto("https://laboratoriofranz.orion-labs.com/informes/ordenes", timeout=30000)
            await new_page.wait_for_load_state("domcontentloaded", timeout=10000)
            await asyncio.sleep(1)  # Wait for page to settle

            # Calculate 1 year ago from today (max range is 13 months)
            from datetime import timedelta
            one_year_ago = (datetime.now() - timedelta(days=365)).strftime("%Y-%m-%d")

            # Set fecha-desde to 1 year ago
            logger.info(f"[Orders] Setting date range from {one_year_ago}...")
            fecha_desde = new_page.locator("#fecha-desde")
            await fecha_desde.click()
            await new_page.keyboard.press("Control+a")
            await fecha_desde.type(one_year_ago, delay=50)
            await asyncio.sleep(0.3)

            # Click somewhere else to close datepicker and apply the date
            await new_page.locator("label[for='fecha-desde']").click()
            await asyncio.sleep(0.5)

            # Set up download handler
            download_dir = Path(__file__).parent / "downloads"
            download_dir.mkdir(exist_ok=True)

            # Click "Generar informe" dropdown
            logger.info("[Orders] Clicking 'Generar informe' dropdown...")
            dropdown_btn = new_page.locator("button.dropdown-toggle", has_text="Generar informe")
            await dropdown_btn.click()
            await asyncio.sleep(0.5)

            # Click "Excel" option and wait for download
            logger.info("[Orders] Clicking 'Excel' and waiting for download...")
            async with new_page.expect_download(timeout=120000) as download_info:
                excel_btn = new_page.locator("#generar-informe-ordenes-excel")
                await excel_btn.click()

            download = await download_info.value

            # Save the downloaded file
            xlsx_path = download_dir / download.suggested_filename
            await download.save_as(xlsx_path)
            logger.info(f"[Orders] Downloaded: {xlsx_path}")

            # Process the XLSX with process_ordenes.py
            logger.info("[Orders] Processing XLSX...")
            from scripts.process_ordenes import process_ordenes
            process_ordenes(str(xlsx_path))

            # Reload the orders cache
            from orders_cache import reload_orders_cache, set_orders_last_update
            orders = reload_orders_cache()

            # Save the update timestamp
            timestamp = datetime.now().isoformat()
            set_orders_last_update(timestamp)

            # Clean up downloaded file
            xlsx_path.unlink(missing_ok=True)

            logger.info(f"[Orders] Update complete! {len(orders)} orders loaded.")

            return {
                "success": True,
                "message": f"Lista de órdenes actualizada: {len(orders)} órdenes",
                "orderCount": len(orders),
                "lastUpdate": timestamp
            }

        finally:
            # Close the tab we opened
            await new_page.close()

    except Exception as e:
        logger.error(f"[Orders] Update failed: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/usage")
async def get_usage():
    """Get current usage stats for all models."""
    from models import get_usage_stats, get_daily_limit, get_num_api_keys

    stats = get_usage_stats()
    daily_limit = get_daily_limit()
    num_keys = get_num_api_keys()

    return {
        "date": stats.get("date"),
        "models": stats.get("models", {}),
        "dailyLimit": daily_limit,
        "numApiKeys": num_keys,
        "freePerKey": 20  # Google AI Studio free tier limit per key
    }


# ============================================================
# PROMPTS CONFIGURATION ENDPOINTS
# ============================================================

@app.get("/api/prompts")
async def get_prompts():
    """Get current prompts configuration."""
    prompts = load_prompts()
    return {
        "prompts": prompts,
        "sections": [
            {
                "key": "system_prompt",
                "label": "Instrucciones del Sistema",
                "description": "Instrucciones principales para la IA sobre cómo comportarse"
            },
            {
                "key": "abbreviations",
                "label": "Abreviaturas",
                "description": "Abreviaturas comunes usadas en el laboratorio"
            },
            {
                "key": "image_interpretation",
                "label": "Interpretación de Imágenes",
                "description": "Instrucciones para interpretar imágenes de resultados"
            },
            {
                "key": "welcome_message",
                "label": "Mensaje de Bienvenida",
                "description": "Mensaje que se muestra al iniciar una nueva conversación"
            }
        ]
    }


class PromptsUpdateRequest(BaseModel):
    prompts: dict


@app.post("/api/prompts")
async def update_prompts(request: PromptsUpdateRequest):
    """Update prompts configuration."""
    success = save_prompts(request.prompts)
    if success:
        # Reload to update cache
        reload_prompts()
        return {"success": True, "message": "Prompts actualizados correctamente"}
    else:
        raise HTTPException(status_code=500, detail="Error al guardar los prompts")


@app.get("/api/prompts/defaults")
async def get_default_prompts_endpoint():
    """Get default prompts configuration (for restore functionality)."""
    return {"prompts": get_default_prompts()}


# ============================================================
# IMAGE ROTATION DETECTION ENDPOINT
# ============================================================

class ImageRotationRequest(BaseModel):
    image: str  # base64 data (with or without data URL prefix)
    mimeType: str = "image/jpeg"


@app.post("/api/detect-rotation")
async def detect_image_rotation(request: ImageRotationRequest):
    """
    Detect if an image needs rotation correction using Gemini vision.
    Uses the existing key rotation system for rate limit handling.
    """
    import time
    from models import get_chat_model

    start_time = time.time()

    try:
        # Get Gemini model with key rotation - use gemini-3-flash-preview for best accuracy
        # Use minimal thinking for faster rotation detection while maintaining quality
        model = get_chat_model(provider="gemini", model_name="gemini-3-flash-preview", thinking_level="minimal")

        # Extract base64 data (remove data URL prefix if present)
        base64_data = request.image
        if base64_data.startswith("data:"):
            base64_data = base64_data.split(",", 1)[1]

        # Create message with image
        message = HumanMessage(content=[
            {
                "type": "text",
                "text": "Look at this image. If the image appears rotated or upside down, respond with ONLY a number: 90, 180, or 270 (degrees clockwise to fix it). If the image is correctly oriented, respond with ONLY: 0"
            },
            {
                "type": "image_url",
                "image_url": {"url": f"data:{request.mimeType};base64,{base64_data}"}
            }
        ])

        # Call model
        result = await model.ainvoke([message])

        # Track this API call against daily usage (rotation uses same API keys)
        from models import increment_usage
        increment_usage("gemini-3-flash-preview")

        # Handle Gemini 3 thinking response format
        # When include_thoughts=True, content is a list: [{'type': 'thinking', ...}, {'type': 'text', 'text': '...'}]
        content = result.content
        if isinstance(content, list):
            # Extract text from the response blocks
            response_text = ""
            for block in content:
                if isinstance(block, dict) and block.get("type") == "text":
                    response_text = block.get("text", "")
                    break
            logger.debug(f"[Rotation] Gemini 3 response blocks: {len(content)}, extracted text: {response_text[:50] if response_text else 'empty'}")
        else:
            response_text = content.strip() if content else ""

        # Parse rotation from response
        rotation = 0
        import re
        match = re.search(r'\b(0|90|180|270)\b', response_text)
        if match:
            rotation = int(match.group(1))

        elapsed_ms = int((time.time() - start_time) * 1000)
        logger.info(f"[Rotation] Gemini detected {rotation}° in {elapsed_ms}ms")

        return {
            "rotation": rotation,
            "detected": rotation != 0,
            "provider": "gemini",
            "timing": elapsed_ms,
            "raw": response_text
        }

    except Exception as e:
        elapsed_ms = int((time.time() - start_time) * 1000)
        import traceback
        logger.error(f"[Rotation] Gemini error ({elapsed_ms}ms): {e}\n{traceback.format_exc()}")
        return {
            "rotation": 0,
            "detected": False,
            "provider": "gemini",
            "error": str(e),
            "timing": elapsed_ms
        }


# ============================================================
# DOCUMENT SEGMENTATION ENDPOINT (SAM3)
# ============================================================

class DocumentSegmentationRequest(BaseModel):
    image: str  # base64 data (with or without data URL prefix)
    mimeType: str = "image/jpeg"
    prompt: str = "document"  # Text prompt for segmentation (e.g., "document", "notebook", "paper")
    padding: int = 10  # Padding around detected region in pixels


# Global SAM3 predictor instance (lazy loaded)
_sam3_predictor = None


def _download_sam3_model(target_path: Path) -> bool:
    """Download SAM3 model from Hugging Face using HF_TOKEN from environment."""
    hf_token = os.environ.get("HF_TOKEN") or os.environ.get("HUGGINGFACE_TOKEN")

    if not hf_token:
        logger.warning("[SAM3] No HF_TOKEN found in environment. Cannot auto-download model.")
        return False

    try:
        from huggingface_hub import hf_hub_download

        logger.info("[SAM3] Downloading model from HuggingFace (facebook/sam3)...")

        # Create target directory if needed
        target_path.parent.mkdir(parents=True, exist_ok=True)

        # Download the model
        downloaded_path = hf_hub_download(
            repo_id="facebook/sam3",
            filename="sam3.pt",
            token=hf_token,
            local_dir=target_path.parent,
            local_dir_use_symlinks=False
        )

        logger.info(f"[SAM3] Model downloaded to: {downloaded_path}")
        return True

    except ImportError:
        logger.error("[SAM3] huggingface_hub not installed. Run: pip install huggingface_hub")
        return False
    except Exception as e:
        logger.error(f"[SAM3] Download failed: {e}")
        logger.error("[SAM3] Make sure you have accepted the license at: https://huggingface.co/facebook/sam3")
        return False


def _get_sam3_predictor():
    """
    Lazy load SAM3 predictor to avoid startup delay.
    Auto-downloads model from HuggingFace if HF_TOKEN is set in environment.
    """
    global _sam3_predictor
    if _sam3_predictor is not None:
        return _sam3_predictor

    try:
        from ultralytics.models.sam import SAM3SemanticPredictor

        # Check if SAM3 weights exist
        sam3_path = Path(__file__).parent / "models" / "sam3.pt"
        if not sam3_path.exists():
            # Try current directory
            sam3_path = Path("sam3.pt")

        if not sam3_path.exists():
            # Try to auto-download
            target_path = Path(__file__).parent / "models" / "sam3.pt"
            if _download_sam3_model(target_path):
                sam3_path = target_path
            else:
                logger.warning("[SAM3] Model not found. Add HF_TOKEN to .env for auto-download, or download manually from: https://huggingface.co/facebook/sam3")
                return None

        logger.info(f"[SAM3] Loading model from {sam3_path}...")
        overrides = dict(
            conf=0.25,
            task="segment",
            mode="predict",
            model=str(sam3_path),
            half=True,
        )
        _sam3_predictor = SAM3SemanticPredictor(overrides=overrides)
        logger.info("[SAM3] Model loaded successfully")
        return _sam3_predictor

    except ImportError as e:
        logger.error(f"[SAM3] Failed to import ultralytics: {e}")
        return None
    except Exception as e:
        logger.error(f"[SAM3] Failed to load model: {e}")
        return None


@app.post("/api/segment-document")
async def segment_document(request: DocumentSegmentationRequest):
    """
    Segment and crop a document from an image using SAM3.
    Uses text prompts to identify the document region.

    If SAM3 is not available, falls back to Gemini vision to detect
    the document region and returns bounding box coordinates.
    """
    import time
    from io import BytesIO

    start_time = time.time()

    # Extract base64 data (remove data URL prefix if present)
    base64_data = request.image
    if base64_data.startswith("data:"):
        base64_data = base64_data.split(",", 1)[1]

    try:
        # Try SAM3 first
        predictor = _get_sam3_predictor()

        if predictor is not None:
            return await _segment_with_sam3(
                predictor, base64_data, request.mimeType, request.prompt, request.padding, start_time
            )

        # Fallback to Gemini vision for document detection
        logger.info("[SAM3] Model not available, falling back to Gemini vision")
        return await _segment_with_gemini(
            base64_data, request.mimeType, request.prompt, request.padding, start_time
        )

    except Exception as e:
        elapsed_ms = int((time.time() - start_time) * 1000)
        import traceback
        logger.error(f"[Segment] Error ({elapsed_ms}ms): {e}\n{traceback.format_exc()}")
        return {
            "segmented": False,
            "error": str(e),
            "timing": elapsed_ms
        }


async def _segment_with_sam3(predictor, base64_data: str, mime_type: str, prompt: str, padding: int, start_time: float):
    """Segment document using SAM3 with text prompt."""
    import numpy as np
    from PIL import Image
    from io import BytesIO
    import time

    try:
        # Decode image
        image_bytes = base64.b64decode(base64_data)
        image = Image.open(BytesIO(image_bytes))
        if image.mode != 'RGB':
            image = image.convert('RGB')

        # Convert to numpy for SAM3
        image_np = np.array(image)

        # Set image and run segmentation with text prompt
        predictor.set_image(image_np)
        results = predictor(text=[prompt])

        if not results or len(results) == 0:
            elapsed_ms = int((time.time() - start_time) * 1000)
            logger.info(f"[SAM3] No '{prompt}' detected ({elapsed_ms}ms)")
            return {
                "segmented": False,
                "reason": f"No '{prompt}' detected in image",
                "provider": "sam3",
                "timing": elapsed_ms
            }

        # Get the first result (best match)
        result = results[0]

        # Extract bounding box
        if result.boxes is not None and len(result.boxes) > 0:
            # Get the largest bounding box (most likely the document)
            boxes = result.boxes.xyxy.cpu().numpy()
            areas = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])
            best_idx = np.argmax(areas)
            x1, y1, x2, y2 = boxes[best_idx].astype(int)

            # Add padding
            h, w = image_np.shape[:2]
            x1 = max(0, x1 - padding)
            y1 = max(0, y1 - padding)
            x2 = min(w, x2 + padding)
            y2 = min(h, y2 + padding)

            # Crop image
            cropped = image.crop((x1, y1, x2, y2))

            # Encode back to base64
            buffer = BytesIO()
            img_format = 'JPEG' if 'jpeg' in mime_type.lower() or 'jpg' in mime_type.lower() else 'PNG'
            cropped.save(buffer, format=img_format, quality=95)
            cropped_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')

            elapsed_ms = int((time.time() - start_time) * 1000)
            logger.info(f"[SAM3] Segmented '{prompt}': ({x1},{y1}) to ({x2},{y2}) in {elapsed_ms}ms")

            return {
                "segmented": True,
                "croppedImage": cropped_base64,
                "mimeType": mime_type,
                "boundingBox": {"x1": int(x1), "y1": int(y1), "x2": int(x2), "y2": int(y2)},
                "originalSize": {"width": w, "height": h},
                "croppedSize": {"width": x2 - x1, "height": y2 - y1},
                "prompt": prompt,
                "provider": "sam3",
                "timing": elapsed_ms
            }

        elapsed_ms = int((time.time() - start_time) * 1000)
        return {
            "segmented": False,
            "reason": "No bounding box found",
            "provider": "sam3",
            "timing": elapsed_ms
        }

    except Exception as e:
        import traceback
        elapsed_ms = int((time.time() - start_time) * 1000)
        logger.error(f"[SAM3] Segmentation error: {e}\n{traceback.format_exc()}")
        raise


async def _segment_with_gemini(base64_data: str, mime_type: str, prompt: str, padding: int, start_time: float):
    """Fallback: Use Gemini vision to detect document region."""
    import time
    from models import get_chat_model, increment_usage
    from PIL import Image
    from io import BytesIO

    try:
        # Get Gemini model
        model = get_chat_model(provider="gemini", model_name="gemini-3-flash-preview", thinking_level="minimal")

        # Create message asking for bounding box
        message = HumanMessage(content=[
            {
                "type": "text",
                "text": f"""Look at this image and find the {prompt}.
If you can see a {prompt} in the image, respond with ONLY the bounding box coordinates in this exact format:
BOX: x1,y1,x2,y2

Where x1,y1 is the top-left corner and x2,y2 is the bottom-right corner, as percentages of image dimensions (0-100).
Example: BOX: 10,15,90,85

If no {prompt} is visible or the image is already zoomed in on a {prompt}, respond with:
NONE"""
            },
            {
                "type": "image_url",
                "image_url": {"url": f"data:{mime_type};base64,{base64_data}"}
            }
        ])

        # Call model
        result = await model.ainvoke([message])
        increment_usage("gemini-3-flash-preview")

        # Parse response
        content = result.content
        if isinstance(content, list):
            response_text = ""
            for block in content:
                if isinstance(block, dict) and block.get("type") == "text":
                    response_text = block.get("text", "")
                    break
        else:
            response_text = content.strip() if content else ""

        # Check if no document detected
        if "NONE" in response_text.upper():
            elapsed_ms = int((time.time() - start_time) * 1000)
            logger.info(f"[Segment/Gemini] No cropping needed ({elapsed_ms}ms)")
            return {
                "segmented": False,
                "reason": "Image already focused on document or no document detected",
                "provider": "gemini",
                "timing": elapsed_ms
            }

        # Parse bounding box
        box_match = re.search(r'BOX:\s*(\d+(?:\.\d+)?)\s*,\s*(\d+(?:\.\d+)?)\s*,\s*(\d+(?:\.\d+)?)\s*,\s*(\d+(?:\.\d+)?)', response_text, re.IGNORECASE)

        if box_match:
            # Parse percentages
            x1_pct, y1_pct, x2_pct, y2_pct = [float(x) for x in box_match.groups()]

            # Decode image to get dimensions
            image_bytes = base64.b64decode(base64_data)
            image = Image.open(BytesIO(image_bytes))
            if image.mode != 'RGB':
                image = image.convert('RGB')
            w, h = image.size

            # Convert percentages to pixels
            x1 = int(w * x1_pct / 100)
            y1 = int(h * y1_pct / 100)
            x2 = int(w * x2_pct / 100)
            y2 = int(h * y2_pct / 100)

            # Add padding
            x1 = max(0, x1 - padding)
            y1 = max(0, y1 - padding)
            x2 = min(w, x2 + padding)
            y2 = min(h, y2 + padding)

            # Crop image
            cropped = image.crop((x1, y1, x2, y2))

            # Encode back to base64
            buffer = BytesIO()
            img_format = 'JPEG' if 'jpeg' in mime_type.lower() or 'jpg' in mime_type.lower() else 'PNG'
            cropped.save(buffer, format=img_format, quality=95)
            cropped_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')

            elapsed_ms = int((time.time() - start_time) * 1000)
            logger.info(f"[Segment/Gemini] Cropped '{prompt}': ({x1},{y1}) to ({x2},{y2}) in {elapsed_ms}ms")

            return {
                "segmented": True,
                "croppedImage": cropped_base64,
                "mimeType": mime_type,
                "boundingBox": {"x1": x1, "y1": y1, "x2": x2, "y2": y2},
                "originalSize": {"width": w, "height": h},
                "croppedSize": {"width": x2 - x1, "height": y2 - y1},
                "prompt": prompt,
                "provider": "gemini",
                "timing": elapsed_ms
            }

        elapsed_ms = int((time.time() - start_time) * 1000)
        logger.info(f"[Segment/Gemini] Could not parse response: {response_text[:100]} ({elapsed_ms}ms)")
        return {
            "segmented": False,
            "reason": "Could not detect document region",
            "provider": "gemini",
            "raw": response_text,
            "timing": elapsed_ms
        }

    except Exception as e:
        import traceback
        elapsed_ms = int((time.time() - start_time) * 1000)
        logger.error(f"[Segment/Gemini] Error: {e}\n{traceback.format_exc()}")
        raise


# ============================================================
# IMAGE PREPROCESSING PIPELINE (YOLOE + Rotations + Labels)
# ============================================================

class ImageData(BaseModel):
    """Single image data for preprocessing."""
    data: str  # base64 data (with or without data URL prefix)
    mimeType: str = "image/jpeg"
    name: Optional[str] = None


class ImagePreprocessRequest(BaseModel):
    """Request for preprocessing images with YOLOE detection and rotations."""
    images: List[ImageData]
    preprocessingModel: str = "gemini-flash-latest"
    thinkingLevel: str = "off"


class LabelInfo(BaseModel):
    """Metadata for a labeled image variant."""
    imageIndex: int
    label: str
    type: str  # "rotation" or "crop"
    rotation: Optional[int] = None


class CropInfo(BaseModel):
    """Crop information for an image."""
    imageIndex: int
    hasCrop: bool
    boundingBox: Optional[dict] = None
    confidence: Optional[float] = None
    className: Optional[str] = None


class ImageVariant(BaseModel):
    """A single labeled image variant."""
    data: str  # base64 JPEG
    mimeType: str = "image/jpeg"
    label: str
    imageIndex: int
    type: str  # "rotation" or "crop"
    rotation: Optional[int] = None


class TimingInfo(BaseModel):
    """Timing information for preprocessing."""
    totalMs: int
    yoloeMs: Optional[int] = None
    labelingMs: Optional[int] = None


class ImagePreprocessResponse(BaseModel):
    """Response from preprocessing endpoint."""
    variants: List[ImageVariant]
    labels: List[LabelInfo]
    crops: List[CropInfo]
    timing: TimingInfo


class PreprocessingChoice(BaseModel):
    """AI's choice for an image."""
    imageIndex: int = Field(description="The image number N from the labels")
    rotation: int = Field(description="Rotation angle: 0, 90, 180, or 270 degrees")
    useCrop: bool = Field(description="True if cropped version improves readability")


class PreprocessingChoicesResponse(BaseModel):
    """Structured response from preprocessing AI."""
    choices: List[PreprocessingChoice] = Field(description="One choice per original image")


class SelectPreprocessingRequest(BaseModel):
    """Request for AI to select preprocessing options."""
    variants: List[ImageVariant]
    labels: List[LabelInfo]
    preprocessingModel: str = "gemini-flash-latest"
    thinkingLevel: str = "off"


class SelectPreprocessingResponse(BaseModel):
    """Response from AI selection."""
    choices: List[PreprocessingChoice]
    timing: int


class ApplyPreprocessingRequest(BaseModel):
    """Request to apply AI's preprocessing choices."""
    images: List[ImageData]
    choices: List[PreprocessingChoice]
    crops: List[CropInfo]


class ProcessedImage(BaseModel):
    """A processed image after applying choices."""
    data: str  # base64 JPEG
    mimeType: str = "image/jpeg"
    imageIndex: int
    rotation: int
    cropped: bool


# Static prefix for prompt caching - DO NOT MODIFY without considering cache invalidation
PREPROCESSING_PROMPT_PREFIX = """You are an image preprocessing assistant. Prepare images for another AI.

For each image N, you see:
- ROTATION OPTIONS: "N: 0°", "N: 90°", "N: 180°", "N: 270°" - pick which shows text upright
- CROP COMPARISON (if available): side-by-side showing "N: Crop=False" (left) vs "N: Crop=True" (right)

ROTATION: Choose the angle where text/content appears right-side up and readable.

CROP DECISION: If a crop comparison is shown, compare left vs right:
- Crop=True if the right side zooms in for EASIER READING while keeping all important content
- Crop=False if the right side cuts off important text, data, or context"""


def get_preprocessing_prompt(num_images: int, image_indices: list) -> str:
    """Generate preprocessing prompt with static prefix + dynamic suffix."""

    # Dynamic suffix with specific image count (changes per request)
    if num_images == 1:
        suffix = "\n\nThis request has 1 image (index: 1)."
    else:
        suffix = f"\n\nThis request has {num_images} images (indices: {image_indices})."

    return PREPROCESSING_PROMPT_PREFIX + suffix


# YOLOE service instance (lazy loaded)
_yoloe_service = None


def _get_yoloe_service():
    """Get or create YOLOE service instance."""
    global _yoloe_service
    if _yoloe_service is None:
        try:
            from services.yoloe_service import YOLOEService
            _yoloe_service = YOLOEService.get_instance()
        except ImportError as e:
            logger.warning(f"[YOLOE] Service not available: {e}")
            return None
    return _yoloe_service


# Image labeling service instance
_labeling_service = None


def _get_labeling_service():
    """Get or create image labeling service instance."""
    global _labeling_service
    if _labeling_service is None:
        from services.image_labeling import ImageLabelingService
        _labeling_service = ImageLabelingService()
    return _labeling_service


@app.post("/api/preprocess-images", response_model=ImagePreprocessResponse)
async def preprocess_images(request: ImagePreprocessRequest):
    """
    Generate labeled rotation variants + crop for each input image.

    For each image:
    1. Resize to max 1080p if needed
    2. Create 4 rotations (0°, 90°, 180°, 270°) with labels
    3. Try to detect document with YOLOE and create cropped variant if found

    Returns separate images (not composite) for Gemini multi-image support.
    """
    import time
    from PIL import Image
    from io import BytesIO
    from services.image_labeling import base64_to_image, image_to_base64

    # Check cache first to prevent duplicate processing
    cache_key = _get_preprocessing_cache_key(request.images)
    cached_result = _get_cached_preprocessing(cache_key)
    if cached_result is not None:
        logger.info(f"[Preprocess] Cache hit for {len(request.images)} images")
        return cached_result

    start_time = time.time()
    yoloe_time = 0
    labeling_time = 0

    labeler = _get_labeling_service()
    yoloe = _get_yoloe_service()

    variants = []
    labels = []
    crops = []

    for idx, img_data in enumerate(request.images):
        image_num = idx + 1

        # Decode image
        base64_data = img_data.data
        if base64_data.startswith("data:"):
            base64_data = base64_data.split(",", 1)[1]

        image = base64_to_image(base64_data)

        # Create labeled rotation variants
        label_start = time.time()
        rotation_variants = labeler.create_labeled_variants(image, image_num, max_size=1080)
        labeling_time += (time.time() - label_start) * 1000

        for labeled_img, metadata in rotation_variants:
            variant_data = image_to_base64(labeled_img)
            variants.append(ImageVariant(
                data=variant_data,
                mimeType="image/jpeg",
                label=metadata["label"],
                imageIndex=metadata["imageIndex"],
                type=metadata["type"],
                rotation=metadata.get("rotation")
            ))
            labels.append(LabelInfo(**metadata))

        # Try to detect and crop document with YOLOE
        crop_info = CropInfo(imageIndex=image_num, hasCrop=False)

        if yoloe is not None:
            try:
                yoloe_start = time.time()
                # Prepare image (EXIF + resize)
                prepared_image = labeler.prepare_image(image, max_size=1080)
                cropped, detection = yoloe.detect_and_crop(
                    prepared_image,
                    confidence_threshold=0.3,
                    padding=10
                )
                yoloe_time += (time.time() - yoloe_start) * 1000

                if cropped is not None and detection is not None:
                    # Create side-by-side comparison: original vs cropped
                    label_start = time.time()
                    comparison_img, crop_metadata = labeler.create_crop_comparison(
                        prepared_image, cropped, image_num, max_size=1080
                    )
                    labeling_time += (time.time() - label_start) * 1000

                    crop_data = image_to_base64(comparison_img)
                    variants.append(ImageVariant(
                        data=crop_data,
                        mimeType="image/jpeg",
                        label=crop_metadata["label"],
                        imageIndex=crop_metadata["imageIndex"],
                        type=crop_metadata["type"]
                    ))
                    labels.append(LabelInfo(**crop_metadata))

                    crop_info = CropInfo(
                        imageIndex=image_num,
                        hasCrop=True,
                        boundingBox=detection["boundingBox"],
                        confidence=detection["confidence"],
                        className=detection["className"]
                    )

            except Exception as e:
                logger.warning(f"[Preprocess] YOLOE detection failed for image {image_num}: {e}")

        crops.append(crop_info)

    total_ms = int((time.time() - start_time) * 1000)

    logger.info(f"[Preprocess] Generated {len(variants)} variants for {len(request.images)} images in {total_ms}ms (YOLOE: {int(yoloe_time)}ms, labeling: {int(labeling_time)}ms)")

    result = ImagePreprocessResponse(
        variants=variants,
        labels=labels,
        crops=crops,
        timing=TimingInfo(
            totalMs=total_ms,
            yoloeMs=int(yoloe_time) if yoloe_time > 0 else None,
            labelingMs=int(labeling_time)
        )
    )

    # Cache result to prevent duplicate processing
    _set_preprocessing_cache(cache_key, result)

    return result


@app.post("/api/select-preprocessing", response_model=SelectPreprocessingResponse)
async def select_preprocessing(request: SelectPreprocessingRequest):
    """
    Send all image variants to AI for rotation + crop selection.

    Uses configurable model and thinking level.
    Returns AI's choices for each input image.
    """
    import time
    from models import get_chat_model, increment_usage

    start_time = time.time()

    # Create/clear debug folder for AI input images
    import shutil
    debug_dir = Path(__file__).parent / "debug_images"
    if debug_dir.exists():
        shutil.rmtree(debug_dir)
    debug_dir.mkdir(exist_ok=True)

    # Build multi-image message for Gemini
    content = []

    # Add all image variants and save to debug folder
    for i, variant in enumerate(request.variants):
        base64_data = variant.data
        if base64_data.startswith("data:"):
            base64_data = base64_data.split(",", 1)[1]

        content.append({
            "type": "image_url",
            "image_url": {"url": f"data:{variant.mimeType};base64,{base64_data}"}
        })

        # Save variant to debug folder (what AI sees)
        try:
            label = request.labels[i].label if i < len(request.labels) else f"variant_{i}"
            # Clean label for filename: "1: 0°" -> "1_0deg"
            safe_label = label.replace(": ", "_").replace("°", "deg").replace(" ", "_")
            debug_filename = f"input_{safe_label}.jpg"

            import base64 as b64
            img_bytes = b64.b64decode(base64_data)
            (debug_dir / debug_filename).write_bytes(img_bytes)
        except Exception as e:
            logger.debug(f"[Select] Failed to save debug image: {e}")

    # Get unique image indices to tell AI exactly how many images to process
    image_indices = sorted(set(label.imageIndex for label in request.labels))
    num_images = len(image_indices)

    # Format labels for context
    label_lines = ["Available image variants:"]
    for label in request.labels:
        label_lines.append(f"- {label.label} (type: {label.type})")
    label_context = "\n".join(label_lines)

    # Generate dynamic prompt based on image count
    system_prompt = get_preprocessing_prompt(num_images, image_indices)

    # Add text prompt
    content.append({
        "type": "text",
        "text": f"{system_prompt}\n\n{label_context}"
    })

    # Get model with appropriate thinking parameter
    # Gemini 3.x uses thinking_level, Gemini 2.5 uses thinking_budget
    is_gemini_3 = 'gemini-3' in request.preprocessingModel

    thinking_kwargs = {}
    if is_gemini_3:
        # Gemini 3 uses thinkingLevel: minimal, low, medium, high
        level_map = {'minimal': 'minimal', 'low': 'low', 'medium': 'medium', 'high': 'high'}
        thinking_kwargs['thinking_level'] = level_map.get(request.thinkingLevel, 'low')
    else:
        # Gemini 2.5 uses thinkingBudget: 0 (off), -1 (dynamic)
        budget_map = {'off': 0, 'dynamic': -1}
        thinking_kwargs['thinking_budget'] = budget_map.get(request.thinkingLevel, 0)

    model = get_chat_model(
        provider="gemini",
        model_name=request.preprocessingModel,
        **thinking_kwargs
    )

    # Get valid image indices from labels
    valid_image_indices = sorted(set(label.imageIndex for label in request.labels))
    logger.info(f"[Select] Valid image indices: {valid_image_indices}, Labels: {[l.label for l in request.labels]}")

    # Use structured output to get validated response
    structured_model = model.with_structured_output(PreprocessingChoicesResponse)

    # Call model with structured output
    message = HumanMessage(content=content)
    choices = []

    try:
        result: PreprocessingChoicesResponse = await structured_model.ainvoke([message])
        logger.info(f"[Select] AI structured response: {result.choices}")

        # Validate and deduplicate choices
        seen_indices = set()
        for choice in result.choices:
            if choice.imageIndex in seen_indices:
                logger.debug(f"[Select] Skipping duplicate choice for imageIndex {choice.imageIndex}")
                continue
            if choice.imageIndex not in valid_image_indices:
                logger.warning(f"[Select] AI returned invalid imageIndex {choice.imageIndex}, valid are {valid_image_indices}")
                continue
            seen_indices.add(choice.imageIndex)
            choices.append(choice)
            logger.info(f"[Select] Image {choice.imageIndex}: rotation={choice.rotation}°, useCrop={choice.useCrop}")

    except Exception as e:
        logger.warning(f"[Select] Structured output failed: {e}")

    # Track usage
    increment_usage(request.preprocessingModel)

    # If no valid choices, create defaults
    if not choices:
        for img_idx in valid_image_indices:
            choices.append(PreprocessingChoice(
                imageIndex=img_idx,
                rotation=0,
                useCrop=False
            ))
            logger.info(f"[Select] Image {img_idx}: using defaults (rotation=0°, useCrop=False)")
        logger.warning(f"[Select] Using default choices for {len(valid_image_indices)} images")

    total_ms = int((time.time() - start_time) * 1000)
    logger.info(f"[Select] AI selected preprocessing for {len(choices)} images in {total_ms}ms")

    return SelectPreprocessingResponse(choices=choices, timing=total_ms)


@app.post("/api/apply-preprocessing")
async def apply_preprocessing(request: ApplyPreprocessingRequest):
    """
    Apply AI's rotation/crop choices to original images.

    Returns processed images ready for final AI consumption.
    Also saves processed images to debug folder for inspection.
    """
    import time
    from PIL import Image, ImageOps
    from io import BytesIO
    from services.image_labeling import base64_to_image, image_to_base64

    start_time = time.time()
    labeler = _get_labeling_service()

    # Debug folder (already created by select_preprocessing with input images)
    debug_dir = Path(__file__).parent / "debug_images"
    debug_dir.mkdir(exist_ok=True)

    processed = []

    for choice in request.choices:
        # Find original image
        original_idx = choice.imageIndex - 1
        if original_idx < 0 or original_idx >= len(request.images):
            logger.warning(f"[Apply] Invalid image index: {choice.imageIndex}")
            continue

        img_data = request.images[original_idx]

        # Decode image
        base64_data = img_data.data
        if base64_data.startswith("data:"):
            base64_data = base64_data.split(",", 1)[1]

        image = base64_to_image(base64_data)

        # Prepare image (EXIF + resize)
        image = labeler.prepare_image(image, max_size=1080)

        # Apply crop FIRST (before rotation) since bbox was detected on original orientation
        was_cropped = False
        if choice.useCrop:
            for crop in request.crops:
                if crop.imageIndex == choice.imageIndex and crop.hasCrop and crop.boundingBox:
                    bbox = crop.boundingBox
                    w, h = image.size
                    padding = 10
                    x1 = max(0, int(bbox['x1']) - padding)
                    y1 = max(0, int(bbox['y1']) - padding)
                    x2 = min(w, int(bbox['x2']) + padding)
                    y2 = min(h, int(bbox['y2']) + padding)
                    image = image.crop((x1, y1, x2, y2))
                    was_cropped = True
                    logger.info(f"[Apply] Cropped image {choice.imageIndex} to bbox ({x1},{y1})-({x2},{y2})")
                    break

        # Apply rotation AFTER crop
        if choice.rotation != 0:
            # Negative = clockwise rotation
            image = image.rotate(-choice.rotation, expand=True)

        # Convert to base64
        result_data = image_to_base64(image)

        # Save to debug folder (output after AI's choice applied)
        debug_filename = f"output_{choice.imageIndex}_rot{choice.rotation}deg_crop{was_cropped}.jpg"
        try:
            image.save(debug_dir / debug_filename, "JPEG", quality=90)
            logger.info(f"[Apply] Saved debug image: {debug_filename}")
        except Exception as e:
            logger.warning(f"[Apply] Failed to save debug image: {e}")

        processed.append(ProcessedImage(
            data=result_data,
            mimeType="image/jpeg",
            imageIndex=choice.imageIndex,
            rotation=choice.rotation,
            cropped=was_cropped
        ))

    total_ms = int((time.time() - start_time) * 1000)
    logger.info(f"[Apply] Processed {len(processed)} images in {total_ms}ms, saved to {debug_dir}")

    return {"processedImages": processed, "timing": total_ms}


# ============================================================
# AI SDK DATA STREAM PROTOCOL ENDPOINT
# ============================================================

class AISdkChatRequest(BaseModel):
    messages: List[dict]
    chatId: Optional[str] = None
    model: Optional[str] = None
    showStats: bool = True


@app.post("/api/chat/aisdk")
async def chat_aisdk(request: AISdkChatRequest):
    """
    AI SDK Data Stream Protocol v1 compatible endpoint.

    This endpoint streams responses in the Vercel AI SDK format,
    which can be consumed directly by useChat on the frontend.

    Documentation: https://sdk.vercel.ai/docs/ai-sdk-ui/stream-protocol
    """
    global orders_context_sent

    thread_id = request.chatId or str(uuid.uuid4())
    config = {"configurable": {"thread_id": thread_id}}

    # Select the appropriate graph based on requested model
    model_name = request.model or DEFAULT_MODEL
    if model_name not in graphs:
        logger.warning(f"[AI SDK] Unknown model '{model_name}', using default: {DEFAULT_MODEL}")
        model_name = DEFAULT_MODEL
    graph = graphs[model_name]
    logger.info(f"[AI SDK] Using model: {model_name}")

    # Convert messages to LangChain format
    conversation_messages = []
    for msg in request.messages:
        role = msg.get("role", "")
        content = msg.get("content", "")

        # Handle multimodal content
        if isinstance(content, list):
            lc_content = []
            for part in content:
                if isinstance(part, dict):
                    if part.get("type") == "text":
                        lc_content.append({"type": "text", "text": part.get("text", "")})
                    elif part.get("type") == "image_url":
                        image_url = part.get("image_url", {})
                        url = image_url.get("url", "") if isinstance(image_url, dict) else image_url
                        lc_content.append({"type": "image_url", "image_url": {"url": url}})
                    elif part.get("type") == "media":
                        lc_content.append({
                            "type": "media",
                            "data": part.get("data", ""),
                            "mime_type": part.get("mime_type", "audio/webm")
                        })
            content = lc_content if lc_content else ""

        if role == "user":
            conversation_messages.append(HumanMessage(content=content))
        elif role == "assistant":
            conversation_messages.append(AIMessage(content=content))

    if not conversation_messages:
        logger.error("[AI SDK] No messages found in request")
        async def error_stream():
            yield StreamAdapter.error("No messages found")
            yield StreamAdapter.finish("error")
        return StreamingResponse(
            error_stream(),
            media_type="text/plain",
            headers={
                "x-vercel-ai-data-stream": "v1",
                "Cache-Control": "no-cache",
                "X-Chat-Id": thread_id,
            }
        )

    # Log request
    last_user_msg = None
    for msg in reversed(request.messages):
        if msg.get("role") == "user":
            last_user_msg = msg.get("content")
            break

    logger.info("=" * 60)
    if isinstance(last_user_msg, list):
        msg_parts = []
        for part in last_user_msg:
            if isinstance(part, dict):
                if part.get('type') == 'text':
                    msg_parts.append(part.get('text', '')[:100])
                elif part.get('type') == 'image_url':
                    # Log image size for debugging
                    image_url_data = part.get('image_url', {})
                    url = image_url_data.get('url', '') if isinstance(image_url_data, dict) else str(image_url_data)
                    if url.startswith('data:'):
                        base64_parts = url.split(',', 1)
                        if len(base64_parts) > 1:
                            base64_size = len(base64_parts[1])
                            approx_bytes = int(base64_size * 0.75)
                            msg_parts.append(f'[IMAGE: ~{approx_bytes // 1024}KB]')
                        else:
                            msg_parts.append('[IMAGE]')
                    else:
                        msg_parts.append('[IMAGE: URL]')
                elif part.get('type') == 'media':
                    msg_parts.append('[MEDIA]')
        user_msg_display = ' + '.join(msg_parts)
    else:
        user_msg_display = str(last_user_msg)[:200] if last_user_msg else ""
    logger.info(f"[AI SDK] USER: {user_msg_display}")
    logger.info(f"[AI SDK] Thread: {thread_id}, Messages: {len(conversation_messages)}")

    async def generate():
        try:
            # Check if first message and reset state
            is_first_message = len(conversation_messages) <= 2
            if is_first_message:
                reset_tab_state()

            # Get context (force refresh for new chats to get latest orders)
            current_context = await get_orders_context(force_refresh=is_first_message)
            tabs_context = await get_browser_tabs_context()

            # Build initial state
            initial_state = {"messages": conversation_messages}
            context_parts = []

            should_include_orders = is_first_message or not orders_context_sent
            if should_include_orders and current_context:
                context_parts.append(current_context)
            if tabs_context:
                context_parts.append(tabs_context)
            if context_parts:
                initial_state["current_page_context"] = "\n\n".join(context_parts)

            # Stream events using new AI SDK v6 protocol
            adapter = StreamAdapter()
            full_response = []
            total_input_tokens = 0
            total_output_tokens = 0
            ai_responses = 0  # Track actual AI responses (for usage tracking)
            counted_run_ids = set()  # Track which LLM calls we've already counted

            # Gemini pricing (per 1M tokens)
            # Gemini Flash: $0.075 input, $0.30 output
            INPUT_PRICE_PER_1M = 0.075
            OUTPUT_PRICE_PER_1M = 0.30

            # Import usage tracking
            from models import increment_usage, get_usage_stats, get_daily_limit

            # Start the message
            yield adapter.start_message()

            async for event in graph.astream_events(
                initial_state,
                config,
                version="v2"
            ):
                event_type = event.get("event", "")

                if event_type == "on_tool_start":
                    tool_name = event.get("name", "unknown")
                    tool_input = event.get("data", {}).get("input", {})
                    run_id = event.get("run_id", "")
                    tool_call_id = f"call_{run_id[:12]}" if run_id else None
                    # Don't log here - agent.py already logs consolidated summary
                    yield adapter.tool_status(tool_name, "start", tool_input, tool_call_id=tool_call_id)

                elif event_type == "on_tool_end":
                    tool_name = event.get("name", "unknown")
                    run_id = event.get("run_id", "")
                    tool_call_id = f"call_{run_id[:12]}" if run_id else None
                    tool_output = event.get("data", {}).get("output", "")

                    # Extract content from ToolMessage if it's a LangChain message object
                    if hasattr(tool_output, 'content'):
                        tool_output = tool_output.content

                    # Parse JSON string to object if possible (for ask_user, etc.)
                    result_data = tool_output
                    if isinstance(tool_output, str) and tool_output.startswith('{'):
                        try:
                            result_data = json.loads(tool_output)
                        except json.JSONDecodeError:
                            result_data = tool_output[:500] if tool_output else "completed"
                    elif tool_output:
                        result_data = str(tool_output)[:500]
                    else:
                        result_data = "completed"

                    # Don't log here - tools.py already logs details
                    yield adapter.tool_status(tool_name, "end", tool_call_id=tool_call_id, result=result_data)

                elif event_type == "on_chat_model_stream":
                    chunk = event["data"].get("chunk")
                    if chunk and hasattr(chunk, 'content') and chunk.content:
                        content = chunk.content
                        if isinstance(content, list):
                            text_parts = [p.get('text', '') for p in content if isinstance(p, dict) and p.get('type') == 'text']
                            content = ''.join(text_parts)
                        if content:
                            full_response.append(content)
                            yield adapter.text_delta(content)

                elif event_type == "on_chat_model_end":
                    run_id = event.get("run_id", "")
                    output = event.get("data", {}).get("output")
                    event_name = event.get("name", "unknown")

                    # Skip if we've already counted this run_id (avoid double-counting)
                    if run_id and run_id in counted_run_ids:
                        continue

                    # LangChain emits on_chat_model_end twice per LLM call:
                    # 1. From base class (ChatGoogleGenerativeAI)
                    # 2. From wrapper class (ChatGoogleGenerativeAIWithKeyRotation)
                    # Only count events from the BASE class to avoid double-counting
                    if event_name != "ChatGoogleGenerativeAI":
                        continue

                    if output:
                        # Only count if LLM returned actual tool_calls or content
                        has_tool_calls = hasattr(output, 'tool_calls') and output.tool_calls
                        has_content = False
                        if hasattr(output, 'content') and output.content:
                            content = output.content
                            if isinstance(content, str):
                                has_content = bool(content.strip())
                            elif isinstance(content, list):
                                # Check for text content in list format (Gemini 3)
                                has_content = any(
                                    isinstance(p, dict) and p.get('type') == 'text' and p.get('text', '').strip()
                                    for p in content
                                )

                        if has_tool_calls or has_content:
                            if run_id:
                                counted_run_ids.add(run_id)
                            ai_responses += 1
                            increment_usage(model_name)

                        # Handle usage metadata
                        usage = getattr(output, 'usage_metadata', None)
                        if usage and isinstance(usage, dict):
                            total_input_tokens += usage.get('input_tokens', 0) or usage.get('prompt_token_count', 0) or 0
                            total_output_tokens += usage.get('output_tokens', 0) or usage.get('candidates_token_count', 0) or 0

                        # If streaming didn't happen, yield the full content here
                        if not full_response and hasattr(output, 'content') and output.content:
                            content = output.content
                            if isinstance(content, str) and content:
                                full_response.append(content)
                                yield adapter.text_delta(content)
                            elif isinstance(content, list):
                                text_parts = [p.get('text', '') for p in content if isinstance(p, dict) and p.get('type') == 'text']
                                text = ''.join(text_parts)
                                if text:
                                    full_response.append(text)
                                    yield adapter.text_delta(text)

            # Calculate and send usage summary as text (if enabled)
            total_tokens = total_input_tokens + total_output_tokens
            input_cost = (total_input_tokens / 1_000_000) * INPUT_PRICE_PER_1M
            output_cost = (total_output_tokens / 1_000_000) * OUTPUT_PRICE_PER_1M
            total_cost = input_cost + output_cost

            if request.showStats:
                # Get current usage stats for this model
                usage_stats = get_usage_stats()
                daily_limit = get_daily_limit()
                model_usage = usage_stats.get("models", {}).get(model_name, 0)

                # Send usage summary at the end
                usage_summary = f"\n\n---\n📊 **Stats**: {ai_responses} prompts ({model_usage}/{daily_limit} today)"
                if total_tokens > 0:
                    usage_summary += f" | Tokens: {total_input_tokens:,} in + {total_output_tokens:,} out"
                    usage_summary += f" | ${total_cost:.6f}"
                usage_summary += "\n"

                yield adapter.text_delta(usage_summary)

            # Send finish with usage
            usage = None
            if total_input_tokens or total_output_tokens:
                usage = {
                    "promptTokens": total_input_tokens,
                    "completionTokens": total_output_tokens,
                    "totalTokens": total_input_tokens + total_output_tokens
                }
            yield adapter.finish("stop", usage)

            # Log summary
            response_preview = ''.join(full_response)[:100]
            logger.info(f"[AI SDK] Done: {ai_responses} AI responses, {total_tokens} tokens, response: {response_preview}...")

        except Exception as e:
            logger.error(f"[AI SDK] Error: {e}", exc_info=True)
            adapter = StreamAdapter()
            yield adapter.error(str(e))
            yield adapter.finish("error")

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "x-vercel-ai-ui-message-stream": "v1",
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Chat-Id": thread_id,
        }
    )


# ============================================================
# OPENAI-COMPATIBLE ENDPOINT (Optional - for LobeChat integration)
# ============================================================

class OpenAIChatRequest(BaseModel):
    model: str
    messages: List[dict]
    stream: bool = False
    temperature: float = 0.7


@app.post("/v1/chat/completions")
async def openai_compatible_chat(request: OpenAIChatRequest):
    """
    OpenAI-compatible chat completions endpoint.

    This translates OpenAI format to our LangGraph agent format,
    allowing LobeChat to use our agent as a model provider.
    """
    import time as time_module

    # Debug: log raw request
    logger.debug(f"[Request] Messages count: {len(request.messages)}")
    for i, msg in enumerate(request.messages):
        content = msg.get('content', '')
        # Handle multimodal content (list with text/images/audio)
        if isinstance(content, list):
            content_summary = []
            for part in content:
                if isinstance(part, dict):
                    if part.get('type') == 'text':
                        content_summary.append(f"text:{part.get('text', '')[:50]}")
                    elif part.get('type') == 'image_url':
                        content_summary.append("[IMAGE]")
                    elif part.get('type') == 'media':
                        mime = part.get('mime_type', 'unknown')
                        content_summary.append(f"[AUDIO:{mime}]" if 'audio' in mime else f"[VIDEO:{mime}]")
                    else:
                        content_summary.append(f"[{part.get('type', 'unknown')}]")
                else:
                    content_summary.append(str(part)[:30])
            content_str = ', '.join(content_summary)
        else:
            content_str = str(content)[:100]
        logger.debug(f"[Request] [{i}] role={msg.get('role')}, content={content_str}")

    # Detect and reject LobeChat's auxiliary requests (topic naming, translation, etc.)
    # These have role=developer/system with summarization/translation prompts
    for msg in request.messages:
        role = msg.get("role", "")
        if role in ["developer", "system"]:
            content = str(msg.get("content", "")).lower()
            if any(keyword in content for keyword in ["summarizer", "summarize", "title", "translate", "translation", "compress"]):
                logger.info(f"[Request] Skipping auxiliary request (topic naming/translation)")
                # Return a simple response without invoking the agent
                response_id = f"chatcmpl-{uuid.uuid4().hex[:8]}"
                created_time = int(time_module.time())
                if request.stream:
                    async def simple_stream():
                        data = {
                            "id": response_id,
                            "object": "chat.completion.chunk",
                            "created": created_time,
                            "model": request.model,
                            "choices": [{"index": 0, "delta": {"content": "Lab Assistant"}, "finish_reason": None}]
                        }
                        yield f"data: {json.dumps(data)}\n\n"
                        final = {
                            "id": response_id,
                            "object": "chat.completion.chunk",
                            "created": created_time,
                            "model": request.model,
                            "choices": [{"index": 0, "delta": {}, "finish_reason": "stop"}]
                        }
                        yield f"data: {json.dumps(final)}\n\n"
                        yield "data: [DONE]\n\n"
                    return StreamingResponse(simple_stream(), media_type="text/event-stream")
                else:
                    return {
                        "id": response_id,
                        "object": "chat.completion",
                        "created": created_time,
                        "model": request.model,
                        "choices": [{"index": 0, "message": {"role": "assistant", "content": "Lab Assistant"}, "finish_reason": "stop"}]
                    }

    thread_id = str(uuid.uuid4())
    config = {"configurable": {"thread_id": thread_id}}

    # Select the appropriate graph based on requested model
    # The request.model from OpenAI format may differ from our internal model names
    model_name = request.model if request.model in graphs else DEFAULT_MODEL
    graph = graphs[model_name]
    logger.info(f"[OpenAI] Using model: {model_name}")

    # Convert OpenAI-format messages to LangGraph messages
    # This preserves full conversation history from the frontend
    conversation_messages = []
    for msg in request.messages:
        role = msg.get("role", "")
        content = msg.get("content", "")

        # Handle multimodal content (images, audio, video) - convert to LangChain format
        if isinstance(content, list):
            # Convert OpenAI multimodal format to LangChain format
            lc_content = []
            for part in content:
                if isinstance(part, dict):
                    if part.get("type") == "text":
                        lc_content.append({"type": "text", "text": part.get("text", "")})
                    elif part.get("type") == "image_url":
                        image_url = part.get("image_url", {})
                        url = image_url.get("url", "") if isinstance(image_url, dict) else image_url
                        lc_content.append({"type": "image_url", "image_url": {"url": url}})
                    elif part.get("type") == "media":
                        # Audio/video for Gemini (native format)
                        lc_content.append({
                            "type": "media",
                            "data": part.get("data", ""),
                            "mime_type": part.get("mime_type", "audio/webm")
                        })
            content = lc_content if lc_content else ""

        if role == "user":
            conversation_messages.append(HumanMessage(content=content))
        elif role == "assistant":
            conversation_messages.append(AIMessage(content=content))
        # Skip system/developer roles - they're handled separately

    if not conversation_messages:
        logger.error("No messages found in request")
        return {"error": "No messages found"}

    # Get the last user message for logging
    last_user_message = None
    for msg in reversed(request.messages):
        if msg["role"] == "user":
            last_user_message = msg["content"]
            break

    logger.info("=" * 60)
    # Handle multimodal user message for logging
    if isinstance(last_user_message, list):
        msg_parts = []
        for part in last_user_message:
            if isinstance(part, dict):
                if part.get('type') == 'text':
                    msg_parts.append(part.get('text', '')[:100])
                elif part.get('type') == 'image_url':
                    # Log image size for debugging
                    image_url_data = part.get('image_url', {})
                    url = image_url_data.get('url', '') if isinstance(image_url_data, dict) else str(image_url_data)
                    if url.startswith('data:'):
                        # Extract base64 size to estimate original image size
                        base64_parts = url.split(',', 1)
                        if len(base64_parts) > 1:
                            base64_size = len(base64_parts[1])
                            approx_bytes = int(base64_size * 0.75)  # base64 is ~33% larger
                            msg_parts.append(f'[IMAGE: ~{approx_bytes // 1024}KB]')
                        else:
                            msg_parts.append('[IMAGE]')
                    else:
                        msg_parts.append('[IMAGE: URL]')
                elif part.get('type') == 'media':
                    mime = part.get('mime_type', 'unknown')
                    msg_parts.append(f'[AUDIO:{mime}]' if 'audio' in mime else f'[VIDEO:{mime}]')
                else:
                    msg_parts.append(f"[{part.get('type', 'unknown')}]")
        user_msg_display = ' + '.join(msg_parts)
    else:
        user_msg_display = str(last_user_message)[:200]
    logger.info(f"USER MESSAGE: {user_msg_display}{'...' if len(str(last_user_message)) > 200 else ''}")
    logger.info(f"Thread ID: {thread_id}, Stream: {request.stream}, History: {len(conversation_messages)} messages")

    if request.stream:
        async def generate():
            global orders_context_sent
            full_response = []
            response_id = f"chatcmpl-{uuid.uuid4().hex[:8]}"  # Same ID for all chunks
            created_time = int(time_module.time())  # Unix timestamp

            # Track AI responses (for usage tracking)
            ai_responses = 0
            total_input_tokens = 0
            total_output_tokens = 0

            # Import usage tracking
            from models import increment_usage, get_usage_stats, get_daily_limit

            # Gemini pricing (per 1M tokens) - adjust based on model
            # Gemini 3 Flash Preview: $0.50 input, $3.00 output (incl. thinking tokens)
            INPUT_PRICE_PER_1M = 0.50
            OUTPUT_PRICE_PER_1M = 3.00

            try:
                # Check if this is first message in conversation (from frontend history)
                is_first_message = len(conversation_messages) <= 2  # Just system + first user msg

                # Reset tab state tracking on new conversation
                if is_first_message:
                    reset_tab_state()

                # Get current context (force refresh for new chats to get latest orders)
                current_context = await get_orders_context(force_refresh=is_first_message)

                # Get browser tabs context (always at each user message)
                tabs_context = await get_browser_tabs_context()

                # Pass full conversation history to the graph
                initial_state = {"messages": conversation_messages}

                # Build full context
                context_parts = []

                # Include orders context if: first message OR we haven't sent valid orders yet
                should_include_orders = is_first_message or not orders_context_sent
                if should_include_orders and current_context:
                    context_parts.append(current_context)
                    if "SESIÓN NO INICIADA" in current_context:
                        logger.info("[Chat] Not logged in - sending login reminder")
                    else:
                        logger.info("[Chat] Including orders context")

                # Always include tabs context at every message
                if tabs_context:
                    context_parts.append(tabs_context)
                    logger.info("[Chat] Including browser tabs context")

                if context_parts:
                    initial_state["current_page_context"] = "\n\n".join(context_parts)

                async for event in graph.astream_events(
                    initial_state,
                    config,
                    version="v2"
                ):
                    event_type = event.get("event", "")

                    # Note: AI response counting now done in on_chat_model_end
                    if event_type == "on_chat_model_start":
                        pass  # Used for debugging only if needed

                    # Stream tool calls to show "thinking" in LobeChat
                    if event_type == "on_tool_start":
                        tool_name = event.get("name", "unknown")
                        tool_input = event.get("data", {}).get("input", {})
                        logger.info(f"TOOL CALL: {tool_name}")
                        logger.debug(f"  Input: {json.dumps(tool_input, ensure_ascii=False)[:500]}")

                        # Send tool call as a "thinking" step to frontend
                        tool_display = f"🔧 **{tool_name}**"
                        if tool_input:
                            # Show ALL parameters
                            params = []
                            for k, v in tool_input.items():
                                if isinstance(v, str):
                                    # Truncate long strings
                                    display_v = v if len(v) < 50 else v[:47] + "..."
                                    params.append(f"{k}={display_v}")
                                elif isinstance(v, list):
                                    # Show list with all items (truncate if too many)
                                    if len(v) <= 10:
                                        params.append(f"{k}={v}")
                                    else:
                                        params.append(f"{k}=[{', '.join(str(x) for x in v[:10])}... +{len(v)-10} more]")
                                elif isinstance(v, (int, float, bool)):
                                    params.append(f"{k}={v}")
                            if params:
                                tool_display += f" ({', '.join(params)})"
                        tool_display += "\n"

                        data = {
                            "id": response_id,
                            "object": "chat.completion.chunk",
                            "created": created_time,
                            "model": request.model,
                            "choices": [{
                                "index": 0,
                                "delta": {"content": tool_display},
                                "finish_reason": None
                            }]
                        }
                        yield f"data: {json.dumps(data)}\n\n"

                    elif event_type == "on_tool_end":
                        tool_name = event.get("name", "unknown")
                        tool_output = event.get("data", {}).get("output", "")
                        logger.info(f"TOOL RESULT: {tool_name}")
                        logger.debug(f"  Output: {str(tool_output)[:500]}")

                        # Send brief result indicator
                        result_display = f"✓ {tool_name} completado\n\n"
                        data = {
                            "id": response_id,
                            "object": "chat.completion.chunk",
                            "created": created_time,
                            "model": request.model,
                            "choices": [{
                                "index": 0,
                                "delta": {"content": result_display},
                                "finish_reason": None
                            }]
                        }
                        yield f"data: {json.dumps(data)}\n\n"

                    elif event_type == "on_chat_model_stream":
                        chunk = event["data"].get("chunk")
                        if chunk and hasattr(chunk, 'content') and chunk.content:
                            # Handle both string and list content (Gemini 3 with thinking)
                            content = chunk.content
                            if isinstance(content, list):
                                # Extract text parts only, skip thinking parts
                                text_parts = [p.get('text', '') for p in content if isinstance(p, dict) and p.get('type') == 'text']
                                content = ''.join(text_parts)
                            if content:
                                full_response.append(content)
                                data = {
                                    "id": response_id,
                                    "object": "chat.completion.chunk",
                                    "created": created_time,
                                    "model": request.model,
                                    "choices": [{
                                        "index": 0,
                                        "delta": {"content": content},
                                        "finish_reason": None
                                    }]
                                }
                                yield f"data: {json.dumps(data)}\n\n"

                    # Handle non-streaming model responses (after key rotation) and extract token usage
                    elif event_type == "on_chat_model_end":
                        output = event.get("data", {}).get("output")
                        event_name = event.get("name", "unknown")

                        # LangChain emits on_chat_model_end twice per LLM call:
                        # 1. From base class (ChatGoogleGenerativeAI)
                        # 2. From wrapper class (ChatGoogleGenerativeAIWithKeyRotation)
                        # Only count events from the BASE class to avoid double-counting
                        if event_name != "ChatGoogleGenerativeAI":
                            continue

                        if output:
                            # Count this as an AI response and increment usage
                            ai_responses += 1
                            increment_usage(model_name)

                            # Try to extract token usage from response
                            # Check for usage_metadata on AIMessage (it's a dict, not object)
                            # LangChain format: {'input_tokens': X, 'output_tokens': Y, 'total_tokens': Z}
                            usage = getattr(output, 'usage_metadata', None)
                            if usage and isinstance(usage, dict):
                                input_tokens = usage.get('input_tokens', 0) or usage.get('prompt_token_count', 0) or 0
                                output_tokens = usage.get('output_tokens', 0) or usage.get('candidates_token_count', 0) or 0
                                if input_tokens or output_tokens:
                                    total_input_tokens += input_tokens
                                    total_output_tokens += output_tokens
                                    # Log thinking tokens if available
                                    output_details = usage.get('output_token_details', {})
                                    thinking_tokens = output_details.get('reasoning', 0) if output_details else 0
                                    logger.info(f"[AI {ai_responses}] Tokens: in={input_tokens}, out={output_tokens}" +
                                               (f" (thinking={thinking_tokens})" if thinking_tokens else ""))

                            # Also check response_metadata for langchain (Google-specific format)
                            resp_meta = getattr(output, 'response_metadata', {}) or {}
                            if resp_meta and 'usage_metadata' in resp_meta:
                                usage_meta = resp_meta['usage_metadata']
                                # Google format: prompt_token_count, candidates_token_count
                                input_tokens = usage_meta.get('prompt_token_count', 0)
                                output_tokens = usage_meta.get('candidates_token_count', 0)
                                # Avoid double counting - only add if we didn't get from usage_metadata above
                                if (input_tokens or output_tokens) and not usage:
                                    total_input_tokens += input_tokens
                                    total_output_tokens += output_tokens
                                    logger.info(f"[AI {ai_responses}] Tokens (from response_meta): in={input_tokens}, out={output_tokens}")

                        if output and hasattr(output, 'content') and output.content:
                            content = output.content
                            if isinstance(content, list):
                                text_parts = [p.get('text', '') for p in content if isinstance(p, dict) and p.get('type') == 'text']
                                content = ''.join(text_parts)
                            # Only send if we haven't already streamed this content
                            if content and content not in ''.join(full_response):
                                full_response.append(content)
                                data = {
                                    "id": response_id,
                                    "object": "chat.completion.chunk",
                                    "created": created_time,
                                    "model": request.model,
                                    "choices": [{
                                        "index": 0,
                                        "delta": {"content": content},
                                        "finish_reason": None
                                    }]
                                }
                                yield f"data: {json.dumps(data)}\n\n"

                if full_response:
                    logger.info(f"AI RESPONSE: {''.join(full_response)[:300]}{'...' if len(''.join(full_response)) > 300 else ''}")

                # Calculate and display usage summary
                total_tokens = total_input_tokens + total_output_tokens
                input_cost = (total_input_tokens / 1_000_000) * INPUT_PRICE_PER_1M
                output_cost = (total_output_tokens / 1_000_000) * OUTPUT_PRICE_PER_1M
                total_cost = input_cost + output_cost

                # Get current usage stats for this model
                usage_stats = get_usage_stats()
                daily_limit = get_daily_limit()
                model_usage = usage_stats.get("models", {}).get(model_name, 0)

                # Send usage summary at the end
                usage_summary = f"\n\n---\n📊 **Stats**: {ai_responses} prompts ({model_usage}/{daily_limit} today)"
                if total_tokens > 0:
                    usage_summary += f" | Tokens: {total_input_tokens:,} in + {total_output_tokens:,} out"
                    usage_summary += f" | ${total_cost:.6f}"
                usage_summary += "\n"

                data = {
                    "id": response_id,
                    "object": "chat.completion.chunk",
                    "created": created_time,
                    "model": request.model,
                    "choices": [{
                        "index": 0,
                        "delta": {"content": usage_summary},
                        "finish_reason": None
                    }]
                }
                yield f"data: {json.dumps(data)}\n\n"

                logger.info(f"[Usage] AI responses: {ai_responses}, Input: {total_input_tokens}, Output: {total_output_tokens}, Cost: ${total_cost:.6f}")

                # Send final chunk with finish_reason to signal completion
                final_chunk = {
                    "id": response_id,
                    "object": "chat.completion.chunk",
                    "created": created_time,
                    "model": request.model,
                    "choices": [{
                        "index": 0,
                        "delta": {},
                        "finish_reason": "stop"
                    }]
                }
                yield f"data: {json.dumps(final_chunk)}\n\n"
                yield "data: [DONE]\n\n"

            except Exception as e:
                logger.error(f"Stream error: {str(e)}", exc_info=True)
                yield f"data: {json.dumps({'error': str(e)})}\n\n"

        return StreamingResponse(
            generate(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",  # Disable nginx buffering
            }
        )

    else:
        try:
            # Check if this is first message in conversation (from frontend history)
            is_first_message = len(conversation_messages) <= 2  # Just system + first user msg

            # Reset tab state tracking on new conversation
            if is_first_message:
                reset_tab_state()

            # Get current context (force refresh for new chats to get latest orders)
            current_context = await get_orders_context(force_refresh=is_first_message)

            # Get browser tabs context (always at each user message)
            tabs_context = await get_browser_tabs_context()

            # Pass full conversation history to the graph
            initial_state = {"messages": conversation_messages}

            # Build full context
            context_parts = []

            # Include orders context if: first message OR we haven't sent valid orders yet
            should_include_orders = is_first_message or not orders_context_sent
            if should_include_orders and current_context:
                context_parts.append(current_context)
                if "SESIÓN NO INICIADA" in current_context:
                    logger.info("[Chat] Not logged in - sending login reminder")
                else:
                    logger.info("[Chat] Including orders context")

            # Always include tabs context at every message
            if tabs_context:
                context_parts.append(tabs_context)
                logger.info("[Chat] Including browser tabs context")

            if context_parts:
                initial_state["current_page_context"] = "\n\n".join(context_parts)

            logger.info("Invoking LangGraph agent...")
            result = await graph.ainvoke(initial_state, config)

            # Log all messages for debugging
            for i, msg in enumerate(result["messages"]):
                msg_type = type(msg).__name__
                content = msg.content if hasattr(msg, 'content') else str(msg)
                if hasattr(msg, 'tool_calls') and msg.tool_calls:
                    logger.info(f"  [{i}] {msg_type}: {len(msg.tool_calls)} tool calls")
                    for tc in msg.tool_calls:
                        logger.info(f"      -> {tc.get('name', 'unknown')}: {json.dumps(tc.get('args', {}), ensure_ascii=False)[:200]}")
                else:
                    logger.info(f"  [{i}] {msg_type}: {content[:150]}{'...' if len(content) > 150 else ''}")

            last_msg = result["messages"][-1]
            response_text = last_msg.content if hasattr(last_msg, 'content') else str(last_msg)

            logger.info(f"AI RESPONSE: {response_text[:300]}{'...' if len(response_text) > 300 else ''}")
            logger.info("=" * 60)

            return {
                "id": f"chatcmpl-{uuid.uuid4().hex[:8]}",
                "object": "chat.completion",
                "model": request.model,
                "choices": [{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": response_text
                    },
                    "finish_reason": "stop"
                }]
            }
        except Exception as e:
            logger.error(f"Error invoking agent: {str(e)}", exc_info=True)
            return {
                "id": f"chatcmpl-{uuid.uuid4().hex[:8]}",
                "object": "chat.completion",
                "model": request.model,
                "choices": [{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": f"Error: {str(e)}"
                    },
                    "finish_reason": "stop"
                }]
            }


# ============================================================
# MAIN
# ============================================================

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "server:app",
        host=settings.host,
        port=settings.port,
        reload=settings.debug
    )
