"""
FastAPI server with LangGraph integration.

DOCUMENTATION:
- LangGraph + FastAPI: https://langchain-ai.github.io/langgraph/how-tos/deploy-self-hosted/
- Streaming: https://langchain-ai.github.io/langgraph/concepts/streaming/
- Checkpointing: https://langchain-ai.github.io/langgraph/concepts/persistence/

ENDPOINTS:
- POST /api/chat: Send message, get response
- GET /api/chat/{thread_id}/history: Get conversation history
- GET /api/browser/screenshot: Get current browser state
- GET /api/health: Health check

NO APPROVAL ENDPOINTS NEEDED - Website's Save button is the human-in-the-loop.
"""
import os
import sys
import asyncio
import base64
import uuid
import json
import logging
import re
import csv
from datetime import datetime
from typing import Optional, List
from contextlib import asynccontextmanager
from pathlib import Path

# Configure logging - use INFO level to reduce noise
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

# Exams list file (generated by scripts/process_tarifas.py)
EXAMS_FILE = Path(__file__).parent / "config" / "lista_de_examenes.csv"
EXAMS_LAST_UPDATE_FILE = Path(__file__).parent / "config" / "exams_last_update.txt"

# Reduce noise from various loggers
logging.getLogger("asyncio").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)

# Set up Windows event loop policy if on Windows
if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from langchain_core.messages import HumanMessage, AIMessage

# LangGraph imports
from langgraph.checkpoint.memory import MemorySaver
# For production with SQLite:
# from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver

# Local imports
from graph.agent import create_lab_agent, compile_agent
from graph.tools import set_browser, close_all_tabs, get_active_tabs, _get_browser_tabs_impl, reset_tab_state, ALL_TOOLS
from browser_manager import BrowserManager
from extractors import EXTRACT_ORDENES_JS
from config import settings
from prompts import SYSTEM_PROMPT, load_prompts, save_prompts, reload_prompts, get_default_prompts
from stream_adapter import StreamAdapter
from claude_provider import ClaudeCodeProvider, get_claude_provider, check_claude_code_status, CLAUDE_SDK_AVAILABLE


def load_exams_from_csv() -> List[dict]:
    """
    Load available exams from CSV file.
    Returns list of exams with codigo, nombre, and prices.
    """
    if not EXAMS_FILE.exists():
        logger.warning(f"[Exams] File not found: {EXAMS_FILE}")
        return []

    exams = []
    with open(EXAMS_FILE, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            exams.append({
                'codigo': row.get('codigo', ''),
                'nombre': row.get('nombre', ''),
                'precio': float(row.get('precio_particular', 0) or 0),
                'precio_desc': float(row.get('precio_descuento', 0) or 0),
            })

    logger.info(f"[Exams] Loaded {len(exams)} exams from {EXAMS_FILE.name}")
    return exams


# Cache exams at module level (loaded once at startup)
_cached_exams: List[dict] = []


def get_available_exams_context() -> str:
    """Generate context string for available exams."""
    global _cached_exams

    if not _cached_exams:
        _cached_exams = load_exams_from_csv()

    if not _cached_exams:
        return ""

    lines = ["# Exámenes Disponibles (Código - Nombre - Precio)"]
    for exam in _cached_exams:
        precio = f"${exam['precio']:.2f}" if exam['precio'] else "N/A"
        lines.append(f"- {exam['codigo']}: {exam['nombre'][:45]} ({precio})")

    lines.append("")
    lines.append("*Para cotización: create_new_order(cedula=\"\", exams=[\"CODIGO1\",...])*")

    return "\n".join(lines)


# ============================================================
# PYDANTIC MODELS
# ============================================================

class ChatRequest(BaseModel):
    message: str
    thread_id: Optional[str] = None


class ChatResponse(BaseModel):
    status: str  # "complete", "error"
    message: str
    thread_id: str
    iterations: Optional[int] = None


# ============================================================
# GLOBAL STATE
# ============================================================

browser: Optional[BrowserManager] = None
graphs: dict = {}  # Dictionary of graphs, keyed by model name
checkpointer = None
initial_orders_context: str = ""  # Store initial orders for context
orders_context_sent: bool = False  # Track if we've sent valid orders context
orders_context_timestamp: float = 0  # When orders were last fetched
ORDERS_FRESHNESS_SECONDS = 120  # Orders are fresh for 2 minutes

# Available models (must match frontend MODEL_CONFIGS)
# Claude models use Claude Code CLI with Max subscription (no API key needed)
# Gemini models use direct API with key rotation
AVAILABLE_MODELS = [
    "claude-opus-4-5",       # Claude Opus 4.5 via Claude Code (most capable)
    "claude-sonnet-4-5",     # Claude Sonnet 4.5 via Claude Code (fast)
    "gemini-3-flash-preview",  # Gemini 3 Flash (fallback)
    "gemini-flash-latest",   # Gemini 2.5 Flash
]
DEFAULT_MODEL = "claude-opus-4-5"  # Default to Claude Opus 4.5

# Claude model mapping (short name -> full model ID)
CLAUDE_MODELS = {
    "claude-opus-4-5": "claude-opus-4-5-20250514",
    "claude-sonnet-4-5": "claude-sonnet-4-5-20250929",
}

# Gemini models (for fallback)
GEMINI_MODELS = ["gemini-3-flash-preview", "gemini-flash-latest"]
GEMINI_FALLBACK_MODEL = "gemini-3-flash-preview"


# ============================================================
# HELPER FUNCTIONS
# ============================================================

def is_logged_in() -> bool:
    """Check if the browser is logged in (not on login page)."""
    if browser and browser.page:
        return "/login" not in browser.page.url
    return False


async def get_orders_context(force_refresh: bool = False) -> str:
    """
    Get orders context, checking login state first.
    Returns empty string with login message if not logged in.
    Returns orders table if logged in and orders found.

    Args:
        force_refresh: If True, re-fetch orders even if already cached.
                       Use this for new chats to get fresh data.
    """
    global browser, initial_orders_context, orders_context_sent, orders_context_timestamp

    if not is_logged_in():
        logger.info("[Context] User not logged in - browser is on login page")
        return "⚠️ SESIÓN NO INICIADA: El navegador está en la página de login. Por favor, inicia sesión en el navegador para que pueda acceder a las órdenes del laboratorio."

    # Check if orders are stale (older than ORDERS_FRESHNESS_SECONDS)
    import time
    current_time = time.time()
    is_stale = (current_time - orders_context_timestamp) > ORDERS_FRESHNESS_SECONDS

    # Force refresh for new chats, stale data, or if we haven't sent valid orders yet
    if force_refresh or is_stale or not orders_context_sent or not initial_orders_context:
        if is_stale and orders_context_sent:
            logger.info(f"[Context] Orders are stale ({int(current_time - orders_context_timestamp)}s old), refreshing...")
        else:
            logger.info("[Context] Extracting orders context...")
        initial_orders_context = await extract_initial_context()
        orders_context_timestamp = current_time
        if initial_orders_context and "Órdenes Recientes" in initial_orders_context:
            orders_context_sent = True
            # Count orders: 7 pipes per row, subtract 2 for header/separator
            order_count = max(0, (initial_orders_context.count('|') // 7) - 2)
            logger.info(f"[Context] Extracted {order_count} orders")

    return initial_orders_context


def get_orders_freshness() -> dict:
    """Get current orders context freshness status."""
    import time
    current_time = time.time()
    age_seconds = current_time - orders_context_timestamp if orders_context_timestamp else 0
    is_fresh = age_seconds < ORDERS_FRESHNESS_SECONDS

    return {
        "has_orders": bool(initial_orders_context and "Órdenes Recientes" in initial_orders_context),
        "age_seconds": int(age_seconds),
        "is_fresh": is_fresh,
        "freshness_threshold": ORDERS_FRESHNESS_SECONDS
    }


async def get_browser_tabs_context() -> str:
    """
    Get browser tabs context with state tracking.

    - Shows all tabs with IDs, patient names, and enumeration for duplicates
    - For NEW tabs: shows full state (exams, fields, etc.)
    - For KNOWN tabs: shows only what CHANGED since last message
    - Marks which tab is active
    """
    try:
        tabs_info = await _get_browser_tabs_impl()

        if not tabs_info.get("tabs"):
            return "# Pestañas del Navegador\nNo hay pestañas abiertas. Para editar resultados, primero usa get_order_results(order_nums) para abrir la pestaña de resultados."

        lines = ["# Pestañas del Navegador"]

        # Track if there are any results tabs
        has_results_tabs = any(t.get("type") == "resultados" for t in tabs_info.get("tabs", []))

        type_display = {
            "ordenes_list": "Lista de Órdenes",
            "nueva_orden": "Nueva Orden",
            "orden_edit": "Editar Orden",
            "resultados": "Resultados",
            "login": "Login",
            "unknown": "Otra"
        }

        for idx, tab in enumerate(tabs_info.get("tabs", [])):
            tab_type = tab.get("type", "unknown")
            is_active = tab.get("active", False)
            is_new = tab.get("is_new", False)
            tab_id = tab.get("id")
            paciente = tab.get("paciente")
            instance = tab.get("instance")  # For duplicate enumeration
            state = tab.get("state")  # Full state for new tabs
            changes = tab.get("changes")  # Only changes for known tabs

            # Build tab header - always include tab_index
            marker = "→ " if is_active else "  "
            tab_line = f"{marker}[tab_index={idx}] {type_display.get(tab_type, tab_type)}"

            # Add ID for saved orders
            if tab_id:
                if tab_type == "resultados":
                    tab_line += f" (order_num={tab_id})"
                elif tab_type == "orden_edit":
                    tab_line += f" (order_id={tab_id})"

            # Add instance number for duplicates
            if instance:
                tab_line += f" #{instance}"

            # Add patient name
            if paciente:
                tab_line += f" - {paciente[:25]}"

            # Add NEW marker for unsaved orders
            if is_new:
                tab_line += " [NUEVA - sin guardar]"

            lines.append(tab_line)

            # For new tabs, show full state
            if is_new and state:
                if tab_type == "resultados":
                    lines.append(f"    Exámenes: {state.get('examenes_count', 0)}")
                    # Show field values (limited)
                    field_values = state.get("field_values", {})
                    filled = [(k, v) for k, v in field_values.items() if v]
                    if filled:
                        lines.append(f"    Campos con valor: {len(filled)}")
                elif tab_type in ["orden_edit", "nueva_orden"]:
                    exams = state.get("exams", [])
                    if exams:
                        lines.append(f"    Exámenes: {', '.join(exams[:8])}")
                    if state.get("total"):
                        lines.append(f"    Total: {state.get('total')}")

            # For known tabs, show only changes
            elif changes:
                lines.append("    **Cambios detectados:**")
                if "field_values" in changes:
                    # Show which fields changed
                    for field_key, new_value in list(changes["field_values"].items())[:5]:
                        if new_value:
                            lines.append(f"    - {field_key}: → {new_value}")
                if "exams" in changes:
                    lines.append(f"    - Exámenes: {', '.join(changes['exams'][:5])}")
                if "total" in changes:
                    lines.append(f"    - Total: {changes['total']}")

        # Add guidance if no results tabs are open
        if not has_results_tabs:
            lines.append("")
            lines.append("⚠️ No hay pestañas de resultados abiertas. Para editar resultados, usa get_order_results(order_nums) primero.")

        return "\n".join(lines)

    except Exception as e:
        logger.warning(f"[Context] Could not get browser tabs context: {e}")
        return ""


# ============================================================
# LIFESPAN
# ============================================================

# Track if an auto-update is already in progress
_orders_auto_update_in_progress = False


async def _trigger_orders_auto_update():
    """Background task to auto-update orders list."""
    global _orders_auto_update_in_progress, browser

    if _orders_auto_update_in_progress:
        logger.info("[Context] Orders auto-update already in progress, skipping...")
        return

    _orders_auto_update_in_progress = True
    try:
        logger.info("[Context] Starting automatic orders list update...")

        if not browser:
            logger.warning("[Context] Browser not initialized, cannot auto-update orders")
            return

        await browser.ensure_page()

        # Create a new tab for this operation
        new_page = await browser.context.new_page()

        try:
            # Navigate to ordenes report page
            logger.info("[Orders-Auto] Navigating to ordenes report page...")
            await new_page.goto("https://laboratoriofranz.orion-labs.com/informes/ordenes", timeout=30000)
            await new_page.wait_for_load_state("domcontentloaded", timeout=10000)
            await asyncio.sleep(1)

            # Calculate 1 year ago from today
            from datetime import timedelta
            one_year_ago = (datetime.now() - timedelta(days=365)).strftime("%Y-%m-%d")

            # Set fecha-desde
            logger.info(f"[Orders-Auto] Setting date range from {one_year_ago}...")
            fecha_desde = new_page.locator("#fecha-desde")
            await fecha_desde.click()
            await new_page.keyboard.press("Control+a")
            await fecha_desde.type(one_year_ago, delay=50)
            await asyncio.sleep(0.3)

            await new_page.locator("label[for='fecha-desde']").click()
            await asyncio.sleep(0.5)

            # Set up download handler
            download_dir = Path(__file__).parent / "downloads"
            download_dir.mkdir(exist_ok=True)

            # Click "Generar informe" dropdown
            logger.info("[Orders-Auto] Clicking 'Generar informe' dropdown...")
            dropdown_btn = new_page.locator("button.dropdown-toggle", has_text="Generar informe")
            await dropdown_btn.click()
            await asyncio.sleep(0.5)

            # Click "Excel" option and wait for download
            logger.info("[Orders-Auto] Clicking 'Excel' and waiting for download...")
            async with new_page.expect_download(timeout=120000) as download_info:
                excel_btn = new_page.locator("#generar-informe-ordenes-excel")
                await excel_btn.click()

            download = await download_info.value

            # Save the downloaded file
            xlsx_path = download_dir / download.suggested_filename
            await download.save_as(xlsx_path)
            logger.info(f"[Orders-Auto] Downloaded: {xlsx_path}")

            # Process the XLSX
            logger.info("[Orders-Auto] Processing XLSX...")
            from scripts.process_ordenes import process_ordenes
            process_ordenes(str(xlsx_path))

            # Reload the orders cache
            from orders_cache import reload_orders_cache, set_orders_last_update
            orders = reload_orders_cache()

            # Save the update timestamp
            timestamp = datetime.now().isoformat()
            set_orders_last_update(timestamp)

            # Clean up
            xlsx_path.unlink(missing_ok=True)

            logger.info(f"[Orders-Auto] Update complete! {len(orders)} orders loaded.")

        finally:
            await new_page.close()

    except Exception as e:
        logger.error(f"[Orders-Auto] Auto-update failed: {e}")
        import traceback
        traceback.print_exc()
    finally:
        _orders_auto_update_in_progress = False


async def extract_initial_context() -> str:
    """
    Extract orders list and available exams from the page for AI context.

    Only fetches page 1 (20 orders), checks overlap with cached orders,
    and triggers update if no overlap exists or cache is empty.
    """
    global browser
    lines = []
    needs_update = False

    try:
        from orders_cache import (
            get_cached_orders, check_overlap, merge_orders,
            reload_orders_cache
        )

        # Fetch orders from page 1 only
        page1_url = "https://laboratoriofranz.orion-labs.com/ordenes?page=1"
        logger.info("[Context] Fetching orders page 1...")
        await browser.page.goto(page1_url, timeout=30000)
        # Wait for table rows instead of fixed 2s delay
        try:
            await browser.page.wait_for_selector('table tbody tr, .order-row', timeout=5000)
        except Exception:
            await browser.page.wait_for_timeout(500)
        ordenes_page1 = await browser.page.evaluate(EXTRACT_ORDENES_JS) or []

        logger.info(f"[Context] Extracted {len(ordenes_page1)} orders from page 1")

        # Get page 1 order numbers
        page1_order_nums = {o.get('num', '') for o in ordenes_page1 if o.get('num')}

        # Check overlap with cached orders
        cached_orders = get_cached_orders()
        has_overlap, overlap_count = check_overlap(page1_order_nums)

        # Determine if we need to trigger an update
        if not cached_orders:
            needs_update = True
            logger.warning("[Context] No orders cache found - triggering auto-update...")
        elif not has_overlap:
            needs_update = True
            logger.warning("[Context] No overlap between page 1 and cached orders - triggering auto-update...")

        # Trigger background update if needed (silently, no AI notification to save tokens)
        if needs_update and not _orders_auto_update_in_progress:
            asyncio.create_task(_trigger_orders_auto_update())

        # Merge page 1 with cached orders (20 most recent)
        if cached_orders:
            all_ordenes = merge_orders(ordenes_page1, cached_orders, max_orders=20)
            logger.info(f"[Context] Merged {len(ordenes_page1)} page 1 orders with cache, showing {len(all_ordenes)}")
        else:
            all_ordenes = ordenes_page1[:20]
            logger.info(f"[Context] No cached orders, showing {len(all_ordenes)} from page 1")

        if all_ordenes:
            lines.append("# Órdenes Recientes (20 más recientes)")
            lines.append("| # | Orden | Fecha | Paciente | Cédula | Estado |")
            lines.append("|---|-------|-------|----------|--------|--------|")
            for i, o in enumerate(all_ordenes[:20]):
                paciente = (o.get('paciente', '') or o.get('patient_name', '') or '')[:30]
                lines.append(f"| {i+1} | {o.get('num','')} | {o.get('fecha','')} | {paciente} | {o.get('cedula','')} | {o.get('estado','')} |")

    except Exception as e:
        logger.warning(f"Could not extract orders context: {e}")

    # Add available exams from CSV file (faster than scraping, complete list)
    exams_context = get_available_exams_context()
    if exams_context:
        lines.append("")
        lines.append(exams_context)

    return "\n".join(lines) if lines else ""


@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Application lifespan - initialize browser and LangGraph.

    For fast startup, we skip initial orders extraction - it will be done
    lazily on first chat message instead.
    """
    global browser, graph, checkpointer, initial_orders_context

    print("Starting Lab Assistant with LangGraph...")

    # Create data directory if it doesn't exist
    data_dir = Path(__file__).parent / "data"
    data_dir.mkdir(exist_ok=True)

    # Initialize browser (required before accepting requests)
    browser = BrowserManager(user_data_dir=settings.browser_data_dir)
    await browser.start(headless=settings.headless, browser=settings.browser_channel)
    await browser.navigate(settings.target_url)
    set_browser(browser)

    # Check if we ended up at the welcome page (session redirect) and navigate to orders
    if browser.page and "/bienvenida" in browser.page.url:
        logger.info("[Startup] Redirected to welcome page, navigating to orders...")
        await browser.navigate("https://laboratoriofranz.orion-labs.com/ordenes?page=1")

    # Check login status - if on login page, auto-click the "Ingresar" button
    if browser.page and "/login" in browser.page.url:
        logger.info("[Startup] On login page - attempting auto-login by clicking 'Ingresar' button...")
        try:
            # Click the Ingresar button to login with saved session
            ingresar_button = browser.page.locator("button:has-text('Ingresar')")
            if await ingresar_button.count() > 0:
                await ingresar_button.click()
                logger.info("[Startup] Clicked 'Ingresar' button, waiting for navigation...")
                # Wait for navigation to complete
                await browser.page.wait_for_load_state("networkidle", timeout=10000)
                logger.info(f"[Startup] After login click, now at: {browser.page.url}")

                # If we're still on login page, the session might have expired
                if "/login" in browser.page.url:
                    logger.warning("[Startup] Still on login page - session may have expired. User needs to login manually.")
                    initial_orders_context = "⚠️ SESIÓN NO INICIADA: El navegador está en la página de login. Por favor, inicia sesión manualmente."
                else:
                    # Navigate to orders page
                    if "/bienvenida" in browser.page.url:
                        logger.info("[Startup] Redirected to welcome page, navigating to orders...")
                        await browser.navigate("https://laboratoriofranz.orion-labs.com/ordenes?page=1")
                    # Orders context will be extracted on-demand by get_orders_context()
                    initial_orders_context = ""
                    logger.info("[Startup] Login successful, orders will be fetched on-demand")
            else:
                logger.warning("[Startup] 'Ingresar' button not found on login page")
                initial_orders_context = "⚠️ SESIÓN NO INICIADA: No se encontró el botón 'Ingresar'."
        except Exception as e:
            logger.error(f"[Startup] Auto-login failed: {e}")
            initial_orders_context = f"⚠️ SESIÓN NO INICIADA: Error al intentar login automático: {e}"
    else:
        # Already logged in - orders context will be extracted on-demand by get_orders_context()
        initial_orders_context = ""
        logger.info("[Startup] Already logged in, orders will be fetched on-demand")

    # Initialize checkpointer for conversation persistence
    # Using MemorySaver for development (in-memory, not persistent across restarts)
    # For production, use AsyncSqliteSaver or PostgresSaver
    checkpointer = MemorySaver()

    # Build and compile a graph for each available model
    for model_name in AVAILABLE_MODELS:
        logger.info(f"[Startup] Creating graph for model: {model_name}")
        builder = create_lab_agent(browser, model_name=model_name)
        graphs[model_name] = compile_agent(builder, checkpointer)
        logger.info(f"[Startup] Graph for {model_name} ready")

    print(f"Lab Assistant ready! Browser at: {browser.page.url}")
    print(f"Available models: {', '.join(AVAILABLE_MODELS)}")

    yield

    # Cleanup
    print("Shutting down...")
    close_all_tabs()
    await browser.stop()


# ============================================================
# FASTAPI APP
# ============================================================

app = FastAPI(
    title="Lab Assistant API",
    description="LangGraph-powered lab assistant for clinical laboratory data entry",
    version="2.0.0",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ============================================================
# ENDPOINTS
# ============================================================

@app.get("/api/health")
async def health_check():
    """Health check endpoint."""
    return {
        "status": "ok",
        "browser_url": browser.page.url if browser and browser.page else None,
        "graphs_ready": list(graphs.keys()),
        "default_model": DEFAULT_MODEL
    }


@app.post("/api/chat", response_model=ChatResponse)
async def chat(
    thread_id: str = Form(default=None),
    message: str = Form(...),
    files: List[UploadFile] = File(default=[])
):
    """
    Send a message to the agent and get a response.

    Supports multi-modal input (text, images, audio).
    The agent will execute tools as needed and return when done.

    Args:
        thread_id: Conversation thread ID (generated if not provided)
        message: User message text
        files: Optional image or audio files

    Returns:
        Agent's response with thread_id for continuation
    """
    # Generate thread_id if not provided
    if not thread_id:
        thread_id = str(uuid.uuid4())

    config = {"configurable": {"thread_id": thread_id}}

    # Build message content (multi-modal support)
    content = []

    if message:
        content.append({"type": "text", "text": message})

    # Process uploaded files
    for file in files:
        file_content = await file.read()
        encoded = base64.b64encode(file_content).decode('utf-8')

        if file.content_type and file.content_type.startswith("image/"):
            # Image for vision models
            content.append({
                "type": "image_url",
                "image_url": {"url": f"data:{file.content_type};base64,{encoded}"}
            })
        elif file.content_type and file.content_type.startswith("audio/"):
            # Audio for Gemini (native audio support)
            content.append({
                "type": "media",
                "data": encoded,
                "mime_type": file.content_type
            })

    # Create human message
    if len(content) == 1 and content[0]["type"] == "text":
        human_msg = HumanMessage(content=message)
    else:
        human_msg = HumanMessage(content=content)

    try:
        # Use default model graph (this endpoint doesn't accept model parameter)
        graph = graphs.get(DEFAULT_MODEL)
        if not graph:
            raise ValueError(f"Default graph '{DEFAULT_MODEL}' not found")

        # Invoke graph - it will loop internally until done
        result = await graph.ainvoke(
            {"messages": [human_msg]},
            config
        )

        # Get the final response
        last_msg = result["messages"][-1]
        response_text = last_msg.content if hasattr(last_msg, 'content') else str(last_msg)

        # Count iterations (based on tool messages)
        tool_messages = [m for m in result["messages"] if hasattr(m, 'type') and getattr(m, 'type', None) == 'tool']
        iterations = len(tool_messages)

        return ChatResponse(
            status="complete",
            message=response_text,
            thread_id=thread_id,
            iterations=iterations
        )

    except Exception as e:
        import traceback
        traceback.print_exc()
        return ChatResponse(
            status="error",
            message=f"Error: {str(e)}",
            thread_id=thread_id
        )


@app.post("/api/chat/stream")
async def chat_stream(request: ChatRequest):
    """
    Stream chat responses in real-time using Server-Sent Events.

    DOCUMENTATION:
    - astream_events: https://langchain-ai.github.io/langgraph/concepts/streaming/

    This streams:
    - Token-by-token LLM output
    - Tool execution notifications
    - Final completion
    """
    thread_id = request.thread_id or str(uuid.uuid4())
    config = {"configurable": {"thread_id": thread_id}}

    # Use default model graph (this endpoint doesn't accept model parameter)
    graph = graphs.get(DEFAULT_MODEL)

    async def generate():
        try:
            async for event in graph.astream_events(
                {"messages": [HumanMessage(content=request.message)]},
                config,
                version="v2"
            ):
                event_type = event.get("event", "")

                # Stream LLM tokens
                if event_type == "on_chat_model_stream":
                    chunk = event["data"].get("chunk")
                    if chunk and hasattr(chunk, 'content') and chunk.content:
                        yield f"data: {json.dumps({'type': 'token', 'content': chunk.content})}\n\n"

                # Notify tool execution
                elif event_type == "on_tool_start":
                    tool_name = event["name"]
                    yield f"data: {json.dumps({'type': 'tool_start', 'tool': tool_name})}\n\n"

                elif event_type == "on_tool_end":
                    tool_name = event["name"]
                    yield f"data: {json.dumps({'type': 'tool_end', 'tool': tool_name})}\n\n"

            yield f"data: {json.dumps({'type': 'done', 'thread_id': thread_id})}\n\n"

        except Exception as e:
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={"X-Thread-ID": thread_id}
    )


@app.get("/api/chat/{thread_id}/history")
async def get_history(thread_id: str):
    """
    Get conversation history for a thread.

    Returns list of messages with role and content.
    """
    config = {"configurable": {"thread_id": thread_id}}

    # Use default graph for state retrieval (all graphs share the same checkpointer)
    graph = graphs.get(DEFAULT_MODEL)

    try:
        state = await graph.aget_state(config)
        messages = state.values.get("messages", [])

        return [
            {
                "role": "user" if isinstance(m, HumanMessage) else "assistant",
                "content": m.content if hasattr(m, 'content') else str(m),
                "type": getattr(m, 'type', 'unknown')
            }
            for m in messages
            if not (hasattr(m, 'type') and getattr(m, 'type', None) == 'tool')  # Skip tool messages
        ]
    except Exception as e:
        return []


@app.get("/api/browser/screenshot")
async def get_screenshot():
    """Get current browser screenshot as base64."""
    if browser and browser.page:
        try:
            screenshot_bytes = await browser.page.screenshot(type='png')
            encoded = base64.b64encode(screenshot_bytes).decode('utf-8')
            return {"screenshot": f"data:image/png;base64,{encoded}"}
        except Exception as e:
            raise HTTPException(500, f"Screenshot failed: {str(e)}")
    raise HTTPException(503, "Browser not available")


@app.get("/api/browser/tabs")
async def get_tabs():
    """Get list of open browser tabs."""
    active_tabs = get_active_tabs()
    return {
        "tabs": list(active_tabs.keys()),
        "count": len(active_tabs)
    }


@app.get("/api/browser/tabs/detailed")
async def get_tabs_detailed():
    """Get detailed info about all browser tabs including state."""
    try:
        tabs_info = await _get_browser_tabs_impl()
        return tabs_info
    except Exception as e:
        logger.error(f"Failed to get detailed tabs: {e}")
        return {"error": str(e), "tabs": []}


@app.post("/api/browser/close-tabs")
async def close_tabs():
    """Close all open browser tabs (cleanup)."""
    close_all_tabs()
    return {"status": "ok", "message": "All tabs closed"}


# ============================================================
# MANUAL TOOL EXECUTION ENDPOINT
# ============================================================

class ManualToolRequest(BaseModel):
    tool: str
    args: dict

@app.post("/api/tools/execute")
async def execute_tool(request: ManualToolRequest):
    """
    Execute a tool manually (for UI-based editing).
    Supports: edit_results, edit_order_exams, create_new_order
    """
    from graph.tools import (
        edit_results,
        edit_order_exams,
        create_new_order,
    )

    tool_name = request.tool
    args = request.args

    logger.info(f"[Manual Tool] Executing {tool_name} with args: {args}")

    try:
        if tool_name == "edit_results":
            # edit_results expects data=[{orden, e, f, v}]
            data = args.get("data", [])
            result = await edit_results.ainvoke({"data": data})
            return {"success": True, "message": f"Editados {len(data)} campos", "result": result}

        elif tool_name == "edit_order_exams":
            # edit_order_exams now supports order_id, tab_index, cedula, add, remove
            order_id = args.get("order_id")
            tab_index = args.get("tab_index")
            cedula = args.get("cedula")
            add_exams = args.get("add", [])
            remove_exams = args.get("remove", [])
            result = await edit_order_exams.ainvoke({
                "order_id": order_id,
                "tab_index": tab_index,
                "cedula": cedula,
                "add": add_exams,
                "remove": remove_exams
            })
            changes = []
            if cedula:
                changes.append(f"cédula: {cedula}")
            if add_exams:
                changes.append(f"agregados: {', '.join(add_exams)}")
            if remove_exams:
                changes.append(f"removidos: {', '.join(remove_exams)}")
            return {"success": True, "message": f"Cambios: {' | '.join(changes) if changes else 'ninguno'}", "result": result}

        elif tool_name == "create_new_order":
            cedula = args.get("cedula", "")
            exams = args.get("exams", [])
            result = await create_new_order.ainvoke({"cedula": cedula, "exams": exams})
            return {"success": True, "message": f"Orden creada con {len(exams)} exámenes", "result": result}

        else:
            return {"success": False, "error": f"Tool '{tool_name}' not supported for manual execution"}

    except Exception as e:
        logger.error(f"[Manual Tool] Error: {e}")
        return {"success": False, "error": str(e)}


@app.get("/api/exams")
async def get_exams():
    """Get list of available exams from CSV (cached)."""
    global _cached_exams
    if not _cached_exams:
        _cached_exams = load_exams_from_csv()
    return {"exams": [{"codigo": e["codigo"], "nombre": e["nombre"]} for e in _cached_exams]}


@app.get("/api/exams/last-update")
async def get_exams_last_update():
    """Get the timestamp of the last exam list update."""
    if EXAMS_LAST_UPDATE_FILE.exists():
        timestamp = EXAMS_LAST_UPDATE_FILE.read_text().strip()
        return {"lastUpdate": timestamp}
    return {"lastUpdate": None}


@app.post("/api/exams/update")
async def update_exams_list():
    """
    Download the tarifas CSV from the website and update the exams list.

    Uses Playwright to:
    1. Navigate to the tarifas page
    2. Check "Incluir exámenes con valor cero"
    3. Click "Generar informe" -> "Excel"
    4. Wait for CSV download
    5. Process with process_tarifas.py
    """
    global _cached_exams, browser

    try:
        logger.info("[Exams] Starting exam list update...")

        # Use the existing browser
        if not browser:
            raise HTTPException(status_code=503, detail="Browser not initialized")

        page = await browser.ensure_page()

        # Create a new tab for this operation
        new_page = await browser.context.new_page()

        try:
            # Navigate to tarifas page
            logger.info("[Exams] Navigating to tarifas page...")
            await new_page.goto("https://laboratoriofranz.orion-labs.com/informes/tarifas", timeout=30000)
            await new_page.wait_for_load_state("domcontentloaded", timeout=10000)
            await asyncio.sleep(1)  # Wait for page to settle

            # Check the "Incluir exámenes con valor cero" checkbox
            logger.info("[Exams] Checking 'Incluir exámenes con valor cero' checkbox...")
            checkbox = new_page.locator("#examenes-valor-cero")
            if not await checkbox.is_checked():
                await checkbox.check()
                await asyncio.sleep(0.5)

            # Set up download handler
            download_dir = Path(__file__).parent / "downloads"
            download_dir.mkdir(exist_ok=True)

            # Click "Generar informe" dropdown
            logger.info("[Exams] Clicking 'Generar informe' dropdown...")
            dropdown_btn = new_page.locator("button.dropdown-toggle", has_text="Generar informe")
            await dropdown_btn.click()
            await asyncio.sleep(0.5)

            # Click "Excel" option and wait for download
            logger.info("[Exams] Clicking 'Excel' and waiting for download...")
            async with new_page.expect_download(timeout=60000) as download_info:
                excel_btn = new_page.locator(".dropdown-item", has_text="Excel")
                await excel_btn.click()

            download = await download_info.value

            # Save the downloaded file
            csv_path = download_dir / download.suggested_filename
            await download.save_as(csv_path)
            logger.info(f"[Exams] Downloaded: {csv_path}")

            # Process the CSV with process_tarifas.py
            logger.info("[Exams] Processing CSV...")
            from scripts.process_tarifas import process_tarifas
            process_tarifas(str(csv_path))

            # Reload the exams cache
            _cached_exams = load_exams_from_csv()

            # Save the update timestamp
            timestamp = datetime.now().isoformat()
            EXAMS_LAST_UPDATE_FILE.parent.mkdir(parents=True, exist_ok=True)
            EXAMS_LAST_UPDATE_FILE.write_text(timestamp)

            # Clean up downloaded file
            csv_path.unlink(missing_ok=True)

            logger.info(f"[Exams] Update complete! {len(_cached_exams)} exams loaded.")

            return {
                "success": True,
                "message": f"Lista de exámenes actualizada: {len(_cached_exams)} exámenes",
                "examCount": len(_cached_exams),
                "lastUpdate": timestamp
            }

        finally:
            # Close the tab we opened
            await new_page.close()

    except Exception as e:
        logger.error(f"[Exams] Update failed: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


# ============================================================
# ORDERS LIST UPDATE ENDPOINTS
# ============================================================

ORDERS_LAST_UPDATE_FILE = Path(__file__).parent / "config" / "ordenes_last_update.txt"


@app.get("/api/orders/last-update")
async def get_orders_last_update():
    """Get the timestamp of the last orders list update."""
    if ORDERS_LAST_UPDATE_FILE.exists():
        timestamp = ORDERS_LAST_UPDATE_FILE.read_text().strip()
        return {"lastUpdate": timestamp}
    return {"lastUpdate": None}


@app.post("/api/orders/prefetch")
async def prefetch_orders_context():
    """
    Prefetch orders context in background.
    Call this when receiving an image to have orders ready when user sends prompt.

    Returns freshness info and triggers background fetch if needed.
    """
    # Get current freshness status
    freshness = get_orders_freshness()

    # If orders are stale or missing, fetch them
    if not freshness["is_fresh"] or not freshness["has_orders"]:
        logger.info("[Prefetch] Orders are stale/missing, prefetching...")
        await get_orders_context(force_refresh=True)
        freshness = get_orders_freshness()
        logger.info(f"[Prefetch] Orders prefetched, now {freshness['age_seconds']}s old")

    return {
        "success": True,
        "freshness": freshness
    }


@app.get("/api/orders/freshness")
async def get_orders_freshness_status():
    """Get current orders context freshness status without fetching."""
    return get_orders_freshness()


@app.post("/api/orders/update")
async def update_orders_list():
    """
    Download the orders XLSX from the website and update the orders list.

    Uses Playwright to:
    1. Navigate to the ordenes report page
    2. Set date range: desde=1 year ago, hasta=today
    3. Click "Generar informe" -> "Excel"
    4. Wait for XLSX download
    5. Process with scripts/process_ordenes.py
    """
    global browser

    try:
        logger.info("[Orders] Starting orders list update...")

        # Use the existing browser
        if not browser:
            raise HTTPException(status_code=503, detail="Browser not initialized")

        await browser.ensure_page()

        # Create a new tab for this operation
        new_page = await browser.context.new_page()

        try:
            # Navigate to ordenes report page
            logger.info("[Orders] Navigating to ordenes report page...")
            await new_page.goto("https://laboratoriofranz.orion-labs.com/informes/ordenes", timeout=30000)
            await new_page.wait_for_load_state("domcontentloaded", timeout=10000)
            await asyncio.sleep(1)  # Wait for page to settle

            # Calculate 1 year ago from today (max range is 13 months)
            from datetime import timedelta
            one_year_ago = (datetime.now() - timedelta(days=365)).strftime("%Y-%m-%d")

            # Set fecha-desde to 1 year ago
            logger.info(f"[Orders] Setting date range from {one_year_ago}...")
            fecha_desde = new_page.locator("#fecha-desde")
            await fecha_desde.click()
            await new_page.keyboard.press("Control+a")
            await fecha_desde.type(one_year_ago, delay=50)
            await asyncio.sleep(0.3)

            # Click somewhere else to close datepicker and apply the date
            await new_page.locator("label[for='fecha-desde']").click()
            await asyncio.sleep(0.5)

            # Set up download handler
            download_dir = Path(__file__).parent / "downloads"
            download_dir.mkdir(exist_ok=True)

            # Click "Generar informe" dropdown
            logger.info("[Orders] Clicking 'Generar informe' dropdown...")
            dropdown_btn = new_page.locator("button.dropdown-toggle", has_text="Generar informe")
            await dropdown_btn.click()
            await asyncio.sleep(0.5)

            # Click "Excel" option and wait for download
            logger.info("[Orders] Clicking 'Excel' and waiting for download...")
            async with new_page.expect_download(timeout=120000) as download_info:
                excel_btn = new_page.locator("#generar-informe-ordenes-excel")
                await excel_btn.click()

            download = await download_info.value

            # Save the downloaded file
            xlsx_path = download_dir / download.suggested_filename
            await download.save_as(xlsx_path)
            logger.info(f"[Orders] Downloaded: {xlsx_path}")

            # Process the XLSX with process_ordenes.py
            logger.info("[Orders] Processing XLSX...")
            from scripts.process_ordenes import process_ordenes
            process_ordenes(str(xlsx_path))

            # Reload the orders cache
            from orders_cache import reload_orders_cache, set_orders_last_update
            orders = reload_orders_cache()

            # Save the update timestamp
            timestamp = datetime.now().isoformat()
            set_orders_last_update(timestamp)

            # Clean up downloaded file
            xlsx_path.unlink(missing_ok=True)

            logger.info(f"[Orders] Update complete! {len(orders)} orders loaded.")

            return {
                "success": True,
                "message": f"Lista de órdenes actualizada: {len(orders)} órdenes",
                "orderCount": len(orders),
                "lastUpdate": timestamp
            }

        finally:
            # Close the tab we opened
            await new_page.close()

    except Exception as e:
        logger.error(f"[Orders] Update failed: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/usage")
async def get_usage():
    """Get current usage stats for all models."""
    from models import get_usage_stats, get_daily_limit, get_num_api_keys

    stats = get_usage_stats()
    daily_limit = get_daily_limit()
    num_keys = get_num_api_keys()

    return {
        "date": stats.get("date"),
        "models": stats.get("models", {}),
        "dailyLimit": daily_limit,
        "numApiKeys": num_keys,
        "freePerKey": 20  # Google AI Studio free tier limit per key
    }


# ============================================================
# PROMPTS CONFIGURATION ENDPOINTS
# ============================================================

@app.get("/api/prompts")
async def get_prompts():
    """Get current prompts configuration."""
    prompts = load_prompts()
    return {
        "prompts": prompts,
        "sections": [
            {
                "key": "system_prompt",
                "label": "Instrucciones del Sistema",
                "description": "Instrucciones principales para la IA sobre cómo comportarse"
            },
            {
                "key": "abbreviations",
                "label": "Abreviaturas",
                "description": "Abreviaturas comunes usadas en el laboratorio"
            },
            {
                "key": "image_interpretation",
                "label": "Interpretación de Imágenes",
                "description": "Instrucciones para interpretar imágenes de resultados"
            },
            {
                "key": "welcome_message",
                "label": "Mensaje de Bienvenida",
                "description": "Mensaje que se muestra al iniciar una nueva conversación"
            }
        ]
    }


class PromptsUpdateRequest(BaseModel):
    prompts: dict


@app.post("/api/prompts")
async def update_prompts(request: PromptsUpdateRequest):
    """Update prompts configuration."""
    success = save_prompts(request.prompts)
    if success:
        # Reload to update cache
        reload_prompts()
        return {"success": True, "message": "Prompts actualizados correctamente"}
    else:
        raise HTTPException(status_code=500, detail="Error al guardar los prompts")


@app.get("/api/prompts/defaults")
async def get_default_prompts_endpoint():
    """Get default prompts configuration (for restore functionality)."""
    return {"prompts": get_default_prompts()}


# ============================================================
# IMAGE ROTATION DETECTION ENDPOINT
# ============================================================

class ImageRotationRequest(BaseModel):
    image: str  # base64 data (with or without data URL prefix)
    mimeType: str = "image/jpeg"


@app.post("/api/detect-rotation")
async def detect_image_rotation(request: ImageRotationRequest):
    """
    Detect if an image needs rotation correction using Gemini vision.
    Uses the existing key rotation system for rate limit handling.
    """
    import time
    from models import get_chat_model

    start_time = time.time()

    try:
        # Get Gemini model with key rotation - use gemini-3-flash-preview for best accuracy
        # Use minimal thinking for faster rotation detection while maintaining quality
        model = get_chat_model(provider="gemini", model_name="gemini-3-flash-preview", thinking_level="minimal")

        # Extract base64 data (remove data URL prefix if present)
        base64_data = request.image
        if base64_data.startswith("data:"):
            base64_data = base64_data.split(",", 1)[1]

        # Create message with image
        message = HumanMessage(content=[
            {
                "type": "text",
                "text": "Look at this image. If the image appears rotated or upside down, respond with ONLY a number: 90, 180, or 270 (degrees clockwise to fix it). If the image is correctly oriented, respond with ONLY: 0"
            },
            {
                "type": "image_url",
                "image_url": {"url": f"data:{request.mimeType};base64,{base64_data}"}
            }
        ])

        # Call model
        result = await model.ainvoke([message])

        # Track this API call against daily usage (rotation uses same API keys)
        from models import increment_usage
        increment_usage("gemini-3-flash-preview")

        # Handle Gemini 3 thinking response format
        # When include_thoughts=True, content is a list: [{'type': 'thinking', ...}, {'type': 'text', 'text': '...'}]
        content = result.content
        if isinstance(content, list):
            # Extract text from the response blocks
            response_text = ""
            for block in content:
                if isinstance(block, dict) and block.get("type") == "text":
                    response_text = block.get("text", "")
                    break
            logger.debug(f"[Rotation] Gemini 3 response blocks: {len(content)}, extracted text: {response_text[:50] if response_text else 'empty'}")
        else:
            response_text = content.strip() if content else ""

        # Parse rotation from response
        rotation = 0
        import re
        match = re.search(r'\b(0|90|180|270)\b', response_text)
        if match:
            rotation = int(match.group(1))

        elapsed_ms = int((time.time() - start_time) * 1000)
        logger.info(f"[Rotation] Gemini detected {rotation}° in {elapsed_ms}ms")

        return {
            "rotation": rotation,
            "detected": rotation != 0,
            "provider": "gemini",
            "timing": elapsed_ms,
            "raw": response_text
        }

    except Exception as e:
        elapsed_ms = int((time.time() - start_time) * 1000)
        import traceback
        logger.error(f"[Rotation] Gemini error ({elapsed_ms}ms): {e}\n{traceback.format_exc()}")
        return {
            "rotation": 0,
            "detected": False,
            "provider": "gemini",
            "error": str(e),
            "timing": elapsed_ms
        }


# ============================================================
# AI SDK DATA STREAM PROTOCOL ENDPOINT
# ============================================================

async def chat_aisdk_claude(request, thread_id: str, model_name: str):
    """
    Handle chat using Claude Code (Max subscription).

    This streams responses from Claude Code CLI using the claude-agent-sdk.
    No API key needed - uses your authenticated Claude Code installation.
    """
    global orders_context_sent

    logger.info(f"[Claude] Using model: {model_name}")
    logger.info(f"[Claude] Thread: {thread_id}, Messages: {len(request.messages)}")

    # Get Claude provider with Gemini fallback
    gemini_fallback = None
    if GEMINI_FALLBACK_MODEL in graphs:
        # Get the underlying model from the graph for fallback
        from models import get_chat_model
        gemini_fallback = get_chat_model(model_name=GEMINI_FALLBACK_MODEL)

    claude_provider = get_claude_provider(gemini_fallback=gemini_fallback)

    async def generate():
        adapter = StreamAdapter()

        try:
            # Check if first message and get context
            is_first_message = len(request.messages) <= 2
            if is_first_message:
                reset_tab_state()

            # Get context
            current_context = await get_orders_context(force_refresh=is_first_message)
            tabs_context = await get_browser_tabs_context()

            context_parts = []
            if is_first_message or not orders_context_sent:
                if current_context:
                    context_parts.append(current_context)
            if tabs_context:
                context_parts.append(tabs_context)

            context = "\n\n".join(context_parts) if context_parts else None

            # Start message
            yield adapter.start_message()

            # Stream from Claude
            full_response = []
            claude_responses = 0

            async for event in claude_provider.stream_chat(
                messages=request.messages,
                system_prompt=SYSTEM_PROMPT,
                context=context
            ):
                if event.type == "text":
                    full_response.append(event.content or "")
                    yield adapter.text_delta(event.content or "")
                    claude_responses += 1

                elif event.type == "thinking":
                    # Send thinking as reasoning (if frontend supports it)
                    yield adapter.text_delta(f"\n*[Thinking: {(event.content or '')[:100]}...]*\n")

                elif event.type == "tool_call":
                    yield adapter.tool_status(
                        event.tool_name or "unknown",
                        "start",
                        event.tool_input,
                        tool_call_id=event.tool_id
                    )

                elif event.type == "tool_result":
                    yield adapter.tool_status(
                        "tool",
                        "end",
                        tool_call_id=event.tool_id,
                        result=event.tool_output
                    )

                elif event.type == "error":
                    yield adapter.text_delta(f"\n*Error: {event.content}*\n")

                elif event.type == "done":
                    metadata = event.metadata or {}
                    if request.showStats:
                        # Show stats (cost is $0 with Max subscription)
                        cost = metadata.get("total_cost_usd", 0)
                        duration = metadata.get("duration_ms", 0)
                        turns = metadata.get("num_turns", 0)

                        stats_msg = f"\n\n---\n📊 **Stats**: Claude {model_name}"
                        if metadata.get("fallback"):
                            stats_msg += " (Gemini fallback)"
                        if duration:
                            stats_msg += f" | {duration/1000:.1f}s"
                        if turns:
                            stats_msg += f" | {turns} turns"
                        stats_msg += f" | ${cost:.4f} (Max subscription)"
                        stats_msg += "\n"

                        yield adapter.text_delta(stats_msg)

            # Finish
            yield adapter.finish("stop")

            # Log summary
            response_preview = ''.join(full_response)[:100]
            logger.info(f"[Claude] Done: {claude_responses} responses, preview: {response_preview}...")

        except Exception as e:
            logger.error(f"[Claude] Error: {e}", exc_info=True)
            yield adapter.error(str(e))
            yield adapter.finish("error")

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "x-vercel-ai-ui-message-stream": "v1",
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Chat-Id": thread_id,
        }
    )


class AISdkChatRequest(BaseModel):
    messages: List[dict]
    chatId: Optional[str] = None
    model: Optional[str] = None
    showStats: bool = True


@app.post("/api/chat/aisdk")
async def chat_aisdk(request: AISdkChatRequest):
    """
    AI SDK Data Stream Protocol v1 compatible endpoint.

    This endpoint streams responses in the Vercel AI SDK format,
    which can be consumed directly by useChat on the frontend.

    Supports both Claude (via Claude Code CLI) and Gemini (via LangGraph).
    Claude models use your Max subscription - no API key needed.

    Documentation: https://sdk.vercel.ai/docs/ai-sdk-ui/stream-protocol
    """
    global orders_context_sent

    thread_id = request.chatId or str(uuid.uuid4())

    # Determine model and provider
    model_name = request.model or DEFAULT_MODEL

    # Check if this is a Claude model
    is_claude_model = model_name in CLAUDE_MODELS

    # If Claude model requested but Claude Code not available, fall back to Gemini
    if is_claude_model:
        claude_provider = get_claude_provider()
        if not claude_provider.is_available:
            logger.warning(f"[AI SDK] Claude Code not available, falling back to {GEMINI_FALLBACK_MODEL}")
            model_name = GEMINI_FALLBACK_MODEL
            is_claude_model = False

    # Route to Claude or Gemini
    if is_claude_model:
        return await chat_aisdk_claude(request, thread_id, model_name)

    # Gemini/LangGraph path
    config = {"configurable": {"thread_id": thread_id}}

    # Select the appropriate graph based on requested model
    if model_name not in graphs:
        logger.warning(f"[AI SDK] Unknown model '{model_name}', using default: {GEMINI_FALLBACK_MODEL}")
        model_name = GEMINI_FALLBACK_MODEL
    graph = graphs[model_name]
    logger.info(f"[AI SDK] Using Gemini model: {model_name}")

    # Convert messages to LangChain format
    conversation_messages = []
    for msg in request.messages:
        role = msg.get("role", "")
        content = msg.get("content", "")

        # Handle multimodal content
        if isinstance(content, list):
            lc_content = []
            for part in content:
                if isinstance(part, dict):
                    if part.get("type") == "text":
                        lc_content.append({"type": "text", "text": part.get("text", "")})
                    elif part.get("type") == "image_url":
                        image_url = part.get("image_url", {})
                        url = image_url.get("url", "") if isinstance(image_url, dict) else image_url
                        lc_content.append({"type": "image_url", "image_url": {"url": url}})
                    elif part.get("type") == "media":
                        lc_content.append({
                            "type": "media",
                            "data": part.get("data", ""),
                            "mime_type": part.get("mime_type", "audio/webm")
                        })
            content = lc_content if lc_content else ""

        if role == "user":
            conversation_messages.append(HumanMessage(content=content))
        elif role == "assistant":
            conversation_messages.append(AIMessage(content=content))

    if not conversation_messages:
        logger.error("[AI SDK] No messages found in request")
        async def error_stream():
            yield StreamAdapter.error("No messages found")
            yield StreamAdapter.finish("error")
        return StreamingResponse(
            error_stream(),
            media_type="text/plain",
            headers={
                "x-vercel-ai-data-stream": "v1",
                "Cache-Control": "no-cache",
                "X-Chat-Id": thread_id,
            }
        )

    # Log request
    last_user_msg = None
    for msg in reversed(request.messages):
        if msg.get("role") == "user":
            last_user_msg = msg.get("content")
            break

    logger.info("=" * 60)
    if isinstance(last_user_msg, list):
        msg_parts = []
        for part in last_user_msg:
            if isinstance(part, dict):
                if part.get('type') == 'text':
                    msg_parts.append(part.get('text', '')[:100])
                elif part.get('type') == 'image_url':
                    # Log image size for debugging
                    image_url_data = part.get('image_url', {})
                    url = image_url_data.get('url', '') if isinstance(image_url_data, dict) else str(image_url_data)
                    if url.startswith('data:'):
                        base64_parts = url.split(',', 1)
                        if len(base64_parts) > 1:
                            base64_size = len(base64_parts[1])
                            approx_bytes = int(base64_size * 0.75)
                            msg_parts.append(f'[IMAGE: ~{approx_bytes // 1024}KB]')
                        else:
                            msg_parts.append('[IMAGE]')
                    else:
                        msg_parts.append('[IMAGE: URL]')
                elif part.get('type') == 'media':
                    msg_parts.append('[MEDIA]')
        user_msg_display = ' + '.join(msg_parts)
    else:
        user_msg_display = str(last_user_msg)[:200] if last_user_msg else ""
    logger.info(f"[AI SDK] USER: {user_msg_display}")
    logger.info(f"[AI SDK] Thread: {thread_id}, Messages: {len(conversation_messages)}")

    async def generate():
        try:
            # Check if first message and reset state
            is_first_message = len(conversation_messages) <= 2
            if is_first_message:
                reset_tab_state()

            # Get context (force refresh for new chats to get latest orders)
            current_context = await get_orders_context(force_refresh=is_first_message)
            tabs_context = await get_browser_tabs_context()

            # Build initial state
            initial_state = {"messages": conversation_messages}
            context_parts = []

            should_include_orders = is_first_message or not orders_context_sent
            if should_include_orders and current_context:
                context_parts.append(current_context)
            if tabs_context:
                context_parts.append(tabs_context)
            if context_parts:
                initial_state["current_page_context"] = "\n\n".join(context_parts)

            # Stream events using new AI SDK v6 protocol
            adapter = StreamAdapter()
            full_response = []
            total_input_tokens = 0
            total_output_tokens = 0
            ai_responses = 0  # Track actual AI responses (for usage tracking)
            counted_run_ids = set()  # Track which LLM calls we've already counted

            # Gemini pricing (per 1M tokens)
            # Gemini Flash: $0.075 input, $0.30 output
            INPUT_PRICE_PER_1M = 0.075
            OUTPUT_PRICE_PER_1M = 0.30

            # Import usage tracking
            from models import increment_usage, get_usage_stats, get_daily_limit

            # Start the message
            yield adapter.start_message()

            async for event in graph.astream_events(
                initial_state,
                config,
                version="v2"
            ):
                event_type = event.get("event", "")

                if event_type == "on_tool_start":
                    tool_name = event.get("name", "unknown")
                    tool_input = event.get("data", {}).get("input", {})
                    run_id = event.get("run_id", "")
                    tool_call_id = f"call_{run_id[:12]}" if run_id else None
                    # Don't log here - agent.py already logs consolidated summary
                    yield adapter.tool_status(tool_name, "start", tool_input, tool_call_id=tool_call_id)

                elif event_type == "on_tool_end":
                    tool_name = event.get("name", "unknown")
                    run_id = event.get("run_id", "")
                    tool_call_id = f"call_{run_id[:12]}" if run_id else None
                    tool_output = event.get("data", {}).get("output", "")

                    # Extract content from ToolMessage if it's a LangChain message object
                    if hasattr(tool_output, 'content'):
                        tool_output = tool_output.content

                    # Parse JSON string to object if possible (for ask_user, etc.)
                    result_data = tool_output
                    if isinstance(tool_output, str) and tool_output.startswith('{'):
                        try:
                            result_data = json.loads(tool_output)
                        except json.JSONDecodeError:
                            result_data = tool_output[:500] if tool_output else "completed"
                    elif tool_output:
                        result_data = str(tool_output)[:500]
                    else:
                        result_data = "completed"

                    # Don't log here - tools.py already logs details
                    yield adapter.tool_status(tool_name, "end", tool_call_id=tool_call_id, result=result_data)

                elif event_type == "on_chat_model_stream":
                    chunk = event["data"].get("chunk")
                    if chunk and hasattr(chunk, 'content') and chunk.content:
                        content = chunk.content
                        if isinstance(content, list):
                            text_parts = [p.get('text', '') for p in content if isinstance(p, dict) and p.get('type') == 'text']
                            content = ''.join(text_parts)
                        if content:
                            full_response.append(content)
                            yield adapter.text_delta(content)

                elif event_type == "on_chat_model_end":
                    run_id = event.get("run_id", "")
                    output = event.get("data", {}).get("output")

                    # Skip if we've already counted this run_id (avoid double-counting)
                    if run_id and run_id in counted_run_ids:
                        continue

                    if output:
                        # Only count if LLM returned actual tool_calls or content
                        has_tool_calls = hasattr(output, 'tool_calls') and output.tool_calls
                        has_content = False
                        if hasattr(output, 'content') and output.content:
                            content = output.content
                            if isinstance(content, str):
                                has_content = bool(content.strip())
                            elif isinstance(content, list):
                                # Check for text content in list format (Gemini 3)
                                has_content = any(
                                    isinstance(p, dict) and p.get('type') == 'text' and p.get('text', '').strip()
                                    for p in content
                                )

                        if has_tool_calls or has_content:
                            if run_id:
                                counted_run_ids.add(run_id)
                            ai_responses += 1
                            increment_usage(model_name)

                        # Handle usage metadata
                        usage = getattr(output, 'usage_metadata', None)
                        if usage and isinstance(usage, dict):
                            total_input_tokens += usage.get('input_tokens', 0) or usage.get('prompt_token_count', 0) or 0
                            total_output_tokens += usage.get('output_tokens', 0) or usage.get('candidates_token_count', 0) or 0

                        # If streaming didn't happen, yield the full content here
                        if not full_response and hasattr(output, 'content') and output.content:
                            content = output.content
                            if isinstance(content, str) and content:
                                full_response.append(content)
                                yield adapter.text_delta(content)
                            elif isinstance(content, list):
                                text_parts = [p.get('text', '') for p in content if isinstance(p, dict) and p.get('type') == 'text']
                                text = ''.join(text_parts)
                                if text:
                                    full_response.append(text)
                                    yield adapter.text_delta(text)

            # Calculate and send usage summary as text (if enabled)
            total_tokens = total_input_tokens + total_output_tokens
            input_cost = (total_input_tokens / 1_000_000) * INPUT_PRICE_PER_1M
            output_cost = (total_output_tokens / 1_000_000) * OUTPUT_PRICE_PER_1M
            total_cost = input_cost + output_cost

            if request.showStats:
                # Get current usage stats for this model
                usage_stats = get_usage_stats()
                daily_limit = get_daily_limit()
                model_usage = usage_stats.get("models", {}).get(model_name, 0)

                # Send usage summary at the end
                usage_summary = f"\n\n---\n📊 **Stats**: {ai_responses} prompts ({model_usage}/{daily_limit} today)"
                if total_tokens > 0:
                    usage_summary += f" | Tokens: {total_input_tokens:,} in + {total_output_tokens:,} out"
                    usage_summary += f" | ${total_cost:.6f}"
                usage_summary += "\n"

                yield adapter.text_delta(usage_summary)

            # Send finish with usage
            usage = None
            if total_input_tokens or total_output_tokens:
                usage = {
                    "promptTokens": total_input_tokens,
                    "completionTokens": total_output_tokens,
                    "totalTokens": total_input_tokens + total_output_tokens
                }
            yield adapter.finish("stop", usage)

            # Log summary
            response_preview = ''.join(full_response)[:100]
            logger.info(f"[AI SDK] Done: {ai_responses} AI responses, {total_tokens} tokens, response: {response_preview}...")

        except Exception as e:
            logger.error(f"[AI SDK] Error: {e}", exc_info=True)
            adapter = StreamAdapter()
            yield adapter.error(str(e))
            yield adapter.finish("error")

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "x-vercel-ai-ui-message-stream": "v1",
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Chat-Id": thread_id,
        }
    )


# ============================================================
# OPENAI-COMPATIBLE ENDPOINT (Optional - for LobeChat integration)
# ============================================================

class OpenAIChatRequest(BaseModel):
    model: str
    messages: List[dict]
    stream: bool = False
    temperature: float = 0.7


@app.post("/v1/chat/completions")
async def openai_compatible_chat(request: OpenAIChatRequest):
    """
    OpenAI-compatible chat completions endpoint.

    This translates OpenAI format to our LangGraph agent format,
    allowing LobeChat to use our agent as a model provider.
    """
    import time as time_module

    # Debug: log raw request
    logger.debug(f"[Request] Messages count: {len(request.messages)}")
    for i, msg in enumerate(request.messages):
        content = msg.get('content', '')
        # Handle multimodal content (list with text/images/audio)
        if isinstance(content, list):
            content_summary = []
            for part in content:
                if isinstance(part, dict):
                    if part.get('type') == 'text':
                        content_summary.append(f"text:{part.get('text', '')[:50]}")
                    elif part.get('type') == 'image_url':
                        content_summary.append("[IMAGE]")
                    elif part.get('type') == 'media':
                        mime = part.get('mime_type', 'unknown')
                        content_summary.append(f"[AUDIO:{mime}]" if 'audio' in mime else f"[VIDEO:{mime}]")
                    else:
                        content_summary.append(f"[{part.get('type', 'unknown')}]")
                else:
                    content_summary.append(str(part)[:30])
            content_str = ', '.join(content_summary)
        else:
            content_str = str(content)[:100]
        logger.debug(f"[Request] [{i}] role={msg.get('role')}, content={content_str}")

    # Detect and reject LobeChat's auxiliary requests (topic naming, translation, etc.)
    # These have role=developer/system with summarization/translation prompts
    for msg in request.messages:
        role = msg.get("role", "")
        if role in ["developer", "system"]:
            content = str(msg.get("content", "")).lower()
            if any(keyword in content for keyword in ["summarizer", "summarize", "title", "translate", "translation", "compress"]):
                logger.info(f"[Request] Skipping auxiliary request (topic naming/translation)")
                # Return a simple response without invoking the agent
                response_id = f"chatcmpl-{uuid.uuid4().hex[:8]}"
                created_time = int(time_module.time())
                if request.stream:
                    async def simple_stream():
                        data = {
                            "id": response_id,
                            "object": "chat.completion.chunk",
                            "created": created_time,
                            "model": request.model,
                            "choices": [{"index": 0, "delta": {"content": "Lab Assistant"}, "finish_reason": None}]
                        }
                        yield f"data: {json.dumps(data)}\n\n"
                        final = {
                            "id": response_id,
                            "object": "chat.completion.chunk",
                            "created": created_time,
                            "model": request.model,
                            "choices": [{"index": 0, "delta": {}, "finish_reason": "stop"}]
                        }
                        yield f"data: {json.dumps(final)}\n\n"
                        yield "data: [DONE]\n\n"
                    return StreamingResponse(simple_stream(), media_type="text/event-stream")
                else:
                    return {
                        "id": response_id,
                        "object": "chat.completion",
                        "created": created_time,
                        "model": request.model,
                        "choices": [{"index": 0, "message": {"role": "assistant", "content": "Lab Assistant"}, "finish_reason": "stop"}]
                    }

    thread_id = str(uuid.uuid4())
    config = {"configurable": {"thread_id": thread_id}}

    # Select the appropriate graph based on requested model
    # The request.model from OpenAI format may differ from our internal model names
    model_name = request.model if request.model in graphs else DEFAULT_MODEL
    graph = graphs[model_name]
    logger.info(f"[OpenAI] Using model: {model_name}")

    # Convert OpenAI-format messages to LangGraph messages
    # This preserves full conversation history from the frontend
    conversation_messages = []
    for msg in request.messages:
        role = msg.get("role", "")
        content = msg.get("content", "")

        # Handle multimodal content (images, audio, video) - convert to LangChain format
        if isinstance(content, list):
            # Convert OpenAI multimodal format to LangChain format
            lc_content = []
            for part in content:
                if isinstance(part, dict):
                    if part.get("type") == "text":
                        lc_content.append({"type": "text", "text": part.get("text", "")})
                    elif part.get("type") == "image_url":
                        image_url = part.get("image_url", {})
                        url = image_url.get("url", "") if isinstance(image_url, dict) else image_url
                        lc_content.append({"type": "image_url", "image_url": {"url": url}})
                    elif part.get("type") == "media":
                        # Audio/video for Gemini (native format)
                        lc_content.append({
                            "type": "media",
                            "data": part.get("data", ""),
                            "mime_type": part.get("mime_type", "audio/webm")
                        })
            content = lc_content if lc_content else ""

        if role == "user":
            conversation_messages.append(HumanMessage(content=content))
        elif role == "assistant":
            conversation_messages.append(AIMessage(content=content))
        # Skip system/developer roles - they're handled separately

    if not conversation_messages:
        logger.error("No messages found in request")
        return {"error": "No messages found"}

    # Get the last user message for logging
    last_user_message = None
    for msg in reversed(request.messages):
        if msg["role"] == "user":
            last_user_message = msg["content"]
            break

    logger.info("=" * 60)
    # Handle multimodal user message for logging
    if isinstance(last_user_message, list):
        msg_parts = []
        for part in last_user_message:
            if isinstance(part, dict):
                if part.get('type') == 'text':
                    msg_parts.append(part.get('text', '')[:100])
                elif part.get('type') == 'image_url':
                    # Log image size for debugging
                    image_url_data = part.get('image_url', {})
                    url = image_url_data.get('url', '') if isinstance(image_url_data, dict) else str(image_url_data)
                    if url.startswith('data:'):
                        # Extract base64 size to estimate original image size
                        base64_parts = url.split(',', 1)
                        if len(base64_parts) > 1:
                            base64_size = len(base64_parts[1])
                            approx_bytes = int(base64_size * 0.75)  # base64 is ~33% larger
                            msg_parts.append(f'[IMAGE: ~{approx_bytes // 1024}KB]')
                        else:
                            msg_parts.append('[IMAGE]')
                    else:
                        msg_parts.append('[IMAGE: URL]')
                elif part.get('type') == 'media':
                    mime = part.get('mime_type', 'unknown')
                    msg_parts.append(f'[AUDIO:{mime}]' if 'audio' in mime else f'[VIDEO:{mime}]')
                else:
                    msg_parts.append(f"[{part.get('type', 'unknown')}]")
        user_msg_display = ' + '.join(msg_parts)
    else:
        user_msg_display = str(last_user_message)[:200]
    logger.info(f"USER MESSAGE: {user_msg_display}{'...' if len(str(last_user_message)) > 200 else ''}")
    logger.info(f"Thread ID: {thread_id}, Stream: {request.stream}, History: {len(conversation_messages)} messages")

    if request.stream:
        async def generate():
            global orders_context_sent
            full_response = []
            response_id = f"chatcmpl-{uuid.uuid4().hex[:8]}"  # Same ID for all chunks
            created_time = int(time_module.time())  # Unix timestamp

            # Track AI responses (for usage tracking)
            ai_responses = 0
            total_input_tokens = 0
            total_output_tokens = 0

            # Import usage tracking
            from models import increment_usage, get_usage_stats, get_daily_limit

            # Gemini pricing (per 1M tokens) - adjust based on model
            # Gemini 3 Flash Preview: $0.50 input, $3.00 output (incl. thinking tokens)
            INPUT_PRICE_PER_1M = 0.50
            OUTPUT_PRICE_PER_1M = 3.00

            try:
                # Check if this is first message in conversation (from frontend history)
                is_first_message = len(conversation_messages) <= 2  # Just system + first user msg

                # Reset tab state tracking on new conversation
                if is_first_message:
                    reset_tab_state()

                # Get current context (force refresh for new chats to get latest orders)
                current_context = await get_orders_context(force_refresh=is_first_message)

                # Get browser tabs context (always at each user message)
                tabs_context = await get_browser_tabs_context()

                # Pass full conversation history to the graph
                initial_state = {"messages": conversation_messages}

                # Build full context
                context_parts = []

                # Include orders context if: first message OR we haven't sent valid orders yet
                should_include_orders = is_first_message or not orders_context_sent
                if should_include_orders and current_context:
                    context_parts.append(current_context)
                    if "SESIÓN NO INICIADA" in current_context:
                        logger.info("[Chat] Not logged in - sending login reminder")
                    else:
                        logger.info("[Chat] Including orders context")

                # Always include tabs context at every message
                if tabs_context:
                    context_parts.append(tabs_context)
                    logger.info("[Chat] Including browser tabs context")

                if context_parts:
                    initial_state["current_page_context"] = "\n\n".join(context_parts)

                async for event in graph.astream_events(
                    initial_state,
                    config,
                    version="v2"
                ):
                    event_type = event.get("event", "")

                    # Note: AI response counting now done in on_chat_model_end
                    if event_type == "on_chat_model_start":
                        pass  # Used for debugging only if needed

                    # Stream tool calls to show "thinking" in LobeChat
                    if event_type == "on_tool_start":
                        tool_name = event.get("name", "unknown")
                        tool_input = event.get("data", {}).get("input", {})
                        logger.info(f"TOOL CALL: {tool_name}")
                        logger.debug(f"  Input: {json.dumps(tool_input, ensure_ascii=False)[:500]}")

                        # Send tool call as a "thinking" step to frontend
                        tool_display = f"🔧 **{tool_name}**"
                        if tool_input:
                            # Show ALL parameters
                            params = []
                            for k, v in tool_input.items():
                                if isinstance(v, str):
                                    # Truncate long strings
                                    display_v = v if len(v) < 50 else v[:47] + "..."
                                    params.append(f"{k}={display_v}")
                                elif isinstance(v, list):
                                    # Show list with all items (truncate if too many)
                                    if len(v) <= 10:
                                        params.append(f"{k}={v}")
                                    else:
                                        params.append(f"{k}=[{', '.join(str(x) for x in v[:10])}... +{len(v)-10} more]")
                                elif isinstance(v, (int, float, bool)):
                                    params.append(f"{k}={v}")
                            if params:
                                tool_display += f" ({', '.join(params)})"
                        tool_display += "\n"

                        data = {
                            "id": response_id,
                            "object": "chat.completion.chunk",
                            "created": created_time,
                            "model": request.model,
                            "choices": [{
                                "index": 0,
                                "delta": {"content": tool_display},
                                "finish_reason": None
                            }]
                        }
                        yield f"data: {json.dumps(data)}\n\n"

                    elif event_type == "on_tool_end":
                        tool_name = event.get("name", "unknown")
                        tool_output = event.get("data", {}).get("output", "")
                        logger.info(f"TOOL RESULT: {tool_name}")
                        logger.debug(f"  Output: {str(tool_output)[:500]}")

                        # Send brief result indicator
                        result_display = f"✓ {tool_name} completado\n\n"
                        data = {
                            "id": response_id,
                            "object": "chat.completion.chunk",
                            "created": created_time,
                            "model": request.model,
                            "choices": [{
                                "index": 0,
                                "delta": {"content": result_display},
                                "finish_reason": None
                            }]
                        }
                        yield f"data: {json.dumps(data)}\n\n"

                    elif event_type == "on_chat_model_stream":
                        chunk = event["data"].get("chunk")
                        if chunk and hasattr(chunk, 'content') and chunk.content:
                            # Handle both string and list content (Gemini 3 with thinking)
                            content = chunk.content
                            if isinstance(content, list):
                                # Extract text parts only, skip thinking parts
                                text_parts = [p.get('text', '') for p in content if isinstance(p, dict) and p.get('type') == 'text']
                                content = ''.join(text_parts)
                            if content:
                                full_response.append(content)
                                data = {
                                    "id": response_id,
                                    "object": "chat.completion.chunk",
                                    "created": created_time,
                                    "model": request.model,
                                    "choices": [{
                                        "index": 0,
                                        "delta": {"content": content},
                                        "finish_reason": None
                                    }]
                                }
                                yield f"data: {json.dumps(data)}\n\n"

                    # Handle non-streaming model responses (after key rotation) and extract token usage
                    elif event_type == "on_chat_model_end":
                        output = event.get("data", {}).get("output")

                        if output:
                            # Count this as an AI response and increment usage
                            ai_responses += 1
                            increment_usage(model_name)

                            # Try to extract token usage from response
                            # Check for usage_metadata on AIMessage (it's a dict, not object)
                            # LangChain format: {'input_tokens': X, 'output_tokens': Y, 'total_tokens': Z}
                            usage = getattr(output, 'usage_metadata', None)
                            if usage and isinstance(usage, dict):
                                input_tokens = usage.get('input_tokens', 0) or usage.get('prompt_token_count', 0) or 0
                                output_tokens = usage.get('output_tokens', 0) or usage.get('candidates_token_count', 0) or 0
                                if input_tokens or output_tokens:
                                    total_input_tokens += input_tokens
                                    total_output_tokens += output_tokens
                                    # Log thinking tokens if available
                                    output_details = usage.get('output_token_details', {})
                                    thinking_tokens = output_details.get('reasoning', 0) if output_details else 0
                                    logger.info(f"[AI {ai_responses}] Tokens: in={input_tokens}, out={output_tokens}" +
                                               (f" (thinking={thinking_tokens})" if thinking_tokens else ""))

                            # Also check response_metadata for langchain (Google-specific format)
                            resp_meta = getattr(output, 'response_metadata', {}) or {}
                            if resp_meta and 'usage_metadata' in resp_meta:
                                usage_meta = resp_meta['usage_metadata']
                                # Google format: prompt_token_count, candidates_token_count
                                input_tokens = usage_meta.get('prompt_token_count', 0)
                                output_tokens = usage_meta.get('candidates_token_count', 0)
                                # Avoid double counting - only add if we didn't get from usage_metadata above
                                if (input_tokens or output_tokens) and not usage:
                                    total_input_tokens += input_tokens
                                    total_output_tokens += output_tokens
                                    logger.info(f"[AI {ai_responses}] Tokens (from response_meta): in={input_tokens}, out={output_tokens}")

                        if output and hasattr(output, 'content') and output.content:
                            content = output.content
                            if isinstance(content, list):
                                text_parts = [p.get('text', '') for p in content if isinstance(p, dict) and p.get('type') == 'text']
                                content = ''.join(text_parts)
                            # Only send if we haven't already streamed this content
                            if content and content not in ''.join(full_response):
                                full_response.append(content)
                                data = {
                                    "id": response_id,
                                    "object": "chat.completion.chunk",
                                    "created": created_time,
                                    "model": request.model,
                                    "choices": [{
                                        "index": 0,
                                        "delta": {"content": content},
                                        "finish_reason": None
                                    }]
                                }
                                yield f"data: {json.dumps(data)}\n\n"

                if full_response:
                    logger.info(f"AI RESPONSE: {''.join(full_response)[:300]}{'...' if len(''.join(full_response)) > 300 else ''}")

                # Calculate and display usage summary
                total_tokens = total_input_tokens + total_output_tokens
                input_cost = (total_input_tokens / 1_000_000) * INPUT_PRICE_PER_1M
                output_cost = (total_output_tokens / 1_000_000) * OUTPUT_PRICE_PER_1M
                total_cost = input_cost + output_cost

                # Get current usage stats for this model
                usage_stats = get_usage_stats()
                daily_limit = get_daily_limit()
                model_usage = usage_stats.get("models", {}).get(model_name, 0)

                # Send usage summary at the end
                usage_summary = f"\n\n---\n📊 **Stats**: {ai_responses} prompts ({model_usage}/{daily_limit} today)"
                if total_tokens > 0:
                    usage_summary += f" | Tokens: {total_input_tokens:,} in + {total_output_tokens:,} out"
                    usage_summary += f" | ${total_cost:.6f}"
                usage_summary += "\n"

                data = {
                    "id": response_id,
                    "object": "chat.completion.chunk",
                    "created": created_time,
                    "model": request.model,
                    "choices": [{
                        "index": 0,
                        "delta": {"content": usage_summary},
                        "finish_reason": None
                    }]
                }
                yield f"data: {json.dumps(data)}\n\n"

                logger.info(f"[Usage] AI responses: {ai_responses}, Input: {total_input_tokens}, Output: {total_output_tokens}, Cost: ${total_cost:.6f}")

                # Send final chunk with finish_reason to signal completion
                final_chunk = {
                    "id": response_id,
                    "object": "chat.completion.chunk",
                    "created": created_time,
                    "model": request.model,
                    "choices": [{
                        "index": 0,
                        "delta": {},
                        "finish_reason": "stop"
                    }]
                }
                yield f"data: {json.dumps(final_chunk)}\n\n"
                yield "data: [DONE]\n\n"

            except Exception as e:
                logger.error(f"Stream error: {str(e)}", exc_info=True)
                yield f"data: {json.dumps({'error': str(e)})}\n\n"

        return StreamingResponse(
            generate(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",  # Disable nginx buffering
            }
        )

    else:
        try:
            # Check if this is first message in conversation (from frontend history)
            is_first_message = len(conversation_messages) <= 2  # Just system + first user msg

            # Reset tab state tracking on new conversation
            if is_first_message:
                reset_tab_state()

            # Get current context (force refresh for new chats to get latest orders)
            current_context = await get_orders_context(force_refresh=is_first_message)

            # Get browser tabs context (always at each user message)
            tabs_context = await get_browser_tabs_context()

            # Pass full conversation history to the graph
            initial_state = {"messages": conversation_messages}

            # Build full context
            context_parts = []

            # Include orders context if: first message OR we haven't sent valid orders yet
            should_include_orders = is_first_message or not orders_context_sent
            if should_include_orders and current_context:
                context_parts.append(current_context)
                if "SESIÓN NO INICIADA" in current_context:
                    logger.info("[Chat] Not logged in - sending login reminder")
                else:
                    logger.info("[Chat] Including orders context")

            # Always include tabs context at every message
            if tabs_context:
                context_parts.append(tabs_context)
                logger.info("[Chat] Including browser tabs context")

            if context_parts:
                initial_state["current_page_context"] = "\n\n".join(context_parts)

            logger.info("Invoking LangGraph agent...")
            result = await graph.ainvoke(initial_state, config)

            # Log all messages for debugging
            for i, msg in enumerate(result["messages"]):
                msg_type = type(msg).__name__
                content = msg.content if hasattr(msg, 'content') else str(msg)
                if hasattr(msg, 'tool_calls') and msg.tool_calls:
                    logger.info(f"  [{i}] {msg_type}: {len(msg.tool_calls)} tool calls")
                    for tc in msg.tool_calls:
                        logger.info(f"      -> {tc.get('name', 'unknown')}: {json.dumps(tc.get('args', {}), ensure_ascii=False)[:200]}")
                else:
                    logger.info(f"  [{i}] {msg_type}: {content[:150]}{'...' if len(content) > 150 else ''}")

            last_msg = result["messages"][-1]
            response_text = last_msg.content if hasattr(last_msg, 'content') else str(last_msg)

            logger.info(f"AI RESPONSE: {response_text[:300]}{'...' if len(response_text) > 300 else ''}")
            logger.info("=" * 60)

            return {
                "id": f"chatcmpl-{uuid.uuid4().hex[:8]}",
                "object": "chat.completion",
                "model": request.model,
                "choices": [{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": response_text
                    },
                    "finish_reason": "stop"
                }]
            }
        except Exception as e:
            logger.error(f"Error invoking agent: {str(e)}", exc_info=True)
            return {
                "id": f"chatcmpl-{uuid.uuid4().hex[:8]}",
                "object": "chat.completion",
                "model": request.model,
                "choices": [{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": f"Error: {str(e)}"
                    },
                    "finish_reason": "stop"
                }]
            }


# ============================================================
# MAIN
# ============================================================

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "server:app",
        host=settings.host,
        port=settings.port,
        reload=settings.debug
    )
