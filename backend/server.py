"""
FastAPI server with LangGraph integration.

DOCUMENTATION:
- LangGraph + FastAPI: https://langchain-ai.github.io/langgraph/how-tos/deploy-self-hosted/
- Streaming: https://langchain-ai.github.io/langgraph/concepts/streaming/
- Checkpointing: https://langchain-ai.github.io/langgraph/concepts/persistence/

ENDPOINTS:
- POST /api/chat: Send message, get response
- GET /api/chat/{thread_id}/history: Get conversation history
- GET /api/browser/screenshot: Get current browser state
- GET /api/health: Health check

NO APPROVAL ENDPOINTS NEEDED - Website's Save button is the human-in-the-loop.
"""
import os
import sys
import asyncio
import base64
import uuid
import json
import logging
import re
import csv
from datetime import datetime
from typing import Optional, List
from contextlib import asynccontextmanager
from pathlib import Path

# Configure logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

# Debug request storage directory
DEBUG_DIR = Path(__file__).parent / "data" / "requests"

# Exams list file (generated by scripts/process_tarifas.py)
EXAMS_FILE = Path(__file__).parent / "config" / "lista_de_examenes.csv"

# Reduce noise from asyncio proactor logs (Windows-specific spam)
logging.getLogger("asyncio").setLevel(logging.WARNING)

# Set up Windows event loop policy if on Windows
if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from langchain_core.messages import HumanMessage, AIMessage

# LangGraph imports
from langgraph.checkpoint.memory import MemorySaver
# For production with SQLite:
# from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver

# Local imports
from graph.agent import create_lab_agent, compile_agent
from graph.tools import set_browser, close_all_tabs, get_active_tabs, _get_browser_tabs_impl, reset_tab_state, ALL_TOOLS
from browser_manager import BrowserManager
from extractors import EXTRACT_ORDENES_JS
from config import settings
from prompts import SYSTEM_PROMPT


# ============================================================
# DEBUG REQUEST LOGGING
# ============================================================

def get_tools_schema() -> List[dict]:
    """Get schema definitions for all tools (for debug logging)."""
    schemas = []
    for tool in ALL_TOOLS:
        schema = {
            "name": tool.name,
            "description": tool.description,
        }
        # Get input schema if available
        if hasattr(tool, 'args_schema') and tool.args_schema:
            try:
                params = tool.args_schema.model_json_schema()
                # Remove duplicate description from parameters (Pydantic includes it)
                params.pop('description', None)
                schema["parameters"] = params
            except Exception:
                pass
        schemas.append(schema)
    return schemas


def summarize_media_content(obj):
    """Replace large base64 content with summaries for logging."""
    if isinstance(obj, str):
        if len(obj) > 200 and ('base64' in obj[:50].lower() or obj.startswith('data:')):
            return f"[BASE64: {len(obj)} chars]"
        return obj
    if isinstance(obj, list):
        return [summarize_media_content(item) for item in obj]
    if isinstance(obj, dict):
        result = {}
        for key, value in obj.items():
            if key in ('data', 'url') and isinstance(value, str) and len(value) > 200:
                if value.startswith('data:'):
                    mime_match = re.match(r'^data:([^;]+)', value)
                    result[key] = f"[DATA URL: {mime_match.group(1) if mime_match else 'unknown'}, {len(value)} chars]"
                else:
                    result[key] = f"[BASE64: {len(value)} chars]"
            else:
                result[key] = summarize_media_content(value)
        return result
    return obj


def save_debug_request(
    thread_id: str,
    raw_messages: List[dict],
    converted_messages: List,
    context: Optional[str] = None
) -> str:
    """
    Save full request details to a JSON file for debugging.

    This logs the COMPLETE request including:
    - System prompt
    - Tools definitions
    - Current page context (orders, tabs)
    - All conversation messages

    Returns the debug ID.
    """
    DEBUG_DIR.mkdir(parents=True, exist_ok=True)

    # Generate timestamped ID: YYYYMMDD_HHMMSS_randomchars
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    debug_id = f"req_{timestamp}_{uuid.uuid4().hex[:8]}"

    # Summarize messages to avoid huge base64 data
    summarized_raw = summarize_media_content(raw_messages)

    # Convert LangChain messages to serializable format
    serialized_messages = []
    for msg in converted_messages:
        msg_data = {
            "type": type(msg).__name__,
            "content": summarize_media_content(msg.content) if hasattr(msg, 'content') else str(msg),
        }
        if hasattr(msg, 'additional_kwargs') and msg.additional_kwargs:
            msg_data["additional_kwargs"] = msg.additional_kwargs
        serialized_messages.append(msg_data)

    debug_data = {
        "id": debug_id,
        "thread_id": thread_id,
        "timestamp": datetime.now().isoformat(),
        "system_prompt": SYSTEM_PROMPT,
        "tools": get_tools_schema(),
        "current_page_context": context,
        "raw_frontend_messages": summarized_raw,
        "converted_langchain_messages": serialized_messages,
    }

    filepath = DEBUG_DIR / f"{debug_id}.json"
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(debug_data, f, ensure_ascii=False, indent=2)

    logger.info(f"[Debug] Saved request to {filepath.name}")
    return debug_id


def update_debug_request(debug_id: str, response: str, error: Optional[str] = None):
    """Update debug request with response."""
    filepath = DEBUG_DIR / f"{debug_id}.json"
    if filepath.exists():
        with open(filepath, "r", encoding="utf-8") as f:
            data = json.load(f)
        data["response"] = response[:5000] if response else None
        data["error"] = error
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)


def load_exams_from_csv() -> List[dict]:
    """
    Load available exams from CSV file.
    Returns list of exams with codigo, nombre, and prices.
    """
    if not EXAMS_FILE.exists():
        logger.warning(f"[Exams] File not found: {EXAMS_FILE}")
        return []

    exams = []
    with open(EXAMS_FILE, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            exams.append({
                'codigo': row.get('codigo', ''),
                'nombre': row.get('nombre', ''),
                'precio': float(row.get('precio_particular', 0) or 0),
                'precio_desc': float(row.get('precio_descuento', 0) or 0),
            })

    logger.info(f"[Exams] Loaded {len(exams)} exams from {EXAMS_FILE.name}")
    return exams


# Cache exams at module level (loaded once at startup)
_cached_exams: List[dict] = []


def get_available_exams_context() -> str:
    """Generate context string for available exams."""
    global _cached_exams

    if not _cached_exams:
        _cached_exams = load_exams_from_csv()

    if not _cached_exams:
        return ""

    lines = ["# ExÃ¡menes Disponibles (CÃ³digo - Nombre - Precio)"]
    for exam in _cached_exams:
        precio = f"${exam['precio']:.2f}" if exam['precio'] else "N/A"
        lines.append(f"- {exam['codigo']}: {exam['nombre'][:45]} ({precio})")

    lines.append("")
    lines.append("*Para cotizaciÃ³n: create_new_order(cedula=\"\", exams=[\"CODIGO1\",...])*")

    return "\n".join(lines)


# ============================================================
# PYDANTIC MODELS
# ============================================================

class ChatRequest(BaseModel):
    message: str
    thread_id: Optional[str] = None


class ChatResponse(BaseModel):
    status: str  # "complete", "error"
    message: str
    thread_id: str
    iterations: Optional[int] = None


# ============================================================
# GLOBAL STATE
# ============================================================

browser: Optional[BrowserManager] = None
graph = None
checkpointer = None
initial_orders_context: str = ""  # Store initial orders for context
orders_context_sent: bool = False  # Track if we've sent valid orders context


# ============================================================
# HELPER FUNCTIONS
# ============================================================

def is_logged_in() -> bool:
    """Check if the browser is logged in (not on login page)."""
    if browser and browser.page:
        return "/login" not in browser.page.url
    return False


async def get_orders_context() -> str:
    """
    Get orders context, checking login state first.
    Returns empty string with login message if not logged in.
    Returns orders table if logged in and orders found.
    """
    global browser, initial_orders_context, orders_context_sent

    if not is_logged_in():
        logger.info("[Context] User not logged in - browser is on login page")
        return "âš ï¸ SESIÃ“N NO INICIADA: El navegador estÃ¡ en la pÃ¡gina de login. Por favor, inicia sesiÃ³n en el navegador para que pueda acceder a las Ã³rdenes del laboratorio."

    # If we haven't sent valid orders yet, try to extract them
    if not orders_context_sent or not initial_orders_context:
        logger.info("[Context] Extracting orders context...")
        initial_orders_context = await extract_initial_context()
        if initial_orders_context and "Ã“rdenes Recientes" in initial_orders_context:
            orders_context_sent = True
            logger.info(f"[Context] Extracted {initial_orders_context.count('|') // 8} orders")

    return initial_orders_context


async def get_browser_tabs_context() -> str:
    """
    Get browser tabs context with state tracking.

    - Shows all tabs with IDs, patient names, and enumeration for duplicates
    - For NEW tabs: shows full state (exams, fields, etc.)
    - For KNOWN tabs: shows only what CHANGED since last message
    - Marks which tab is active
    """
    try:
        tabs_info = await _get_browser_tabs_impl()

        if not tabs_info.get("tabs"):
            return ""

        lines = ["# PestaÃ±as del Navegador"]

        type_display = {
            "ordenes_list": "Lista de Ã“rdenes",
            "nueva_orden": "Nueva Orden",
            "orden_edit": "Editar Orden",
            "resultados": "Resultados",
            "login": "Login",
            "unknown": "Otra"
        }

        for idx, tab in enumerate(tabs_info.get("tabs", [])):
            tab_type = tab.get("type", "unknown")
            is_active = tab.get("active", False)
            is_new = tab.get("is_new", False)
            tab_id = tab.get("id")
            paciente = tab.get("paciente")
            instance = tab.get("instance")  # For duplicate enumeration
            state = tab.get("state")  # Full state for new tabs
            changes = tab.get("changes")  # Only changes for known tabs

            # Build tab header - always include tab_index
            marker = "â†’ " if is_active else "  "
            tab_line = f"{marker}[tab_index={idx}] {type_display.get(tab_type, tab_type)}"

            # Add ID for saved orders
            if tab_id:
                if tab_type == "resultados":
                    tab_line += f" (order_num={tab_id})"
                elif tab_type == "orden_edit":
                    tab_line += f" (order_id={tab_id})"

            # Add instance number for duplicates
            if instance:
                tab_line += f" #{instance}"

            # Add patient name
            if paciente:
                tab_line += f" - {paciente[:25]}"

            # Add NEW marker for unsaved orders
            if is_new:
                tab_line += " [NUEVA - sin guardar]"

            lines.append(tab_line)

            # For new tabs, show full state
            if is_new and state:
                if tab_type == "resultados":
                    lines.append(f"    ExÃ¡menes: {state.get('examenes_count', 0)}")
                    # Show field values (limited)
                    field_values = state.get("field_values", {})
                    filled = [(k, v) for k, v in field_values.items() if v]
                    if filled:
                        lines.append(f"    Campos con valor: {len(filled)}")
                elif tab_type in ["orden_edit", "nueva_orden"]:
                    exams = state.get("exams", [])
                    if exams:
                        lines.append(f"    ExÃ¡menes: {', '.join(exams[:8])}")
                    if state.get("total"):
                        lines.append(f"    Total: {state.get('total')}")

            # For known tabs, show only changes
            elif changes:
                lines.append("    **Cambios detectados:**")
                if "field_values" in changes:
                    # Show which fields changed
                    for field_key, new_value in list(changes["field_values"].items())[:5]:
                        if new_value:
                            lines.append(f"    - {field_key}: â†’ {new_value}")
                if "exams" in changes:
                    lines.append(f"    - ExÃ¡menes: {', '.join(changes['exams'][:5])}")
                if "total" in changes:
                    lines.append(f"    - Total: {changes['total']}")

        return "\n".join(lines)

    except Exception as e:
        logger.warning(f"[Context] Could not get browser tabs context: {e}")
        return ""


# ============================================================
# LIFESPAN
# ============================================================

async def extract_initial_context() -> str:
    """Extract initial orders list and available exams from the page for AI context."""
    global browser
    lines = []

    try:
        # Fetch orders from page 1
        page1_url = "https://laboratoriofranz.orion-labs.com/ordenes?page=1"
        logger.info("[Context] Fetching orders page 1...")
        await browser.page.goto(page1_url, timeout=30000)
        await browser.page.wait_for_timeout(2000)
        ordenes_page1 = await browser.page.evaluate(EXTRACT_ORDENES_JS) or []

        # Fetch orders from page 2
        page2_url = "https://laboratoriofranz.orion-labs.com/ordenes?page=2"
        logger.info("[Context] Fetching orders page 2...")
        await browser.page.goto(page2_url, timeout=30000)
        await browser.page.wait_for_timeout(2000)
        ordenes_page2 = await browser.page.evaluate(EXTRACT_ORDENES_JS) or []

        # Combine orders from both pages
        all_ordenes = ordenes_page1 + ordenes_page2

        if all_ordenes:
            lines.append("# Ã“rdenes Recientes (40 mÃ¡s recientes)")
            lines.append("| # | Orden | Fecha | Paciente | CÃ©dula | Estado | ID |")
            lines.append("|---|-------|-------|----------|--------|--------|-----|")
            for i, o in enumerate(all_ordenes[:40]):
                paciente = (o.get('paciente', '') or '')[:30]
                lines.append(f"| {i+1} | {o.get('num','')} | {o.get('fecha','')} | {paciente} | {o.get('cedula','')} | {o.get('estado','')} | {o.get('id','')} |")
            logger.info(f"[Context] Extracted {len(all_ordenes)} orders from 2 pages")

        # Navigate back to orders page
        await browser.page.goto("https://laboratoriofranz.orion-labs.com/ordenes", timeout=30000)

    except Exception as e:
        logger.warning(f"Could not extract orders context: {e}")

    # Add available exams from CSV file (faster than scraping, complete list)
    exams_context = get_available_exams_context()
    if exams_context:
        lines.append("")
        lines.append(exams_context)

    return "\n".join(lines) if lines else ""


@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Application lifespan - initialize browser and LangGraph.
    """
    global browser, graph, checkpointer, initial_orders_context

    print("Starting Lab Assistant with LangGraph...")

    # Create data directory if it doesn't exist
    data_dir = Path(__file__).parent / "data"
    data_dir.mkdir(exist_ok=True)

    # Initialize browser
    browser = BrowserManager(user_data_dir=settings.browser_data_dir)
    await browser.start(headless=settings.headless, browser=settings.browser_channel)
    await browser.navigate(settings.target_url)
    set_browser(browser)

    # Extract initial orders context (will handle login state)
    initial_orders_context = await get_orders_context()
    if initial_orders_context and "Ã“rdenes Recientes" in initial_orders_context:
        logger.info(f"Extracted initial context with {initial_orders_context.count('|') // 8} orders")
    elif "SESIÃ“N NO INICIADA" in initial_orders_context:
        logger.warning("[Startup] Not logged in - waiting for user to login")

    # Initialize checkpointer for conversation persistence
    # Using MemorySaver for development (in-memory, not persistent across restarts)
    # For production, use AsyncSqliteSaver or PostgresSaver
    checkpointer = MemorySaver()

    # Build and compile graph
    builder = create_lab_agent(browser)
    graph = compile_agent(builder, checkpointer)

    print(f"Lab Assistant ready! Browser at: {browser.page.url}")

    yield

    # Cleanup
    print("Shutting down...")
    close_all_tabs()
    await browser.stop()


# ============================================================
# FASTAPI APP
# ============================================================

app = FastAPI(
    title="Lab Assistant API",
    description="LangGraph-powered lab assistant for clinical laboratory data entry",
    version="2.0.0",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ============================================================
# ENDPOINTS
# ============================================================

@app.get("/api/health")
async def health_check():
    """Health check endpoint."""
    return {
        "status": "ok",
        "browser_url": browser.page.url if browser and browser.page else None,
        "graph_ready": graph is not None
    }


@app.post("/api/chat", response_model=ChatResponse)
async def chat(
    thread_id: str = Form(default=None),
    message: str = Form(...),
    files: List[UploadFile] = File(default=[])
):
    """
    Send a message to the agent and get a response.

    Supports multi-modal input (text, images, audio).
    The agent will execute tools as needed and return when done.

    Args:
        thread_id: Conversation thread ID (generated if not provided)
        message: User message text
        files: Optional image or audio files

    Returns:
        Agent's response with thread_id for continuation
    """
    # Generate thread_id if not provided
    if not thread_id:
        thread_id = str(uuid.uuid4())

    config = {"configurable": {"thread_id": thread_id}}

    # Build message content (multi-modal support)
    content = []

    if message:
        content.append({"type": "text", "text": message})

    # Process uploaded files
    for file in files:
        file_content = await file.read()
        encoded = base64.b64encode(file_content).decode('utf-8')

        if file.content_type and file.content_type.startswith("image/"):
            # Image for vision models
            content.append({
                "type": "image_url",
                "image_url": {"url": f"data:{file.content_type};base64,{encoded}"}
            })
        elif file.content_type and file.content_type.startswith("audio/"):
            # Audio for Gemini (native audio support)
            content.append({
                "type": "media",
                "data": encoded,
                "mime_type": file.content_type
            })

    # Create human message
    if len(content) == 1 and content[0]["type"] == "text":
        human_msg = HumanMessage(content=message)
    else:
        human_msg = HumanMessage(content=content)

    try:
        # Invoke graph - it will loop internally until done
        result = await graph.ainvoke(
            {"messages": [human_msg]},
            config
        )

        # Get the final response
        last_msg = result["messages"][-1]
        response_text = last_msg.content if hasattr(last_msg, 'content') else str(last_msg)

        # Count iterations (based on tool messages)
        tool_messages = [m for m in result["messages"] if hasattr(m, 'type') and getattr(m, 'type', None) == 'tool']
        iterations = len(tool_messages)

        return ChatResponse(
            status="complete",
            message=response_text,
            thread_id=thread_id,
            iterations=iterations
        )

    except Exception as e:
        import traceback
        traceback.print_exc()
        return ChatResponse(
            status="error",
            message=f"Error: {str(e)}",
            thread_id=thread_id
        )


@app.post("/api/chat/stream")
async def chat_stream(request: ChatRequest):
    """
    Stream chat responses in real-time using Server-Sent Events.

    DOCUMENTATION:
    - astream_events: https://langchain-ai.github.io/langgraph/concepts/streaming/

    This streams:
    - Token-by-token LLM output
    - Tool execution notifications
    - Final completion
    """
    thread_id = request.thread_id or str(uuid.uuid4())
    config = {"configurable": {"thread_id": thread_id}}

    async def generate():
        try:
            async for event in graph.astream_events(
                {"messages": [HumanMessage(content=request.message)]},
                config,
                version="v2"
            ):
                event_type = event.get("event", "")

                # Stream LLM tokens
                if event_type == "on_chat_model_stream":
                    chunk = event["data"].get("chunk")
                    if chunk and hasattr(chunk, 'content') and chunk.content:
                        yield f"data: {json.dumps({'type': 'token', 'content': chunk.content})}\n\n"

                # Notify tool execution
                elif event_type == "on_tool_start":
                    tool_name = event["name"]
                    yield f"data: {json.dumps({'type': 'tool_start', 'tool': tool_name})}\n\n"

                elif event_type == "on_tool_end":
                    tool_name = event["name"]
                    yield f"data: {json.dumps({'type': 'tool_end', 'tool': tool_name})}\n\n"

            yield f"data: {json.dumps({'type': 'done', 'thread_id': thread_id})}\n\n"

        except Exception as e:
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={"X-Thread-ID": thread_id}
    )


@app.get("/api/chat/{thread_id}/history")
async def get_history(thread_id: str):
    """
    Get conversation history for a thread.

    Returns list of messages with role and content.
    """
    config = {"configurable": {"thread_id": thread_id}}

    try:
        state = await graph.aget_state(config)
        messages = state.values.get("messages", [])

        return [
            {
                "role": "user" if isinstance(m, HumanMessage) else "assistant",
                "content": m.content if hasattr(m, 'content') else str(m),
                "type": getattr(m, 'type', 'unknown')
            }
            for m in messages
            if not (hasattr(m, 'type') and getattr(m, 'type', None) == 'tool')  # Skip tool messages
        ]
    except Exception as e:
        return []


@app.get("/api/browser/screenshot")
async def get_screenshot():
    """Get current browser screenshot as base64."""
    if browser and browser.page:
        try:
            screenshot_bytes = await browser.page.screenshot(type='png')
            encoded = base64.b64encode(screenshot_bytes).decode('utf-8')
            return {"screenshot": f"data:image/png;base64,{encoded}"}
        except Exception as e:
            raise HTTPException(500, f"Screenshot failed: {str(e)}")
    raise HTTPException(503, "Browser not available")


@app.get("/api/browser/tabs")
async def get_tabs():
    """Get list of open browser tabs."""
    active_tabs = get_active_tabs()
    return {
        "tabs": list(active_tabs.keys()),
        "count": len(active_tabs)
    }


@app.get("/api/browser/tabs/detailed")
async def get_tabs_detailed():
    """Get detailed info about all browser tabs including state."""
    try:
        tabs_info = await _get_browser_tabs_impl()
        return tabs_info
    except Exception as e:
        logger.error(f"Failed to get detailed tabs: {e}")
        return {"error": str(e), "tabs": []}


@app.post("/api/browser/close-tabs")
async def close_tabs():
    """Close all open browser tabs (cleanup)."""
    close_all_tabs()
    return {"status": "ok", "message": "All tabs closed"}


# ============================================================
# MANUAL TOOL EXECUTION ENDPOINT
# ============================================================

class ManualToolRequest(BaseModel):
    tool: str
    args: dict

@app.post("/api/tools/execute")
async def execute_tool(request: ManualToolRequest):
    """
    Execute a tool manually (for UI-based editing).
    Supports: edit_results, edit_order_exams, create_new_order
    """
    from graph.tools import (
        edit_results,
        edit_order_exams,
        create_new_order,
    )

    tool_name = request.tool
    args = request.args

    logger.info(f"[Manual Tool] Executing {tool_name} with args: {args}")

    try:
        if tool_name == "edit_results":
            # edit_results expects data=[{orden, e, f, v}]
            data = args.get("data", [])
            result = await edit_results.ainvoke({"data": data})
            return {"success": True, "message": f"Editados {len(data)} campos", "result": result}

        elif tool_name == "edit_order_exams":
            # edit_order_exams now supports order_id, tab_index, cedula, add, remove
            order_id = args.get("order_id")
            tab_index = args.get("tab_index")
            cedula = args.get("cedula")
            add_exams = args.get("add", [])
            remove_exams = args.get("remove", [])
            result = await edit_order_exams.ainvoke({
                "order_id": order_id,
                "tab_index": tab_index,
                "cedula": cedula,
                "add": add_exams,
                "remove": remove_exams
            })
            changes = []
            if cedula:
                changes.append(f"cÃ©dula: {cedula}")
            if add_exams:
                changes.append(f"agregados: {', '.join(add_exams)}")
            if remove_exams:
                changes.append(f"removidos: {', '.join(remove_exams)}")
            return {"success": True, "message": f"Cambios: {' | '.join(changes) if changes else 'ninguno'}", "result": result}

        elif tool_name == "create_new_order":
            cedula = args.get("cedula", "")
            exams = args.get("exams", [])
            result = await create_new_order.ainvoke({"cedula": cedula, "exams": exams})
            return {"success": True, "message": f"Orden creada con {len(exams)} exÃ¡menes", "result": result}

        else:
            return {"success": False, "error": f"Tool '{tool_name}' not supported for manual execution"}

    except Exception as e:
        logger.error(f"[Manual Tool] Error: {e}")
        return {"success": False, "error": str(e)}


@app.get("/api/exams")
async def get_exams():
    """Get list of available exams from CSV."""
    exams = load_exams_from_csv()
    return {"exams": [{"codigo": e["codigo"], "nombre": e["nombre"]} for e in exams]}


# ============================================================
# OPENAI-COMPATIBLE ENDPOINT (Optional - for LobeChat integration)
# ============================================================

class OpenAIChatRequest(BaseModel):
    model: str
    messages: List[dict]
    stream: bool = False
    temperature: float = 0.7


@app.post("/v1/chat/completions")
async def openai_compatible_chat(request: OpenAIChatRequest):
    """
    OpenAI-compatible chat completions endpoint.

    This translates OpenAI format to our LangGraph agent format,
    allowing LobeChat to use our agent as a model provider.
    """
    import time as time_module

    # Debug: log raw request
    logger.debug(f"[Request] Messages count: {len(request.messages)}")
    for i, msg in enumerate(request.messages):
        content = msg.get('content', '')
        # Handle multimodal content (list with text/images/audio)
        if isinstance(content, list):
            content_summary = []
            for part in content:
                if isinstance(part, dict):
                    if part.get('type') == 'text':
                        content_summary.append(f"text:{part.get('text', '')[:50]}")
                    elif part.get('type') == 'image_url':
                        content_summary.append("[IMAGE]")
                    elif part.get('type') == 'media':
                        mime = part.get('mime_type', 'unknown')
                        content_summary.append(f"[AUDIO:{mime}]" if 'audio' in mime else f"[VIDEO:{mime}]")
                    else:
                        content_summary.append(f"[{part.get('type', 'unknown')}]")
                else:
                    content_summary.append(str(part)[:30])
            content_str = ', '.join(content_summary)
        else:
            content_str = str(content)[:100]
        logger.debug(f"[Request] [{i}] role={msg.get('role')}, content={content_str}")

    # Detect and reject LobeChat's auxiliary requests (topic naming, translation, etc.)
    # These have role=developer/system with summarization/translation prompts
    for msg in request.messages:
        role = msg.get("role", "")
        if role in ["developer", "system"]:
            content = str(msg.get("content", "")).lower()
            if any(keyword in content for keyword in ["summarizer", "summarize", "title", "translate", "translation", "compress"]):
                logger.info(f"[Request] Skipping auxiliary request (topic naming/translation)")
                # Return a simple response without invoking the agent
                response_id = f"chatcmpl-{uuid.uuid4().hex[:8]}"
                created_time = int(time_module.time())
                if request.stream:
                    async def simple_stream():
                        data = {
                            "id": response_id,
                            "object": "chat.completion.chunk",
                            "created": created_time,
                            "model": request.model,
                            "choices": [{"index": 0, "delta": {"content": "Lab Assistant"}, "finish_reason": None}]
                        }
                        yield f"data: {json.dumps(data)}\n\n"
                        final = {
                            "id": response_id,
                            "object": "chat.completion.chunk",
                            "created": created_time,
                            "model": request.model,
                            "choices": [{"index": 0, "delta": {}, "finish_reason": "stop"}]
                        }
                        yield f"data: {json.dumps(final)}\n\n"
                        yield "data: [DONE]\n\n"
                    return StreamingResponse(simple_stream(), media_type="text/event-stream")
                else:
                    return {
                        "id": response_id,
                        "object": "chat.completion",
                        "created": created_time,
                        "model": request.model,
                        "choices": [{"index": 0, "message": {"role": "assistant", "content": "Lab Assistant"}, "finish_reason": "stop"}]
                    }

    thread_id = str(uuid.uuid4())
    config = {"configurable": {"thread_id": thread_id}}

    # Convert OpenAI-format messages to LangGraph messages
    # This preserves full conversation history from the frontend
    conversation_messages = []
    for msg in request.messages:
        role = msg.get("role", "")
        content = msg.get("content", "")

        # Handle multimodal content (images, audio, video) - convert to LangChain format
        if isinstance(content, list):
            # Convert OpenAI multimodal format to LangChain format
            lc_content = []
            for part in content:
                if isinstance(part, dict):
                    if part.get("type") == "text":
                        lc_content.append({"type": "text", "text": part.get("text", "")})
                    elif part.get("type") == "image_url":
                        image_url = part.get("image_url", {})
                        url = image_url.get("url", "") if isinstance(image_url, dict) else image_url
                        lc_content.append({"type": "image_url", "image_url": {"url": url}})
                    elif part.get("type") == "media":
                        # Audio/video for Gemini (native format)
                        lc_content.append({
                            "type": "media",
                            "data": part.get("data", ""),
                            "mime_type": part.get("mime_type", "audio/webm")
                        })
            content = lc_content if lc_content else ""

        if role == "user":
            conversation_messages.append(HumanMessage(content=content))
        elif role == "assistant":
            conversation_messages.append(AIMessage(content=content))
        # Skip system/developer roles - they're handled separately

    if not conversation_messages:
        logger.error("No messages found in request")
        return {"error": "No messages found"}

    # Get the last user message for logging
    last_user_message = None
    for msg in reversed(request.messages):
        if msg["role"] == "user":
            last_user_message = msg["content"]
            break

    logger.info("=" * 60)
    # Handle multimodal user message for logging
    if isinstance(last_user_message, list):
        msg_parts = []
        for part in last_user_message:
            if isinstance(part, dict):
                if part.get('type') == 'text':
                    msg_parts.append(part.get('text', '')[:100])
                elif part.get('type') == 'image_url':
                    msg_parts.append('[IMAGE]')
                elif part.get('type') == 'media':
                    mime = part.get('mime_type', 'unknown')
                    msg_parts.append(f'[AUDIO:{mime}]' if 'audio' in mime else f'[VIDEO:{mime}]')
                else:
                    msg_parts.append(f"[{part.get('type', 'unknown')}]")
        user_msg_display = ' + '.join(msg_parts)
    else:
        user_msg_display = str(last_user_message)[:200]
    logger.info(f"USER MESSAGE: {user_msg_display}{'...' if len(str(last_user_message)) > 200 else ''}")
    logger.info(f"Thread ID: {thread_id}, Stream: {request.stream}, History: {len(conversation_messages)} messages")

    if request.stream:
        async def generate():
            global orders_context_sent
            full_response = []
            response_id = f"chatcmpl-{uuid.uuid4().hex[:8]}"  # Same ID for all chunks
            created_time = int(time_module.time())  # Unix timestamp

            # Step tracking for enumeration
            step_counter = 0
            total_input_tokens = 0
            total_output_tokens = 0
            seen_run_ids = set()  # Track unique LLM calls to avoid double counting

            # Gemini pricing (per 1M tokens) - adjust based on model
            # Gemini 3 Flash Preview: $0.50 input, $3.00 output (incl. thinking tokens)
            INPUT_PRICE_PER_1M = 0.50
            OUTPUT_PRICE_PER_1M = 3.00

            try:
                # Check if this is first message in conversation (from frontend history)
                is_first_message = len(conversation_messages) <= 2  # Just system + first user msg

                # Reset tab state tracking on new conversation
                if is_first_message:
                    reset_tab_state()

                # Get current context (checks login state, fetches orders if needed)
                current_context = await get_orders_context()

                # Get browser tabs context (always at each user message)
                tabs_context = await get_browser_tabs_context()

                # Pass full conversation history to the graph
                initial_state = {"messages": conversation_messages}

                # Build full context
                context_parts = []

                # Include orders context if: first message OR we haven't sent valid orders yet
                should_include_orders = is_first_message or not orders_context_sent
                if should_include_orders and current_context:
                    context_parts.append(current_context)
                    if "SESIÃ“N NO INICIADA" in current_context:
                        logger.info("[Chat] Not logged in - sending login reminder")
                    else:
                        logger.info("[Chat] Including orders context")

                # Always include tabs context at every message
                if tabs_context:
                    context_parts.append(tabs_context)
                    logger.info("[Chat] Including browser tabs context")

                if context_parts:
                    initial_state["current_page_context"] = "\n\n".join(context_parts)

                # Save debug request with full info (system prompt, tools, context, messages)
                full_context = initial_state.get("current_page_context", "")
                debug_id = save_debug_request(
                    thread_id=thread_id,
                    raw_messages=request.messages,
                    converted_messages=conversation_messages,
                    context=full_context
                )

                async for event in graph.astream_events(
                    initial_state,
                    config,
                    version="v2"
                ):
                    event_type = event.get("event", "")

                    # Track LLM calls for step enumeration (avoid double counting from wrapper)
                    if event_type == "on_chat_model_start":
                        run_id = event.get("run_id", "")
                        # Only count if this is a new run_id (wrapper and inner model share same run_id parent)
                        # Check metadata to see if this is the actual Gemini call
                        metadata = event.get("metadata", {})
                        model_name = event.get("name", "")
                        # Only count events from the actual ChatGoogleGenerativeAI, not the wrapper
                        if "ChatGoogleGenerativeAI" in model_name or "gemini" in model_name.lower():
                            if run_id and run_id not in seen_run_ids:
                                seen_run_ids.add(run_id)
                                step_counter += 1
                                logger.info(f"[Step {step_counter}] LLM call started (run_id: {run_id[:8]})")

                    # Stream tool calls to show "thinking" in LobeChat
                    if event_type == "on_tool_start":
                        tool_name = event.get("name", "unknown")
                        tool_input = event.get("data", {}).get("input", {})
                        logger.info(f"TOOL CALL: {tool_name}")
                        logger.debug(f"  Input: {json.dumps(tool_input, ensure_ascii=False)[:500]}")

                        # Send tool call as a "thinking" step to frontend with step number
                        tool_display = f"**[{step_counter}]** ðŸ”§ **{tool_name}**"
                        if tool_input:
                            # Show key parameters
                            params = []
                            for k, v in tool_input.items():
                                if isinstance(v, str) and len(v) < 50:
                                    params.append(f"{k}={v}")
                                elif isinstance(v, list) and len(v) < 5:
                                    params.append(f"{k}={v}")
                            if params:
                                tool_display += f" ({', '.join(params[:3])})"
                        tool_display += "\n"

                        data = {
                            "id": response_id,
                            "object": "chat.completion.chunk",
                            "created": created_time,
                            "model": request.model,
                            "choices": [{
                                "index": 0,
                                "delta": {"content": tool_display},
                                "finish_reason": None
                            }]
                        }
                        yield f"data: {json.dumps(data)}\n\n"

                    elif event_type == "on_tool_end":
                        tool_name = event.get("name", "unknown")
                        tool_output = event.get("data", {}).get("output", "")
                        logger.info(f"TOOL RESULT: {tool_name}")
                        logger.debug(f"  Output: {str(tool_output)[:500]}")

                        # Send brief result indicator
                        result_display = f"âœ“ {tool_name} completado\n\n"
                        data = {
                            "id": response_id,
                            "object": "chat.completion.chunk",
                            "created": created_time,
                            "model": request.model,
                            "choices": [{
                                "index": 0,
                                "delta": {"content": result_display},
                                "finish_reason": None
                            }]
                        }
                        yield f"data: {json.dumps(data)}\n\n"

                    elif event_type == "on_chat_model_stream":
                        chunk = event["data"].get("chunk")
                        if chunk and hasattr(chunk, 'content') and chunk.content:
                            # Handle both string and list content (Gemini 3 with thinking)
                            content = chunk.content
                            if isinstance(content, list):
                                # Extract text parts only, skip thinking parts
                                text_parts = [p.get('text', '') for p in content if isinstance(p, dict) and p.get('type') == 'text']
                                content = ''.join(text_parts)
                            if content:
                                full_response.append(content)
                                data = {
                                    "id": response_id,
                                    "object": "chat.completion.chunk",
                                    "created": created_time,
                                    "model": request.model,
                                    "choices": [{
                                        "index": 0,
                                        "delta": {"content": content},
                                        "finish_reason": None
                                    }]
                                }
                                yield f"data: {json.dumps(data)}\n\n"

                    # Handle non-streaming model responses (after key rotation) and extract token usage
                    elif event_type == "on_chat_model_end":
                        output = event.get("data", {}).get("output")

                        # Try to extract token usage from response
                        if output:
                            # Check for usage_metadata on AIMessage (it's a dict, not object)
                            # LangChain format: {'input_tokens': X, 'output_tokens': Y, 'total_tokens': Z}
                            usage = getattr(output, 'usage_metadata', None)
                            if usage and isinstance(usage, dict):
                                input_tokens = usage.get('input_tokens', 0) or usage.get('prompt_token_count', 0) or 0
                                output_tokens = usage.get('output_tokens', 0) or usage.get('candidates_token_count', 0) or 0
                                if input_tokens or output_tokens:
                                    total_input_tokens += input_tokens
                                    total_output_tokens += output_tokens
                                    # Log thinking tokens if available
                                    output_details = usage.get('output_token_details', {})
                                    thinking_tokens = output_details.get('reasoning', 0) if output_details else 0
                                    logger.info(f"[Step {step_counter}] Tokens: in={input_tokens}, out={output_tokens}" +
                                               (f" (thinking={thinking_tokens})" if thinking_tokens else ""))

                            # Also check response_metadata for langchain (Google-specific format)
                            resp_meta = getattr(output, 'response_metadata', {}) or {}
                            if resp_meta and 'usage_metadata' in resp_meta:
                                usage_meta = resp_meta['usage_metadata']
                                # Google format: prompt_token_count, candidates_token_count
                                input_tokens = usage_meta.get('prompt_token_count', 0)
                                output_tokens = usage_meta.get('candidates_token_count', 0)
                                # Avoid double counting - only add if we didn't get from usage_metadata above
                                if (input_tokens or output_tokens) and not usage:
                                    total_input_tokens += input_tokens
                                    total_output_tokens += output_tokens
                                    logger.info(f"[Step {step_counter}] Tokens (from response_meta): in={input_tokens}, out={output_tokens}")

                        if output and hasattr(output, 'content') and output.content:
                            content = output.content
                            if isinstance(content, list):
                                text_parts = [p.get('text', '') for p in content if isinstance(p, dict) and p.get('type') == 'text']
                                content = ''.join(text_parts)
                            # Only send if we haven't already streamed this content
                            if content and content not in ''.join(full_response):
                                full_response.append(content)
                                data = {
                                    "id": response_id,
                                    "object": "chat.completion.chunk",
                                    "created": created_time,
                                    "model": request.model,
                                    "choices": [{
                                        "index": 0,
                                        "delta": {"content": content},
                                        "finish_reason": None
                                    }]
                                }
                                yield f"data: {json.dumps(data)}\n\n"

                if full_response:
                    logger.info(f"AI RESPONSE: {''.join(full_response)[:300]}{'...' if len(''.join(full_response)) > 300 else ''}")
                    # Update debug request with response
                    update_debug_request(debug_id, ''.join(full_response))

                # Calculate and display usage summary
                total_tokens = total_input_tokens + total_output_tokens
                input_cost = (total_input_tokens / 1_000_000) * INPUT_PRICE_PER_1M
                output_cost = (total_output_tokens / 1_000_000) * OUTPUT_PRICE_PER_1M
                total_cost = input_cost + output_cost

                # Send usage summary at the end
                usage_summary = f"\n\n---\nðŸ“Š **Stats**: {step_counter} LLM calls"
                if total_tokens > 0:
                    usage_summary += f" | Tokens: {total_input_tokens:,} in + {total_output_tokens:,} out = {total_tokens:,}"
                    usage_summary += f" | Est. cost: ${total_cost:.6f}"
                usage_summary += "\n"

                data = {
                    "id": response_id,
                    "object": "chat.completion.chunk",
                    "created": created_time,
                    "model": request.model,
                    "choices": [{
                        "index": 0,
                        "delta": {"content": usage_summary},
                        "finish_reason": None
                    }]
                }
                yield f"data: {json.dumps(data)}\n\n"

                logger.info(f"[Usage] Steps: {step_counter}, Input: {total_input_tokens}, Output: {total_output_tokens}, Cost: ${total_cost:.6f}")

                # Send final chunk with finish_reason to signal completion
                final_chunk = {
                    "id": response_id,
                    "object": "chat.completion.chunk",
                    "created": created_time,
                    "model": request.model,
                    "choices": [{
                        "index": 0,
                        "delta": {},
                        "finish_reason": "stop"
                    }]
                }
                yield f"data: {json.dumps(final_chunk)}\n\n"
                yield "data: [DONE]\n\n"

            except Exception as e:
                logger.error(f"Stream error: {str(e)}", exc_info=True)
                # Update debug request with error
                update_debug_request(debug_id, '', error=str(e))
                yield f"data: {json.dumps({'error': str(e)})}\n\n"

        return StreamingResponse(
            generate(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",  # Disable nginx buffering
            }
        )

    else:
        try:
            # Check if this is first message in conversation (from frontend history)
            is_first_message = len(conversation_messages) <= 2  # Just system + first user msg

            # Reset tab state tracking on new conversation
            if is_first_message:
                reset_tab_state()

            # Get current context (checks login state, fetches orders if needed)
            current_context = await get_orders_context()

            # Get browser tabs context (always at each user message)
            tabs_context = await get_browser_tabs_context()

            # Pass full conversation history to the graph
            initial_state = {"messages": conversation_messages}

            # Build full context
            context_parts = []

            # Include orders context if: first message OR we haven't sent valid orders yet
            should_include_orders = is_first_message or not orders_context_sent
            if should_include_orders and current_context:
                context_parts.append(current_context)
                if "SESIÃ“N NO INICIADA" in current_context:
                    logger.info("[Chat] Not logged in - sending login reminder")
                else:
                    logger.info("[Chat] Including orders context")

            # Always include tabs context at every message
            if tabs_context:
                context_parts.append(tabs_context)
                logger.info("[Chat] Including browser tabs context")

            if context_parts:
                initial_state["current_page_context"] = "\n\n".join(context_parts)

            # Save debug request with full info (system prompt, tools, context, messages)
            full_context = initial_state.get("current_page_context", "")
            debug_id = save_debug_request(
                thread_id=thread_id,
                raw_messages=request.messages,
                converted_messages=conversation_messages,
                context=full_context
            )

            logger.info("Invoking LangGraph agent...")
            result = await graph.ainvoke(initial_state, config)

            # Log all messages for debugging
            for i, msg in enumerate(result["messages"]):
                msg_type = type(msg).__name__
                content = msg.content if hasattr(msg, 'content') else str(msg)
                if hasattr(msg, 'tool_calls') and msg.tool_calls:
                    logger.info(f"  [{i}] {msg_type}: {len(msg.tool_calls)} tool calls")
                    for tc in msg.tool_calls:
                        logger.info(f"      -> {tc.get('name', 'unknown')}: {json.dumps(tc.get('args', {}), ensure_ascii=False)[:200]}")
                else:
                    logger.info(f"  [{i}] {msg_type}: {content[:150]}{'...' if len(content) > 150 else ''}")

            last_msg = result["messages"][-1]
            response_text = last_msg.content if hasattr(last_msg, 'content') else str(last_msg)

            logger.info(f"AI RESPONSE: {response_text[:300]}{'...' if len(response_text) > 300 else ''}")
            logger.info("=" * 60)

            # Update debug request with response
            update_debug_request(debug_id, response_text)

            return {
                "id": f"chatcmpl-{uuid.uuid4().hex[:8]}",
                "object": "chat.completion",
                "model": request.model,
                "choices": [{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": response_text
                    },
                    "finish_reason": "stop"
                }]
            }
        except Exception as e:
            logger.error(f"Error invoking agent: {str(e)}", exc_info=True)
            # Update debug request with error
            update_debug_request(debug_id, '', error=str(e))
            return {
                "id": f"chatcmpl-{uuid.uuid4().hex[:8]}",
                "object": "chat.completion",
                "model": request.model,
                "choices": [{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": f"Error: {str(e)}"
                    },
                    "finish_reason": "stop"
                }]
            }


# ============================================================
# MAIN
# ============================================================

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "server:app",
        host=settings.host,
        port=settings.port,
        reload=settings.debug
    )
