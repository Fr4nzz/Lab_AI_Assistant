# Open Notebook with Claude Code (Max Subscription)
#
# This setup uses Claude Code via your Max subscription instead of API keys.
# The Claude Code proxy runs locally (not in Docker) to use your authenticated Claude.
#
# SETUP:
# 1. First, start the Claude Code proxy locally:
#    cd claude-code-proxy
#    run.bat  (Windows) or ./run.sh (Linux/macOS)
#
# 2. Then start Open Notebook:
#    docker compose up -d
#
# 3. Download required models:
#    docker exec open-notebook-ollama ollama pull mxbai-embed-large
#    docker compose exec speaches uv tool run speaches-cli model download speaches-ai/Kokoro-82M-v1.0-ONNX
#
# 4. Access Open Notebook at: http://localhost:8502

services:
  # SurrealDB - Database for Open Notebook
  surrealdb:
    image: surrealdb/surrealdb:v2
    container_name: open-notebook-db
    # Use memory storage to avoid permission issues, or use file with proper path
    command: start --user root --pass password --bind 0.0.0.0:8000 memory
    # For persistent storage, uncomment below and comment out memory line above:
    # command: start --user root --pass password --bind 0.0.0.0:8000 file:/data/database.db
    ports:
      - "8000:8000"
    volumes:
      - surreal_data:/data
    user: root
    restart: unless-stopped

  # Open Notebook - Main Application
  open_notebook:
    image: lfnovo/open_notebook:v1-latest
    container_name: open-notebook-app
    ports:
      - "8502:8502"  # Web UI
      - "5055:5055"  # REST API
    environment:
      # Database connection
      - SURREAL_URL=ws://surrealdb:8000/rpc
      - SURREAL_USER=root
      - SURREAL_PASSWORD=password
      - SURREAL_NAMESPACE=open_notebook
      - SURREAL_DATABASE=open_notebook

      # Claude Code Proxy (OpenAI-compatible endpoint)
      # host.docker.internal allows Docker to connect to host machine
      # IMPORTANT: Need BOTH generic URL (for provider detection) and LLM-specific URL
      - OPENAI_COMPATIBLE_BASE_URL=http://host.docker.internal:8080/v1
      - OPENAI_COMPATIBLE_API_KEY=dummy-key
      - OPENAI_COMPATIBLE_BASE_URL_LLM=http://host.docker.internal:8080/v1
      - OPENAI_COMPATIBLE_API_KEY_LLM=dummy-key

      # Speaches TTS for podcast generation (Kokoro model)
      - OPENAI_COMPATIBLE_BASE_URL_TTS=http://speaches:8000/v1
      - OPENAI_COMPATIBLE_API_KEY_TTS=dummy-key

      # Ollama for embeddings (Claude Code doesn't provide embeddings)
      - OLLAMA_API_BASE=http://ollama:11434

      # Timeout settings (Claude Code can be slow)
      - ESPERANTO_LLM_TIMEOUT=120
      - API_CLIENT_TIMEOUT=300

      # Optional: Password protect your instance
      # - OPEN_NOTEBOOK_PASSWORD=your-password

    volumes:
      - ./notebook_data:/app/data
    depends_on:
      - surrealdb
      - ollama
      - speaches
    extra_hosts:
      - "host.docker.internal:host-gateway"  # Allow access to host machine
    restart: unless-stopped

  # Ollama for local embeddings (mxbai-embed-large recommended)
  ollama:
    image: ollama/ollama:latest
    container_name: open-notebook-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
    # GPU support for NVIDIA (RTX 4050)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

  # Speaches TTS for podcast generation (Kokoro model - GPU accelerated)
  speaches:
    image: ghcr.io/speaches-ai/speaches:v0.6.0-cuda
    container_name: open-notebook-speaches
    ports:
      - "8969:8000"
    volumes:
      - speaches_models:/root/.cache/huggingface
    environment:
      - SPEACHES_LOG_LEVEL=info
    # GPU support for NVIDIA (RTX 4050) - much faster TTS
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

volumes:
  surreal_data:
  ollama_models:
  speaches_models:
